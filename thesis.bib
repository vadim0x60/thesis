
@inproceedings{vaithilingamExpectationVsExperience2022,
	address = {New Orleans LA USA},
	title = {Expectation vs. {Experience}: {Evaluating} the {Usability} of {Code} {Generation} {Tools} {Powered} by {Large} {Language} {Models}},
	copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
	isbn = {978-1-4503-9156-6},
	shorttitle = {Expectation vs. {Experience}},
	url = {https://dl.acm.org/doi/10.1145/3491101.3519665},
	doi = {10.1145/3491101.3519665},
	abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants’ feedback.},
	language = {en},
	urldate = {2024-06-07},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} {Extended} {Abstracts}},
	publisher = {ACM},
	author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
	month = apr,
	year = {2022},
	pages = {1--7},
}

@article{pratherItWeirdThat2024,
	title = {"{It}'s {Weird} {That} it {Knows} {What} {I} {Want}": {Usability} and {Interactions} with {Copilot} for {Novice} {Programmers}},
	volume = {31},
	issn = {1073-0516, 1557-7325},
	shorttitle = {"{It}'s {Weird} {That} it {Knows} {What} {I} {Want}"},
	url = {http://arxiv.org/abs/2304.02491},
	doi = {10.1145/3617367},
	abstract = {Recent developments in deep learning have resulted in code-generation models that produce source code from natural language and code-based prompts with high accuracy. This is likely to have profound effects in the classroom, where novices learning to code can now use free tools to automatically suggest solutions to programming exercises and assignments. However, little is currently known about how novices interact with these tools in practice. We present the first study that observes students at the introductory level using one such code auto-generating tool, Github Copilot, on a typical introductory programming (CS1) assignment. Through observations and interviews we explore student perceptions of the benefits and pitfalls of this technology for learning, present new observed interaction patterns, and discuss cognitive and metacognitive difficulties faced by students. We consider design implications of these findings, specifically in terms of how tools like Copilot can better support and scaffold the novice programming experience.},
	number = {1},
	urldate = {2024-06-07},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Prather, James and Reeves, Brent N. and Denny, Paul and Becker, Brett A. and Leinonen, Juho and Luxton-Reilly, Andrew and Powell, Garrett and Finnie-Ansley, James and Santos, Eddie Antonio},
	month = feb,
	year = {2024},
	note = {arXiv:2304.02491 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	pages = {1--31},
}

@misc{erhaborMeasuringRuntimePerformance2023,
	title = {Measuring the {Runtime} {Performance} of {Code} {Produced} with {GitHub} {Copilot}},
	url = {http://arxiv.org/abs/2305.06439},
	doi = {10.48550/arXiv.2305.06439},
	abstract = {GitHub Copilot is an artificially intelligent programming assistant used by many developers. While a few studies have evaluated the security risks of using Copilot, there has not been any study to show if it aids developers in producing code with better runtime performance. We evaluate the runtime performance of code produced when developers use GitHub Copilot versus when they do not. To this end, we conducted a user study with 32 participants where each participant solved two C++ programming problems, one with Copilot and the other without it and measured the runtime performance of the participants' solutions on our test data. Our results suggest that using Copilot may produce code with a significantly slower runtime performance.},
	urldate = {2024-06-07},
	publisher = {arXiv},
	author = {Erhabor, Daniel and Udayashankar, Sreeharsha and Nagappan, Meiyappan and Al-Kiswany, Samer},
	month = may,
	year = {2023},
	note = {arXiv:2305.06439 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{barkeGroundedCopilotHow2023,
	title = {Grounded {Copilot}: {How} {Programmers} {Interact} with {Code}-{Generating} {Models}},
	volume = {7},
	shorttitle = {Grounded {Copilot}},
	url = {https://dl.acm.org/doi/10.1145/3586030},
	doi = {10.1145/3586030},
	abstract = {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants—with a range of prior experience using the assistant—as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
	number = {OOPSLA1},
	urldate = {2024-06-07},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
	month = apr,
	year = {2023},
	keywords = {AI Assistants, Grounded Theory, Program Synthesis},
	pages = {78:85--78:111},
}

@article{lomontFastInverseSquare2003,
	title = {Fast inverse square root},
	volume = {32},
	journal = {Tech-315 nical Report},
	author = {Lomont, Chris},
	year = {2003},
}

@inproceedings{nguyenEmpiricalEvaluationGitHub2022,
	title = {An empirical evaluation of {GitHub} copilot's code suggestions},
	author = {Nguyen, Nhan and Nadi, Sarah},
	year = {2022},
	pages = {1--5},
}

@article{dakhelGithubCopilotAi2023,
	title = {Github copilot ai pair programmer: {Asset} or liability?},
	volume = {203},
	issn = {0164-1212},
	journal = {Journal of Systems and Software},
	author = {Dakhel, Arghavan Moradi and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Jiang, Zhen Ming Jack},
	year = {2023},
	note = {Publisher: Elsevier},
	pages = {111734},
}

@inproceedings{wermelingerUsingGithubCopilot2023,
	title = {Using github copilot to solve simple programming problems},
	author = {Wermelinger, Michel},
	year = {2023},
	pages = {172--178},
}

@article{caseComputeraidedSoftwareEngineering1985,
	title = {Computer-aided software engineering ({CASE}) technology for improving software development productivity},
	volume = {17},
	issn = {0095-0033},
	number = {1},
	journal = {ACM SIGMIS Database: the DATABASE for Advances in Information Systems},
	author = {Case, Albert F},
	year = {1985},
	note = {Publisher: ACM New York, NY, USA},
	pages = {35--43},
}

@misc{freszHowShouldAI2024,
	title = {How should {AI} decisions be explained? {Requirements} for {Explanations} from the {Perspective} of {European} {Law}},
	shorttitle = {How should {AI} decisions be explained?},
	url = {http://arxiv.org/abs/2404.12762},
	doi = {10.48550/arXiv.2404.12762},
	abstract = {This paper investigates the relationship between law and eXplainable Artificial Intelligence (XAI). While there is much discussion about the AI Act, for which the trilogue of the European Parliament, Council and Commission recently concluded, other areas of law seem underexplored. This paper focuses on European (and in part German) law, although with international concepts and regulations such as fiduciary plausibility checks, the General Data Protection Regulation (GDPR), and product safety and liability. Based on XAI-taxonomies, requirements for XAI-methods are derived from each of the legal bases, resulting in the conclusion that each legal basis requires different XAI properties and that the current state of the art does not fulfill these to full satisfaction, especially regarding the correctness (sometimes called fidelity) and confidence estimates of XAI-methods.},
	urldate = {2024-06-07},
	publisher = {arXiv},
	author = {Fresz, Benjamin and Dubovitskaya, Elena and Brajovic, Danilo and Huber, Marco and Horz, Christian},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12762 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@article{hackerExplainableAIContract2020,
	title = {Explainable {AI} under contract and tort law: legal incentives and technical challenges},
	volume = {28},
	issn = {0924-8463, 1572-8382},
	shorttitle = {Explainable {AI} under contract and tort law},
	url = {http://link.springer.com/10.1007/s10506-020-09260-6},
	doi = {10.1007/s10506-020-09260-6},
	abstract = {This paper shows that the law, in subtle ways, may set hitherto unrecognized incentives for the adoption of explainable machine learning applications. In doing so, we make two novel contributions. First, on the legal side, we show that to avoid liability, professional actors, such as doctors and managers, may soon be legally compelled to use explainable ML models. We argue that the importance of explainability reaches far beyond data protection law, and crucially influences questions of contractual and tort liability for the use of ML models. To this effect, we conduct two legal case studies, in medical and corporate merger applications of ML. As a second contribution, we discuss the (legally required) trade-off between accuracy and explainability and demonstrate the effect in a technical case study in the context of spam classification.},
	language = {en},
	number = {4},
	urldate = {2024-06-07},
	journal = {Artificial Intelligence and Law},
	author = {Hacker, Philipp and Krestel, Ralf and Grundmann, Stefan and Naumann, Felix},
	month = dec,
	year = {2020},
	pages = {415--439},
}

@article{bagPeerTransparencyTeams2012,
	title = {Peer transparency in teams: {Does} it help or hinder incentives?},
	volume = {53},
	issn = {0020-6598},
	number = {4},
	journal = {International Economic Review},
	author = {Bag, Parimal Kanti and Pepito, Nona},
	year = {2012},
	note = {Publisher: Wiley Online Library},
	pages = {1257--1286},
}

@article{mnihPlayingAtariDeep2013,
	title = {Playing atari with deep reinforcement learning},
	journal = {arXiv preprint arXiv:1312.5602},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year = {2013},
}

@article{watsonClinicalApplicationsMachine2019,
	title = {Clinical applications of machine learning algorithms: beyond the black box},
	volume = {364},
	issn = {0959-8138},
	journal = {Bmj},
	author = {Watson, David S and Krutzinna, Jenny and Bruce, Ian N and Griffiths, Christopher EM and McInnes, Iain B and Barnes, Michael R and Floridi, Luciano},
	year = {2019},
	note = {Publisher: British Medical Journal Publishing Group},
}

@article{priceBigDataBlackbox2018,
	title = {Big data and black-box medical algorithms},
	volume = {10},
	issn = {1946-6234},
	number = {471},
	journal = {Science translational medicine},
	author = {Price, W Nicholson},
	year = {2018},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaao5333},
}

@misc{liventsevPhilHumansBenchmarkingMachine2024,
	title = {{PhilHumans}: {Benchmarking} {Machine} {Learning} for {Personal} {Health}},
	copyright = {All rights reserved},
	shorttitle = {{PhilHumans}},
	url = {http://arxiv.org/abs/2405.02770},
	doi = {10.48550/arXiv.2405.02770},
	abstract = {The use of machine learning in Healthcare has the potential to improve patient outcomes as well as broaden the reach and affordability of Healthcare. The history of other application areas indicates that strong benchmarks are essential for the development of intelligent systems. We present Personal Health Interfaces Leveraging HUman-MAchine Natural interactions (PhilHumans), a holistic suite of benchmarks for machine learning across different Healthcare settings - talk therapy, diet coaching, emergency care, intensive care, obstetric sonography - as well as different learning settings, such as action anticipation, timeseries modeling, insight mining, language modeling, computer vision, reinforcement learning and program synthesis},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Liventsev, Vadim and Kumar, Vivek and Susaiyah, Allmin Pradhap Singh and Wu, Zixiu and Rodin, Ivan and Yaar, Asfand and Balloccu, Simone and Beraziuk, Marharyta and Battiato, Sebastiano and Farinella, Giovanni Maria and Härmä, Aki and Helaoui, Rim and Petkovic, Milan and Recupero, Diego Reforgiato and Reiter, Ehud and Riboni, Daniele and Sterling, Raymond},
	month = may,
	year = {2024},
	note = {arXiv:2405.02770 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{lancetHealthcareSystemStaffing2018,
	title = {Health-care system staffing: a universal shortfall},
	volume = {392},
	issn = {1474-547X},
	number = {10161},
	journal = {Lancet (London, England)},
	author = {Lancet, The},
	year = {2018},
	pages = {2238},
}

@book{faltys2021,
	title = {{HiRID}, a high time-resolution icu dataset (version 1.1.1},
	language = {en},
	publisher = {PhysioNet},
	author = {Faltys, M. and Zimmermann, M. and Lyu, X. and Hüser, M. and Hyland, S. and Rätsch, G. and Merz, T.},
	year = {2021},
}

@article{campbellUniversalHealthCoverage2013,
	title = {Towards universal health coverage: a health workforce fit for purpose and practice},
	volume = {91},
	number = {11},
	journal = {Bulletin of the World Health Organization},
	author = {Campbell, James},
	year = {2013},
	note = {Publisher: World Health Organization},
	pages = {887},
}

@article{gulwaniProgramSynthesis2017,
	title = {Program {Synthesis}},
	volume = {4},
	issn = {2325-1107, 2325-1131},
	url = {https://www.nowpublishers.com/article/Details/PGL-010},
	doi = {10.1561/2500000010},
	abstract = {Program Synthesis},
	language = {English},
	number = {1-2},
	urldate = {2023-12-14},
	journal = {Foundations and Trends® in Programming Languages},
	author = {Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
	month = jul,
	year = {2017},
	note = {Publisher: Now Publishers, Inc.},
	pages = {1--119},
}

@misc{liventsevBFLanguageGeneralpurpose2022,
	title = {{BF}++: a language for general-purpose program synthesis},
	shorttitle = {{BF}++},
	url = {http://arxiv.org/abs/2101.09571},
	abstract = {Most state of the art decision systems based on Reinforcement Learning (RL) are data-driven black-box neural models, where it is often difficult to incorporate expert knowledge into the models or let experts review and validate the learned decision mechanisms. Knowledge-insertion and model review are important requirements in many applications involving human health and safety. One way to bridge the gap between data and knowledge driven systems is program synthesis: replacing a neural network that outputs decisions with a symbolic program generated by a neural network or by means of genetic programming. We propose a new programming language, BF++, designed specifically for automatic programming of agents in a Partially Observable Markov Decision Process (POMDP) setting and apply neural program synthesis to solve standard OpenAI Gym benchmarks.},
	language = {en},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Liventsev, Vadim and Härmä, Aki and Petković, Milan},
	month = jul,
	year = {2022},
	note = {arXiv:2101.09571 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.2},
}

@inproceedings{dhillonShapingHumanAICollaboration2024,
	title = {Shaping {Human}-{AI} {Collaboration}: {Varied} {Scaffolding} {Levels} in {Co}-writing with {Language} {Models}},
	author = {Dhillon, Paramveer S and Molaei, Somayeh and Li, Jiaqi and Golub, Maximilian and Zheng, Shaochun and Robert, Lionel Peter},
	year = {2024},
	pages = {1--18},
}

@article{choPropertiesNeuralMachine2014,
	title = {On the properties of neural machine translation: {Encoder}-decoder approaches},
	journal = {arXiv preprint arXiv:1409.1259},
	author = {Cho, Kyunghyun and Van Merriënboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	year = {2014},
}

@misc{beckXLSTMExtendedLong2024,
	title = {{xLSTM}: {Extended} {Long} {Short}-{Term} {Memory}},
	shorttitle = {{xLSTM}},
	url = {http://arxiv.org/abs/2405.04517},
	abstract = {In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.},
	language = {en},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Beck, Maximilian and Pöppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = may,
	year = {2024},
	note = {arXiv:2405.04517 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yuSpiderLargescaleHumanlabeled2018,
	title = {Spider: {A} large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task},
	journal = {arXiv preprint arXiv:1809.08887},
	author = {Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle},
	year = {2018},
}

@book{pattisKarelRobotGentle1994,
	title = {Karel the robot: a gentle introduction to the art of programming},
	isbn = {0-471-59725-2},
	publisher = {John Wiley \& Sons},
	author = {Pattis, Richard E.},
	year = {1994},
}

@article{liCanLlmAlready2024,
	title = {Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Geng, Ruiying and Huo, Nan},
	year = {2024},
}

@book{groffSQLCompleteReference2002,
	title = {{SQL}: the complete reference},
	volume = {2},
	publisher = {McGraw-Hill/Osborne},
	author = {Groff, James R. and Weinberg, Paul N. and Oppel, Andrew J.},
	year = {2002},
}

@misc{ShapingHumanAICollaboration,
	title = {Shaping {Human}-{AI} {Collaboration}: {Varied} {Scaffolding} {Levels} in {Co}-writing with {Language} {Models} {\textbar} {Proceedings} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	shorttitle = {Shaping {Human}-{AI} {Collaboration}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642134},
	language = {EN},
	urldate = {2024-06-04},
	journal = {ACM Conferences},
	doi = {10.1145/3613904.3642134},
	note = {Archive Location: world},
}

@misc{liventsevIntensiveCareOne2024,
	title = {Intensive {Care} as {One} {Big} {Sequence} {Modeling} {Problem}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2402.17501},
	doi = {10.48550/arXiv.2402.17501},
	abstract = {Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities.},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Liventsev, Vadim and Fritz, Tobias},
	month = feb,
	year = {2024},
	note = {arXiv:2402.17501 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{NewsFinnair,
	title = {News {\textbar} {Finnair}},
	url = {https://company.finnair.com/en/media-centre/all-releases/news},
	language = {en},
	urldate = {2024-04-29},
}

@article{neelakantanTextCodeEmbeddings2022a,
	title = {Text and code embeddings by contrastive pre-training},
	journal = {arXiv preprint arXiv:2201.10005},
	author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris},
	year = {2022},
}

@article{neelakantanTextCodeEmbeddings2022,
	title = {Text and code embeddings by contrastive pre-training},
	journal = {arXiv preprint arXiv:2201.10005},
	author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris},
	year = {2022},
}

@article{kosarComparingGeneralpurposeDomainspecific2010,
	title = {Comparing general-purpose and domain-specific languages: {An} empirical study},
	volume = {7},
	number = {2},
	journal = {Computer Science and Information Systems},
	author = {Kosar, Tomaž and Oliveira, Nuno and Mernik, Marjan and Pereira, Varanda João Maria and Črepinšek, Matej and Da, Cruz Daniela and Henriques, Rangel Pedro},
	year = {2010},
	pages = {247--264},
}

@article{karsaiDesignGuidelinesDomain2014,
	title = {Design guidelines for domain specific languages},
	journal = {arXiv preprint arXiv:1409.2378},
	author = {Karsai, Gabor and Krahn, Holger and Pinkernell, Claas and Rumpe, Bernhard and Schindler, Martin and Völkel, Steven},
	year = {2014},
}

@article{kosarDomainspecificLanguagesSystematic2016,
	title = {Domain-specific languages: {A} systematic mapping study},
	volume = {71},
	issn = {0950-5849},
	journal = {Information and Software Technology},
	author = {Kosar, Tomaž and Bohra, Sudev and Mernik, Marjan},
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {77--91},
}

@article{hudakDomainspecificLanguages1997,
	title = {Domain-specific languages},
	volume = {3},
	number = {39-60},
	journal = {Handbook of programming languages},
	author = {Hudak, Paul},
	year = {1997},
	pages = {21},
}

@article{mernikWhenHowDevelop2005,
	title = {When and how to develop domain-specific languages},
	volume = {37},
	issn = {0360-0300},
	number = {4},
	journal = {ACM computing surveys (CSUR)},
	author = {Mernik, Marjan and Heering, Jan and Sloane, Anthony M},
	year = {2005},
	note = {Publisher: ACM New York, NY, USA},
	pages = {316--344},
}

@book{fowlerDomainspecificLanguages2010,
	title = {Domain-specific languages},
	isbn = {0-13-139280-8},
	publisher = {Pearson Education},
	author = {Fowler, Martin},
	year = {2010},
}

@inproceedings{gorodniaiaStudyProgrammingParadigms2016,
	title = {Study of {Programming} {Paradigms}},
	isbn = {84-608-5617-8},
	booktitle = {{INTED2016} {Proceedings}},
	publisher = {IATED},
	author = {Gorodniaia, Lidia and Andreyeva, Tatiana},
	year = {2016},
	pages = {7482--7491},
}

@article{vanroyProgrammingParadigmsDummies2009,
	title = {Programming paradigms for dummies: {What} every programmer should know},
	volume = {104},
	journal = {New computational paradigms for computer music},
	author = {Van Roy, Peter},
	year = {2009},
	note = {Publisher: IRCAM/Delatour France},
	pages = {616--621},
}

@incollection{floydParadigmsProgramming2007,
	title = {The paradigms of programming},
	booktitle = {{ACM} {Turing} award lectures},
	author = {Floyd, Robert W.},
	year = {2007},
	pages = {1978},
}

@article{krishnamurthi13ProgrammingParadigms2019,
	title = {13 {Programming} {Paradigms} and {Beyond}},
	journal = {The Cambridge handbook of computing education research},
	author = {Krishnamurthi, Shriram and Fisler, Kathi},
	year = {2019},
}

@book{doetsLogicLogicProgramming1994,
	title = {From logic to logic programming},
	isbn = {0-262-04142-1},
	publisher = {Mit Press},
	author = {Doets, Kees},
	year = {1994},
}

@book{lloydFoundationsLogicProgramming2012,
	title = {Foundations of logic programming},
	isbn = {3-642-83189-3},
	publisher = {Springer Science \& Business Media},
	author = {Lloyd, John W.},
	year = {2012},
}

@incollection{clackPerformanceEnhancedGenetic1997,
	title = {Performance enhanced genetic programming},
	isbn = {978-3-540-62788-3},
	url = {http://dx.doi.org/10.1007/BFb0014803},
	booktitle = {Evolutionary {Programming} {VI}},
	publisher = {Springer Berlin Heidelberg},
	author = {Clack, Chris and Yu, Tina},
	year = {1997},
	doi = {10.1007/bfb0014803},
	pages = {85--100},
}

@article{collomApplyingGeneticProgramming2014,
	title = {Applying {Genetic} {Programming} to {Bytecode} and {Assembly}},
	volume = {1},
	issn = {2576-2176},
	url = {http://dx.doi.org/10.61366/2576-2176.1013},
	doi = {10.61366/2576-2176.1013},
	language = {en},
	number = {2},
	journal = {Scholarly Horizons: University of Minnesota, Morris Undergraduate Jour nal},
	author = {Collom, Eric},
	month = aug,
	year = {2014},
}

@incollection{kozaFutureWorkPractical,
	title = {Future work and practical applications of genetic programming},
	url = {http://dx.doi.org/10.1887/0750308958/B386C126},
	booktitle = {Handbook of {Evolutionary} {Computation}},
	publisher = {IOP Publishing Ltd},
	author = {Koza, John R},
	doi = {10.1887/0750308958/b386c126},
}

@inproceedings{willisGeneticProgrammingIntroduction1997,
	title = {Genetic programming: an introduction and survey of applications},
	url = {http://dx.doi.org/10.1049/CP:19971199},
	doi = {10.1049/cp:19971199},
	booktitle = {Second {International} {Conference} on {Genetic} {Algorithms} in {Engineering} {S} ystems},
	publisher = {IEE},
	author = {Willis, M.-J.},
	year = {1997},
}

@article{d.whiteGeneticProgrammingLowresource2009,
	title = {Genetic programming for low-resource systems},
	author = {{D. White}},
	year = {2009},
}

@incollection{degarisGENETICPROGRAMMING1990,
	title = {{GENETIC} {PROGRAMMING}},
	isbn = {978-1-55860-141-3},
	url = {http://dx.doi.org/10.1016/b978-1-55860-141-3.50019-5},
	booktitle = {Machine {Learning} {Proceedings} 1990},
	publisher = {Elsevier},
	author = {de Garis, Hugo},
	year = {1990},
	doi = {10.1016/b978-1-55860-141-3.50019-5},
	pages = {132--139},
}

@incollection{kozaHumancompetitiveApplicationsGenetic2003,
	title = {Human-competitive {Applications} of {Genetic} {Programming}},
	isbn = {978-3-642-62386-8},
	url = {http://dx.doi.org/10.1007/978-3-642-18965-4_26},
	booktitle = {Natural {Computing} {Series}},
	publisher = {Springer Berlin Heidelberg},
	author = {Koza, John R.},
	year = {2003},
	doi = {10.1007/978-3-642-18965-4_26},
	pages = {663--682},
}

@article{dracopoulosGeneticProgrammingPrediction1997,
	title = {Genetic programming for prediction and control},
	volume = {6},
	issn = {0941-0643},
	url = {http://dx.doi.org/10.1007/BF01501508},
	doi = {10.1007/bf01501508},
	language = {en},
	number = {4},
	journal = {Neural Computing \& Applications},
	author = {Dracopoulos, D. C. and Kent, S.},
	month = dec,
	year = {1997},
	pages = {214--228},
}

@misc{alshahwanAutomatedUnitTest2024,
	title = {Automated {Unit} {Test} {Improvement} using {Large} {Language} {Models} at {Meta}},
	url = {http://arxiv.org/abs/2402.09171},
	doi = {10.48550/arXiv.2402.09171},
	abstract = {This paper describes Meta's TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75\% of TestGen-LLM's test cases built correctly, 57\% passed reliably, and 25\% increased coverage. During Meta's Instagram and Facebook test-a-thons, it improved 11.5\% of all classes to which it was applied, with 73\% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Alshahwan, Nadia and Chheda, Jubin and Finegenova, Anastasia and Gokkaya, Beliz and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09171 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{blondelElementsDifferentiableProgramming2024,
	title = {The {Elements} of {Differentiable} {Programming}},
	url = {http://arxiv.org/abs/2403.14606},
	doi = {10.48550/arXiv.2403.14606},
	abstract = {Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Blondel, Mathieu and Roulet, Vincent},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14606 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@article{liventsevEffectivePatientSimulators2021,
	title = {Towards {Effective} {Patient} {Simulators}},
	volume = {4},
	copyright = {All rights reserved},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2021.798659},
	abstract = {In this paper we give an overview of the field of patient simulators and provide qualitative and quantitative comparison of different modeling and simulation approaches. Simulators can be used to train human caregivers but also to develop and optimize algorithms for clinical decision support applications and test and validate interventions. In this paper we introduce three novel patient simulators with different levels of representational accuracy: HeartPole, a simplistic transparent rule-based system, GraphSim, a graph-based model trained on intensive care data, and Auto-ALS—an adjusted version of an educational software package used for training junior healthcare professionals. We provide a qualitative and quantitative comparison of the previously existing as well as proposed simulators.},
	urldate = {2023-06-24},
	journal = {Frontiers in Artificial Intelligence},
	author = {Liventsev, Vadim and Härmä, Aki and Petković, Milan},
	year = {2021},
}

@article{solomonInferenceTransparencyEffective2021b,
	title = {The {Inference} of {Transparency} in {Effective} {Team} {Management} {In} the {IT} {Industry}},
	author = {Solomon, Daniel},
	month = aug,
	year = {2021},
	note = {Publisher: SAGE Publications},
}

@article{solomonInferenceTransparencyEffective2021,
	title = {The {Inference} of {Transparency} in {Effective} {Team} {Management} {In} the {IT} {I} ndustry},
	url = {http://dx.doi.org/10.31124/advance.15021138.v1},
	doi = {10.31124/advance.15021138.v1},
	abstract = {{\textless}jats:p{\textgreater}\&lt;div\&gt;A transparent working environment has been observed to have a momentous impact in making teams more effective, happy, and creative (Scholl, 2019). It helps develop consistent communication, a pparent and authentic workplaces which help the team to feel secure an d proposes ideas enhancing inventiveness (Lencion, 2016). This study e xamines if there was a significant relationship between a transparent work environment and a sort of creative, effective, and productive fun ctional team in the IT industry that delivers the job in time and with in budget without compromising quality. To this end, a questionnaire h as been remitted to such a successful team of IT professionals working under various capacities for a software developing company within the IT Industry. GPower and Jamovi have been used to determine the sample size population and analyze the data gathered thru a questionnaire to fetch the purpose of the study. The finding exhibits that there is a significant relationship between the transparent work environment and the creativity, accountability, and productivity.\&lt;br\&gt;\&lt;/div\&gt ;{\textless}/jats:p{\textgreater}},
	author = {Solomon, Daniel},
	month = aug,
	year = {2021},
}

@article{everittRewardTamperingProblems2021,
	title = {Reward tampering problems and solutions in reinforcement learning: {A} causal influence diagram perspective},
	volume = {198},
	number = {Suppl 27},
	journal = {Synthese},
	author = {Everitt, Tom and Hutter, Marcus and Kumar, Ramana and Krakovna, Victoria},
	year = {2021},
	note = {ISBN: 0039-7857
Publisher: Springer},
	pages = {6435--6467},
}

@inproceedings{pirl,
	address = {Stockholmsmässan, Stockholm Sweden},
	series = {Proceedings of machine learning research},
	title = {Programmatically interpretable reinforcement learning},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/verma18a.html},
	abstract = {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.},
	booktitle = {Proceedings of the 35th international conference on machine learning},
	publisher = {PMLR},
	author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {5045--5054},
}

@book{genprogast,
	title = {Genetic programming: on the programming of computers by means of natural selection},
	volume = {1},
	publisher = {MIT press},
	author = {Koza, John R},
	year = {1992},
}

@article{liventsev2021neurogenetic,
	title = {Neurogenetic programming framework for explainable reinforcement learning},
	copyright = {All rights reserved},
	journal = {arXiv preprint arXiv:2102.04231},
	author = {Liventsev, Vadim and Härmä, Aki and Petković, Milan},
	year = {2021},
}

@article{panganibanTransparencyAutonomousTeammates2020,
	title = {Transparency in autonomous teammates: intention to support as teaming information},
	volume = {14},
	number = {2},
	journal = {Journal of Cognitive Engineering and Decision Making},
	author = {Panganiban, April Rose and Matthews, Gerald and Long, Michael D.},
	year = {2020},
	note = {ISBN: 1555-3434
Publisher: Sage Publications Sage CA: Los Angeles, CA},
	pages = {174--190},
}

@book{genprog2,
	title = {A field guide to genetic programming},
	publisher = {Lulu. com},
	author = {Poli, Riccardo and Langdon, William B and McPhee, Nicholas F and Koza, John R},
	year = {2008},
}

@book{genprog1,
	title = {Genetic programming: an introduction},
	volume = {1},
	publisher = {Morgan Kaufmann Publishers San Francisco},
	author = {Banzhaf, Wolfgang and Nordin, Peter and Keller, Robert E and Francone, Frank D},
	year = {1998},
}

@article{gouesAutomatedProgramRepair2019,
	title = {Automated program repair},
	volume = {62},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3318162},
	doi = {10.1145/3318162},
	abstract = {Automated program repair can relieve programmers from the burden of manually fixing the ever-increasing number of programming mistakes.},
	number = {12},
	urldate = {2023-12-14},
	journal = {Communications of the ACM},
	author = {Goues, Claire Le and Pradel, Michael and Roychoudhury, Abhik},
	month = nov,
	year = {2019},
	pages = {56--65},
}

@article{openai-gym,
	title = {Openai gym},
	journal = {arXiv preprint arXiv:1606.01540},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	year = {2016},
}

@inproceedings{decoupling-rl,
	series = {Proceedings of machine learning research},
	title = {Decoupling representation learning from reinforcement learning},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/stooke21a.html},
	abstract = {In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at {\textless}a href="https://github.com/astooke/rlpyt/tree/master/rlpyt/ul"{\textgreater}https://github.com/astooke/rlpyt/tree/master/rlpyt/ul{\textless}/a{\textgreater}.},
	booktitle = {Proceedings of the 38th international conference on machine learning},
	publisher = {PMLR},
	author = {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	pages = {9870--9879},
}

@incollection{health-ai-explainability2,
	title = {Explainable {AI} for healthcare: from black box to interpretable models},
	booktitle = {Embedded systems and artificial intelligence},
	publisher = {Springer},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2020},
	pages = {327--337},
}

@article{health-ai-explainability1,
	title = {Explainability for artificial intelligence in healthcare: a multidisciplinary perspective},
	volume = {20},
	number = {1},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I},
	year = {2020},
	note = {Publisher: Springer},
	pages = {1--9},
}

@misc{spinal-guidelines,
	title = {Spinal sonography and applications of ultrasound for central neuraxial blocks},
	publisher = {New York, NY: McGraw-Hill Education},
	author = {Karmakar, Manoj K and Chin, Ki Jinn},
	year = {2017},
}

@article{isoug-guidelines,
	title = {Practice guidelines for performance of the routine mid-trimester fetal ultrasound scan},
	volume = {37},
	number = {1},
	journal = {Ultrasound in Obstetrics \& Gynecology},
	author = {Salomon, Laurent Julien and Alfirevic, Z and Berghella, V and Bilardo, C and Hernandez-Andrade, E and Johnsen, SL and Kalache, K and Leung, K-Y and Malinger, G and Munoz, H and {others}},
	year = {2011},
	note = {Publisher: John Wiley \& Sons, Ltd. Chichester, UK},
	pages = {116--126},
}

@article{sim2real,
	title = {{Sim2Real} predictivity: {Does} evaluation in simulation predict real-world performance?},
	volume = {5},
	doi = {10.1109/LRA.2020.3013848},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kadian, Abhishek and Truong, Joanne and Gokaslan, Aaron and Clegg, Alexander and Wijmans, Erik and Lee, Stefan and Savva, Manolis and Chernova, Sonia and Batra, Dhruv},
	year = {2020},
	pages = {6670--6677},
}

@article{commoncrawl,
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	journal = {arXiv preprint arXiv:1910.10683},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	year = {2019},
}

@misc{datashortage,
	title = {Will we ever solve the {Shortage} of {Data} in {Medical} {Applications}?},
	url = {https://towardsdatascience.com/will-we-ever-solve-the-shortage-of-data-in-medical-applications-70da163e2c2d},
	author = {Maier, Andreas},
	month = apr,
	year = {2020},
}

@article{obstetrics-sonography,
	title = {Ultrasonography in obstetrics and gynecology},
	author = {Callen, Peter W},
	year = {1988},
	note = {Publisher: WB Saunders CBS Educ. and Professional Publ., New York, NY},
}

@article{autonomous-ultrasound-review,
	title = {An overview of systems and techniques for autonomous robotic ultrasound acquisitions},
	volume = {3},
	doi = {10.1109/TMRB.2021.3072190},
	number = {2},
	journal = {IEEE Transactions on Medical Robotics and Bionics},
	author = {Li, Keyu and Xu, Yangxin and Meng, Max Q.-H.},
	year = {2021},
	pages = {510--524},
}

@article{sonorl,
	title = {Autonomous navigation of an ultrasound probe towards standard scan planes with deep reinforcement learning},
	volume = {abs/2103.00718},
	url = {https://arxiv.org/abs/2103.00718},
	journal = {CoRR},
	author = {Li, Keyu and Wang, Jian and Xu, Yangxin and Qin, Hao and Liu, Dongsheng and Liu, Li and Meng, Max Q.-H.},
	year = {2021},
	note = {arXiv: 2103.00718
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Wed, 16 Feb 2022 17:43:37 +0100},
}

@article{mannaDeductiveApproachProgram1980,
	title = {A {Deductive} {Approach} to {Program} {Synthesis}},
	volume = {2},
	issn = {0164-0925, 1558-4593},
	url = {https://dl.acm.org/doi/10.1145/357084.357090},
	doi = {10.1145/357084.357090},
	abstract = {Program synthesis is the systematic derivation of a program from a given specification. A deductive approach to program synthesis is presented for the construction of recursive programs. This approach regards program synthesis as a theorem-proving task and relies on a theorem-proving method that combines the features of transformation rules, unification, and mathematical induction within a single framework.},
	language = {en},
	number = {1},
	urldate = {2024-03-15},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Manna, Zohar and Waldinger, Richard},
	month = jan,
	year = {1980},
	pages = {90--121},
}

@article{weerarathnaHumanRobotCollaborationHealthcare2023,
	title = {Human-{Robot} {Collaboration} for {Healthcare}: {A} {Narrative} {Review}},
	issn = {2168-8184},
	shorttitle = {Human-{Robot} {Collaboration} for {Healthcare}},
	url = {https://www.cureus.com/articles/206887-human-robot-collaboration-for-healthcare-a-narrative-review},
	doi = {10.7759/cureus.49210},
	abstract = {Robotic applications have often quickly transitioned from industrial to social. Because of this, robots can now engage with people in a natural way and blend in with their surroundings. Due to the lack of medical professionals, growing healthcare costs, and the exponential rise in the population of vulnerable groups like the ill, elderly, and children with developmental disabilities, the use of social robots in the healthcare system is expanding. As a result, social robots are employed in the medical field to entertain and educate hospitalized patients about health issues, as well as to assist the elderly and sick. They are also employed in the dispensing of medications, rehabilitation, and emotional and geriatric care. Thus, social robots raise the standard and effectiveness of medical care. This article explains how patients and healthcare professionals collaborate with robots in the healthcare industry. The objectives of this collaboration are to resolve moral and legal concerns, improve patient outcomes, and improve healthcare delivery. It has a broad range of uses, including telemedicine, rehabilitation, and robotic surgical support. Human-robot interaction is the term used to describe interactions between social robots and people. Many obstacles stand in the way of human-robot interaction in healthcare, including safety concerns, acceptability issues, appropriateness, usefulness, and the worry that robots may replace human carers. In the end, these difficulties result in a poor adoption rate for robotic technology. As a result, the applications and difficulties of human-robot interaction in healthcare are thoroughly evaluated in this research. This study also reviews future safety prospects from human-robot interaction in healthcare, as well as ethical and usability issues including privacy, trust, and safety, and our aims to provide a comprehensive overview of the use of robots in healthcare, including their applications, benefits, challenges, and prospects, to facilitate a deeper understanding of this evolving field.},
	language = {en},
	urldate = {2024-03-15},
	journal = {Cureus},
	author = {Weerarathna, Induni N and Raymond, David and Luharia, Anurag},
	month = nov,
	year = {2023},
}

@article{castilloConsiderationsSuccessfulClinical2013,
	title = {Considerations for a {Successful} {Clinical} {Decision} {Support} {System}},
	volume = {31},
	issn = {1538-2931},
	url = {http://dx.doi.org/10.1097/NXN.0b013e3182997a9c},
	doi = {10.1097/nxn.0b013e3182997a9c},
	language = {en},
	number = {7},
	journal = {CIN: Computers, Informatics, Nursing},
	author = {CASTILLO, RANIELLE S. and KELEMEN, ARPAD},
	month = jul,
	year = {2013},
	pages = {319--326},
}

@article{stangeThisIssueClinical2011,
	title = {In {This} {Issue}: {Clinical} {Decision} {Support}},
	volume = {9},
	issn = {1544-1709},
	url = {http://dx.doi.org/10.1370/AFM.1215},
	doi = {10.1370/afm.1215},
	language = {en},
	number = {1},
	journal = {The Annals of Family Medicine},
	author = {Stange, K. C.},
	month = jan,
	year = {2011},
	pages = {2--2},
}

@article{osullivanDecisionTimeClinical2014,
	title = {Decision time for clinical decision support systems},
	volume = {14},
	issn = {1470-2118},
	url = {http://dx.doi.org/10.7861/clinmedicine.14-4-338},
	doi = {10.7861/clinmedicine.14-4-338},
	language = {en},
	number = {4},
	journal = {Clinical Medicine},
	author = {O’Sullivan, Dympna and Fraccaro, Paolo and Carson, Ewart and Weller, Peter},
	month = aug,
	year = {2014},
	pages = {338--341},
}

@article{purcellWhatMakesGood2005,
	title = {What makes a good clinical decision support system},
	volume = {330},
	issn = {0959-8138},
	url = {http://dx.doi.org/10.1136/bmj.330.7494.740},
	doi = {10.1136/bmj.330.7494.740},
	language = {en},
	number = {7494},
	journal = {BMJ},
	author = {Purcell, Gretchen P},
	month = mar,
	year = {2005},
	pages = {740--741},
}

@article{selfClinicalDecisionSupport2008,
	title = {Clinical {Decision} {Support} {Tool}: {A} rational needs-based approach to mak ing clinical decisions},
	volume = {17},
	issn = {0963-8237},
	url = {http://dx.doi.org/10.1080/09638230701505806},
	doi = {10.1080/09638230701505806},
	language = {en},
	number = {1},
	journal = {Journal of Mental Health},
	author = {Self, Roland and Rigby, Anna and Leggett, Clive and Paxton, Roger},
	month = jan,
	year = {2008},
	pages = {33--48},
}

@article{wrightClinicalDecisionSupport2009,
	title = {Clinical {Decision} {Support} {Capabilities} of {Commercially}-available {Clini} cal {Information} {Systems}},
	volume = {16},
	issn = {1067-5027},
	url = {http://dx.doi.org/10.1197/jamia.M3111},
	doi = {10.1197/jamia.m3111},
	language = {en},
	number = {5},
	journal = {Journal of the American Medical Informatics Association},
	author = {Wright, A. and Sittig, D. F. and Ash, J. S. and Sharma, S. and Pang, J. E. and Middleton, B.},
	month = sep,
	year = {2009},
	pages = {637--644},
}

@article{mooreIntroductionClinicalDecision2011,
	title = {An {Introduction} to {Clinical} {Decision} {Support} {Systems}},
	volume = {8},
	issn = {1542-4065},
	url = {http://dx.doi.org/10.1080/15424065.2011.626345},
	doi = {10.1080/15424065.2011.626345},
	language = {en},
	number = {4},
	journal = {Journal of Electronic Resources in Medical Libraries},
	author = {Moore, Mary and Loper, Kimberly A.},
	month = oct,
	year = {2011},
	pages = {348--366},
}

@article{sakallarisClinicalDecisionSupport2000,
	title = {Clinical {Decision} {Support} {Systems} for {Outcome} {Measurement} and {Manageme} nt},
	volume = {11},
	issn = {1079-0713},
	url = {http://dx.doi.org/10.1097/00044067-200008000-00003},
	doi = {10.1097/00044067-200008000-00003},
	language = {en},
	number = {3},
	journal = {AACN Clinical Issues: Advanced Practice in Acute \& Critical Care},
	author = {Sakallaris, Bonnie R. and Jastremski, Connie A. and Von Rueden, Kathryn T.},
	month = aug,
	year = {2000},
	pages = {351--362},
}

@techreport{chidesterPersonalityFactorsFlight1990,
	title = {Personality factors in flight operations. {Volume} 1: {Leader} characteristics and crew performance in a full-mission air transport simulation},
	author = {Chidester, Thomas R. and Kanki, Barbara G. and Foushee, H. Clayton and Dickinson, Cortlandt L. and Bowles, Stephen V.},
	year = {1990},
}

@article{chidesterPilotPersonalityCrew1991,
	title = {Pilot personality and crew coordination: {Implications} for training and selection},
	volume = {1},
	number = {1},
	journal = {The International Journal of Aviation Psychology},
	author = {Chidester, Thomas R. and Helmreich, Robert L. and Gregorich, Steven E. and Geis, Craig E.},
	year = {1991},
	note = {ISBN: 1050-8414
Publisher: Taylor \& Francis},
	pages = {25--44},
}

@techreport{smithSimulatorStudyInteraction1979,
	title = {A simulator study of the interaction of pilot workload with errors, vigilance, and decisions},
	author = {Smith, HP Ruffell},
	year = {1979},
}

@inproceedings{shivelyCrewResourceManagement2018,
	address = {Cham},
	title = {Crew {Resource} {Management} for {Automated} {Teammates} ({CRM}-{A})},
	isbn = {978-3-319-91122-9},
	doi = {10.1007/978-3-319-91122-9_19},
	abstract = {Crew Resource Management (CRM) is the application of human factors knowledge and skills to ensure that teams make effective use of all resources. This includes ensuring that pilots bring in opinions of other teammates and utilize their unique capabilities. CRM was originally developed 40 years ago in response to a number of airline accidents in which the crew was found to be at fault. The goal was to improve teamwork among airline cockpit crews. The notion of “team” was later expanded to include cabin crew and ground resources. CRM has also been adopted by other industries, most notably medicine. Automation research now finds itself faced with similar issues to those faced by aviation 40 years ago: how to create a more robust system by making full use of both the automation and its human operators. With advances in machine intelligence, processing speed and cheap and plentiful memory, automation has advanced to the point that it can and should be treated as a teammate to fully take advantage of its capabilities and contributions to the system. This area of research is known as Human-Autonomy Teaming (HAT). Research on HAT has identified reusable patterns that can be applied in a wide range of applications. These patterns include features such as bi-directional communication and working agreements. This paper will explore the synergies between CRM and HAT. We believe that HAT research has much to learn from CRM and that there are benefits to expanding CRM to cover automation.},
	language = {en},
	booktitle = {Engineering {Psychology} and {Cognitive} {Ergonomics}},
	publisher = {Springer International Publishing},
	author = {Shively, Robert J. and Lachter, Joel and Koteskey, Robert and Brandt, Summer L.},
	editor = {Harris, Don},
	year = {2018},
	keywords = {Automation, Crew Resource Management (CRM), Human-Autonomy Teaming (HAT)},
	pages = {215--229},
}

@article{gegoffTransparentAutomatedAdvice2023,
	title = {Transparent {Automated} {Advice} to {Mitigate} the {Impact} of {Variation} in {Au} tomation {Reliability}},
	issn = {0018-7208},
	url = {http://dx.doi.org/10.1177/00187208231196738},
	doi = {10.1177/00187208231196738},
	abstract = {{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Objective{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} To examine the ex tent to which increased automation transparency can mitigate the poten tial negative effects of low and high automation reliability on disuse and misuse of automated advice, and perceived trust in automation. {\textless}/ jats:p{\textgreater}{\textless}/jats:sec{\textgreater}{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Background{\textless}/jats:title{\textgreater}{\textless}jats:p {\textgreater} Automated decision aids that vary in the reliability of their advice are increasingly used in workplaces. Low-reliability automation can i ncrease disuse of automated advice, while high-reliability automation can increase misuse. These effects could be reduced if the rationale u nderlying automated advice is made more transparent. {\textless}/jats:p{\textgreater}{\textless}/jats:s ec{\textgreater}{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Methods{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} Participants sel ected the optimal UV to complete missions. The Recommender (automated decision aid) assisted participants by providing advice; however, it w as not always reliable. Participants determined whether the Recommende r provided accurate information and whether to accept or reject advice . The level of automation transparency (medium, high) and reliability (low: 65\%, high: 90\%) were manipulated between-subjects. {\textless}/jats:p{\textgreater}{\textless}/ja ts:sec{\textgreater}{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Results{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} With high- c ompared to low-reliability automation, participants made more accurate (correctly accepted advice and identified whether information was acc urate/inaccurate) and faster decisions, and reported increased trust i n automation. Increased transparency led to more accurate and faster d ecisions, lower subjective workload, and higher usability ratings. It also eliminated the increased automation disuse associated with low-re liability automation. However, transparency did not mitigate the misus e associated with high-reliability automation. {\textless}/jats:p{\textgreater}{\textless}/jats:sec{\textgreater}{\textless}ja ts:sec{\textgreater}{\textless}jats:title{\textgreater}Conclusion{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} Transparency protec ted against low-reliability automation disuse, but not against the inc reased misuse potentially associated with the reduced monitoring and v erification of high-reliability automation. {\textless}/jats:p{\textgreater}{\textless}/jats:sec{\textgreater}{\textless}jats: sec{\textgreater}{\textless}jats:title{\textgreater}Application{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} These outcomes can in form the design of transparent automation to improve human-automation teaming under conditions of varied automation reliability. {\textless}/jats:p{\textgreater}{\textless}/ jats:sec{\textgreater}},
	language = {en},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	author = {Gegoff, Isabella and Tatasciore, Monica and Bowden, Vanessa and McCarley, Jason and Loft, Shayne},
	month = aug,
	year = {2023},
}

@article{eyalwinterTransparencyPeersIncentives2006,
	title = {Transparency among {Peers} and {Incentives} ∗},
	author = {{Eyal Winter}},
	year = {2006},
}

@article{winterTransparencyIncentivesPeers2010,
	title = {Transparency and incentives among peers},
	volume = {41},
	issn = {0741-6261},
	url = {http://dx.doi.org/10.1111/J.1756-2171.2010.00109.X},
	doi = {10.1111/j.1756-2171.2010.00109.x},
	language = {en},
	number = {3},
	journal = {The RAND Journal of Economics},
	author = {Winter, Eyal},
	month = aug,
	year = {2010},
	pages = {504--523},
}

@article{m.agastyaTransparencyPeerActivities2009,
	title = {Transparency of peer activities confounds cheap talk in joint projects .},
	author = {{M. Agastya}},
	year = {2009},
}

@article{kantibagDoubleedgedTransparencyTeams2011,
	title = {Double-edged transparency in teams {Parimal}},
	author = {{Kanti Bag} and {Nona Pepito}},
	year = {2011},
}

@article{p.bagHarmfulTransparencyTeams2019,
	title = {Harmful transparency in teams {Kanti}},
	author = {{P. Bag} and {Nona Pepito}},
	year = {2019},
}

@article{drouvelisShouldTransparencyBe2021,
	title = {Should {Transparency} {Be} ({In}-){Transparent}? {On} {Monitoring} {Aversion} and {Co} operation in {Teams}},
	issn = {1556-5068},
	url = {http://dx.doi.org/10.2139/ssrn.3992582},
	doi = {10.2139/ssrn.3992582},
	language = {en},
	journal = {SSRN Electronic Journal},
	author = {Drouvelis, Michalis and Jarke-Neuert, Johannes and Lohse, Johannes},
	year = {2021},
}

@article{solomonInferenceTransparencyEffective2021a,
	title = {The {Inference} of {Transparency} in {Effective} {Team} {Management} {In} the {IT} {I} ndustry},
	url = {http://dx.doi.org/10.31124/advance.15021138},
	doi = {10.31124/advance.15021138},
	abstract = {{\textless}jats:p{\textgreater}\&lt;div\&gt;A transparent working environment has been observed to have a momentous impact in making teams more effective, happy, and creative (Scholl, 2019). It helps develop consistent communication, a pparent and authentic workplaces which help the team to feel secure an d proposes ideas enhancing inventiveness (Lencion, 2016). This study e xamines if there was a significant relationship between a transparent work environment and a sort of creative, effective, and productive fun ctional team in the IT industry that delivers the job in time and with in budget without compromising quality. To this end, a questionnaire h as been remitted to such a successful team of IT professionals working under various capacities for a software developing company within the IT Industry. GPower and Jamovi have been used to determine the sample size population and analyze the data gathered thru a questionnaire to fetch the purpose of the study. The finding exhibits that there is a significant relationship between the transparent work environment and the creativity, accountability, and productivity.\&lt;br\&gt;\&lt;/div\&gt ;{\textless}/jats:p{\textgreater}},
	author = {Solomon, Daniel},
	month = aug,
	year = {2021},
}

@article{sein-echaluceImpactTransparencyTeamwork2021,
	title = {Impact of {Transparency} in the {Teamwork} {Development} through {Cloud} {Compu} ting},
	volume = {11},
	issn = {2076-3417},
	url = {http://dx.doi.org/10.3390/APP11093887},
	doi = {10.3390/app11093887},
	abstract = {{\textless}jats:p{\textgreater}Active educational methodologies promote students to take an a ctive role in their own learning, enhance cooperative work, and develo p a collective understanding of the subject as a common learning area. Cloud Computing enables the learning space to be supported while also revolutionizing it by allowing it to be used as a link between active methodology and students’ learning activities. A Cloud Computing syst em is used in conjunction with an active methodology to recognize and manage individual, group, and collective evidence of the students’ wor k in this research. The key hypothesis shown in this work is that if e vidence management is made clear and evidence is consistently and grad ually presented to students, their level of involvement will increase, and their learning outcomes will improve. The model was implemented i n a university subject of a first academic year using the active Flipp ed Classroom methodology, and the individual, group and collective evi dence is constantly worked with throughout the implementation of a tea mwork method.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {9},
	journal = {Applied Sciences},
	author = {Sein-Echaluce, María Luisa and Fidalgo-Blanco, Angel and García-Peñalvo, Francisco José and Fonseca, David},
	month = apr,
	year = {2021},
	pages = {3887},
}

@article{Sein2021Impact,
	title = {Impact of {Transparency} in the {Teamwork} {Development} through {Cloud} {Computing}},
	volume = {11},
	number = {9},
	journal = {Applied Sciences},
	author = {Sein-Echaluce, María Luisa and Fidalgo-Blanco, Angel and García-Peñalvo, Francisco José and Fonseca, David},
	month = apr,
	year = {2021},
	note = {Publisher: MDPI AG},
	pages = {3887},
}

@article{Drouvelis2021Should,
	title = {Should {Transparency} {Be} ({In}-){Transparent}? {On} {Monitoring} {Aversion} and {Cooperation} in {Teams}},
	journal = {SSRN Electronic Journal},
	author = {Drouvelis, Michalis and Jarke-Neuert, Johannes and Lohse, Johannes},
	year = {2021},
	note = {Publisher: Elsevier BV},
}

@inproceedings{ososkyDeterminantsSystemTransparency2014,
	title = {Determinants of system transparency and its influence on trust in and reliance on unmanned robotic systems},
	url = {http://dx.doi.org/10.1117/12.2050622},
	doi = {10.1117/12.2050622},
	booktitle = {{SPIE} {Proceedings}},
	publisher = {SPIE},
	author = {Ososky, Scott and Sanders, Tracy and Jentsch, Florian and Hancock, Peter and Chen, Jessie Y. C.},
	editor = {Karlsen, Robert E. and Gage, Douglas W. and Shoemaker, Charles M. and Gerhart, Grant R.},
	month = jun,
	year = {2014},
}

@article{guznovRobotTransparencyTeam2019,
	title = {Robot {Transparency} and {Team} {Orientation} {Effects} on {Human}–{Robot} {Teaming}},
	volume = {36},
	issn = {1044-7318},
	url = {http://dx.doi.org/10.1080/10447318.2019.1676519},
	doi = {10.1080/10447318.2019.1676519},
	language = {en},
	number = {7},
	journal = {International Journal of Human–Computer Interaction},
	author = {Guznov, S. and Lyons, J. and Pfahler, M. and Heironimus, A. and Woolley, M. and Friedman, J. and Neimeier, A.},
	month = oct,
	year = {2019},
	pages = {650--660},
}

@article{ezenyilimbaImpactTransparencyExplanations2022,
	title = {Impact of {Transparency} and {Explanations} on {Trust} and {Situation} {Awarene} ss in {Human}–{Robot} {Teams}},
	volume = {17},
	issn = {1555-3434},
	url = {http://dx.doi.org/10.1177/15553434221136358},
	doi = {10.1177/15553434221136358},
	abstract = {{\textless}jats:p{\textgreater} Urban Search and Rescue (USAR) missions continue to benefit f rom the incorporation of human–robot teams (HRTs). USAR environments c an be ambiguous, hazardous, and unstable. The integration of robot tea mmates into USAR missions has enabled human teammates to access areas of uncertainty, including hazardous locations. For HRTs to be effectiv e, it is pertinent to understand the factors that influence team effec tiveness, such as having shared goals, mutual understanding, and effic ient communication. The purpose of our research is to determine how to (1) better establish human trust, (2) identify useful levels of robot transparency and robot explanations, (3) ensure situation awareness, and (4) encourage a bipartisan role amongst teammates. By implementing robot transparency and robot explanations, we found that the driving factors for effective HRTs rely on robot explanations that are context -driven and are readily available to the human teammate. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Journal of Cognitive Engineering and Decision Making},
	author = {Ezenyilimba, Akuadasuo and Wong, Margaret and Hehr, Alexander and Demir, Mustafa and Wolff, Alexandra and Chiou, Erin and Cooke, Nancy},
	month = nov,
	year = {2022},
	pages = {75--93},
}

@article{mercadoIntelligentAgentTransparency2016,
	title = {Intelligent {Agent} {Transparency} in {Human}–{Agent} {Teaming} for {Multi}-{UxV} {Ma} nagement},
	volume = {58},
	issn = {0018-7208},
	url = {http://dx.doi.org/10.1177/0018720815621206},
	doi = {10.1177/0018720815621206},
	abstract = {{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Objective:{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} We investigated the effects of level of agent transparency on operator performance, tr ust, and workload in a context of human–agent teaming for multirobot m anagement. {\textless}/jats:p{\textgreater}{\textless}/jats:sec{\textgreater}{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Background:{\textless}/jats :title{\textgreater}{\textless}jats:p{\textgreater} Participants played the role of a heterogeneous unmann ed vehicle (UxV) operator and were instructed to complete various miss ions by giving orders to UxVs through a computer interface. An intelli gent agent (IA) assisted the participant by recommending two plans—a t op recommendation and a secondary recommendation—for every mission. {\textless}/ jats:p{\textgreater}{\textless}/jats:sec{\textgreater}{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Method:{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} A within-subjects design with three levels of agent transparency was em ployed in the present experiment. There were eight missions in each of three experimental blocks, grouped by level of transparency. During e ach experimental block, the IA was incorrect three out of eight times due to external information (e.g., commander’s intent and intelligence ). Operator performance, trust, workload, and usability data were coll ected. {\textless}/jats:p{\textgreater}{\textless}/jats:sec{\textgreater}{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Results:{\textless}/jats:title{\textgreater} {\textless}jats:p{\textgreater} Results indicate that operator performance, trust, and percei ved usability increased as a function of transparency level. Subjectiv e and objective workload data indicate that participants’ workload did not increase as a function of transparency. Furthermore, response tim e did not increase as a function of transparency. {\textless}/jats:p{\textgreater}{\textless}/jats:sec{\textgreater} {\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Conclusion:{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} Unlike previous research, which showed that increased transparency resulted in increa sed performance and trust calibration at the cost of greater workload and longer response time, our results support the benefits of transpar ency for performance effectiveness without additional costs. {\textless}/jats:p{\textgreater} {\textless}/jats:sec{\textgreater}{\textless}jats:sec{\textgreater}{\textless}jats:title{\textgreater}Application:{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater} The current results will facilitate the implementation of IAs in military settings and will provide useful data to the design of heterogeneous UxV teams. {\textless}/jats:p{\textgreater}{\textless}/jats:sec{\textgreater}},
	language = {en},
	number = {3},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	author = {Mercado, Joseph E. and Rupp, Michael A. and Chen, Jessie Y. C. and Barnes, Michael J. and Barber, Daniel and Procci, Katelyn},
	month = feb,
	year = {2016},
	pages = {401--415},
}

@article{holderDesigningBiDirectionalTransparency2021,
	title = {Designing for {Bi}-{Directional} {Transparency} in {Human}-{AI}-{Robot}-{Teaming}},
	volume = {65},
	issn = {2169-5067},
	url = {http://dx.doi.org/10.1177/1071181321651052},
	doi = {10.1177/1071181321651052},
	abstract = {{\textless}jats:p{\textgreater} This paper takes a practitioner’s perspective on advancing bi -directional transparency in human-AI-robot teams (HARTs). Bi-directio nal transparency is important for HARTs because the better that people and artificially intelligent agents can understand one another’s capa bilities, limits, inputs, outputs and contexts in a given task environ ment; the better they can work as a team to accomplish shared goals, i nterdependent tasks, and overall missions. This understanding can be b uilt, augmented, broken and repaired at various stages across the tech nology life cycle, including the conceptual design; iterative design o f software, hardware and interfaces; marketing and sales; system train ing; operational use; and system updating and adaptation stages. This paper provides an overview of some best practices and challenges in bu ilding this bi-directional transparency at different points in the tec hnology life cycle of human-AI-robot systems. The goal is to help adva nce a wider discussion and sharing of lessons learned from recent work in this area. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Holder, Eric and Huang, Lixiao and Chiou, Erin and Jeon, Myounghoon and Lyons, Joseph B.},
	month = sep,
	year = {2021},
	pages = {57--61},
}

@incollection{lakhmaniProposedApproachDetermining2016,
	title = {A {Proposed} {Approach} for {Determining} the {Influence} of {Multimodal} {Robot}- of-{Human} {Transparency} {Information} on {Human}-{Agent} {Teams}},
	isbn = {978-3-319-39951-5},
	url = {http://dx.doi.org/10.1007/978-3-319-39952-2_29},
	booktitle = {Lecture {Notes} in {Computer} {Science}},
	publisher = {Springer International Publishing},
	author = {Lakhmani, Shan and Abich, IV, Julian and Barber, Daniel and Chen, Jessie},
	year = {2016},
	doi = {10.1007/978-3-319-39952-2_29},
	pages = {296--307},
}

@article{lakhmaniExploringEffectCommunication2019,
	title = {Exploring the {Effect} of {Communication} {Patterns} and {Transparency} on {Per} formance in a {Human}-{Robot} {Team}},
	volume = {63},
	issn = {2169-5067},
	url = {http://dx.doi.org/10.1177/1071181319631054},
	doi = {10.1177/1071181319631054},
	abstract = {{\textless}jats:p{\textgreater} Human-robot interaction requires communication, however what form this communication should take to facilitate effective team perfo rmance is still undetermined. One notion is that effective human-agent communications can be achieved by combining transparent information-s haring techniques with specific communication patterns. This study exa mines how transparency and a robot’s communication patterns interact t o affect human performance in a human-robot teaming task. Participants ’ performance in a target identification task was affected by the robo t’s communication pattern. Participants missed identifying more target s when they worked with a bidirectionally communicating robot than whe n they were working with a unidirectionally communicating one. Further more, working with a bidirectionally communicating robot led to fewer correct identifications than working with a unidirectionally communica ting robot, but only when the robot provided less transparency informa tion. The implications these findings have for future robot interface designs are discussed. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Lakhmani, Shan G. and Wright, Julia L. and Schwartz, Michael R. and Barber, Daniel},
	month = nov,
	year = {2019},
	pages = {160--164},
}

@inproceedings{ronconeTransparentRoleAssignment2017,
	title = {Transparent role assignment and task allocation in human robot collabo ration},
	url = {http://dx.doi.org/10.1109/ICRA.2017.7989122},
	doi = {10.1109/icra.2017.7989122},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Roncone, Alessandro and Mangin, Olivier and Scassellati, Brian},
	month = may,
	year = {2017},
}

@book{arnoldNoisyOptimizationEvolution2002,
	title = {Noisy optimization with evolution strategies},
	volume = {8},
	isbn = {1-4020-7105-1},
	publisher = {Springer Science \& Business Media},
	author = {Arnold, Dirk V.},
	year = {2002},
}

@article{raviExactInteriorKerr2018,
	title = {An {Exact} {Interior} {Kerr} {Solution}},
	volume = {64},
	issn = {13841076},
	url = {http://arxiv.org/abs/1705.06496},
	doi = {10.1016/j.newast.2018.04.003},
	abstract = {We present a simple exact solution for the interior of a rotating star. The interpretation of the stress energy tensor as that of a fluid requires the existence of a high viscosity, which is quite expected for a rotating fluid. In spite of the negative stresses, energy conditions are in fact all satisfied.},
	urldate = {2024-03-15},
	journal = {New Astronomy},
	author = {Ravi, Aravind P. and Banerjee, Narayan},
	month = oct,
	year = {2018},
	note = {arXiv:1705.06496 [gr-qc]},
	keywords = {General Relativity and Quantum Cosmology},
	pages = {31--33},
}

@article{muggletonInductiveLogicProgramming1994,
	series = {Special {Issue}: {Ten} {Years} of {Logic} {Programming}},
	title = {Inductive {Logic} {Programming}: {Theory} and methods},
	volume = {19-20},
	issn = {0743-1066},
	shorttitle = {Inductive {Logic} {Programming}},
	url = {https://www.sciencedirect.com/science/article/pii/0743106694900353},
	doi = {10.1016/0743-1066(94)90035-3},
	abstract = {Inductive Logic Programming (ILP) is a new discipline which investigates the inductive construction of first-order clausal theories from examples and background knowledge. We survey the most important theories and methods of this new field. First, various problem specifications of ILP are formalized in semantic settings for ILP, yielding a “model-theory” for ILP. Second, a generic ILP algorithm is presented. Third, the inference rules and corresponding operators used in ILP are presented, resulting in a “proof-theory” for ILP. Fourth, since inductive inference does not produce statements which are assured to follow from what is given, inductive inferences require an alternative form of justification. This can take the form of either probabilistic support or logical constraints on the hypothesis language. Information compression techniques used within ILP are presented within a unifying Bayesian approach to confirmation and corroboration of hypotheses. Also, different ways to constrain the hypothesis language or specify the declarative bias are presented. Fifth, some advanced topics in ILP are addressed. These include aspects of computational learning theory as applied to ILP, and the issue of predicate invention. Finally, we survey some applications and implementations of ILP. ILP applications fall under two different categories: first, scientific discovery and knowledge acquisition, and second, programming assistants.},
	urldate = {2024-03-15},
	journal = {The Journal of Logic Programming},
	author = {Muggleton, Stephen and de Raedt, Luc},
	month = may,
	year = {1994},
	pages = {629--679},
}

@inproceedings{alurSyntaxguidedSynthesis2013,
	title = {Syntax-guided synthesis},
	url = {https://ieeexplore.ieee.org/abstract/document/6679385},
	doi = {10.1109/FMCAD.2013.6679385},
	abstract = {The classical formulation of the program-synthesis problem is to find a program that meets a correctness specification given as a logical formula. Recent work on program synthesis and program optimization illustrates many potential benefits of allowing the user to supplement the logical specification with a syntactic template that constrains the space of allowed implementations. Our goal is to identify the core computational problem common to these proposals in a logical framework. The input to the syntax-guided synthesis problem (SyGuS) consists of a background theory, a semantic correctness specification for the desired program given by a logical formula, and a syntactic set of candidate implementations given by a grammar. The computational problem then is to find an implementation from the set of candidate expressions so that it satisfies the specification in the given theory. We describe three different instantiations of the counter-example-guided-inductive-synthesis (CEGIS) strategy for solving the synthesis problem, report on prototype implementations, and present experimental results on an initial set of benchmarks.},
	urldate = {2024-03-15},
	booktitle = {2013 {Formal} {Methods} in {Computer}-{Aided} {Design}},
	author = {Alur, Rajeev and Bodik, Rastislav and Juniwal, Garvit and Martin, Milo M. K. and Raghothaman, Mukund and Seshia, Sanjit A. and Singh, Rishabh and Solar-Lezama, Armando and Torlak, Emina and Udupa, Abhishek},
	month = oct,
	year = {2013},
	keywords = {Concrete, Grammar, Heuristic algorithms, Libraries, Production, Search problems, Syntactics},
	pages = {1--8},
}

@incollection{sobaniaChallengesProgramSynthesis2020,
	title = {Challenges of {Program} {Synthesis} with {Grammatical} {Evolution}},
	isbn = {978-3-030-44093-0},
	author = {Sobania, Dominik and Rothlauf, Franz},
	month = apr,
	year = {2020},
	doi = {10.1007/978-3-030-44094-7_14},
	pages = {211--227},
}

@article{davidProgramSynthesisChallenges2017,
	title = {Program synthesis: challenges and opportunities},
	volume = {375},
	shorttitle = {Program synthesis},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0403},
	doi = {10.1098/rsta.2015.0403},
	abstract = {Program synthesis is the mechanized construction of software, dubbed ‘self-writing code’. Synthesis tools relieve the programmer from thinking about how the problem is to be solved; instead, the programmer only provides a description of what is to be achieved. Given a specification of what the program should do, the synthesizer generates an implementation that provably satisfies this specification. From a logical point of view, a program synthesizer is a solver for second-order existential logic. Owing to the expressiveness of second-order logic, program synthesis has an extremely broad range of applications. We survey some of these applications as well as recent trends in the algorithms that solve the program synthesis problem. In particular, we focus on an approach that has raised the profile of program synthesis and ushered in a generation of new synthesis tools, namely counter-example-guided inductive synthesis (CEGIS). We provide a description of the CEGIS architecture, followed by recent algorithmic improvements. We conjecture that the capacity of program synthesis engines will see further step change, in a manner that is transparent to the applications, which will open up an even broader range of use-cases.

This article is part of the themed issue ‘Verified trustworthy software systems’.},
	number = {2104},
	urldate = {2024-03-15},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {David, Cristina and Kroening, Daniel},
	month = sep,
	year = {2017},
	note = {Publisher: Royal Society},
	keywords = {model checking, synthesis, verification},
	pages = {20150403},
}

@article{hochreiterLongShorttermMemory1997,
	title = {Long short-term memory},
	volume = {9},
	issn = {0899-7667},
	number = {8},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	note = {Publisher: MIT press},
	pages = {1735--1780},
}

@inproceedings{choLearningPhraseRepresentations2014,
	title = {Learning phrase representations using {RNN} encoder-decoder for statistical machine translation},
	isbn = {978-1-937284-96-1},
	doi = {10.3115/v1/d14-1179},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	booktitle = {{EMNLP} 2014 - 2014 conference on empirical methods in natural language processing, proceedings of the conference},
	author = {Cho, Kyunghyun and Van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	year = {2014},
	note = {arXiv: 1406.1078
tex.arxivid: 1406.1078
tex.mendeley-groups: langmodels},
	pages = {1724--1734},
}

@misc{kuzniaLessMoreSummary2022,
	title = {Less is {More}: {Summary} of {Long} {Instructions} is {Better} for {Program} {Synthesis}},
	abstract = {Despite the success of large pre-trained language models (LMs) such as Codex, they show below-par performance on the larger and more complicated programming related questions. We show that LMs benefit from the summarized version of complicated questions. Our findings show that superfluous information often present in problem description such as human characters, background stories, and names (which are included to help humans in understanding a task) does not help models in understanding a task. To this extent, we create a meta-dataset from the frequently used APPS dataset and the newly created CodeContests dataset for the program synthesis task. Our meta-dataset consists of human and synthesized summaries of the long and complicated programming questions. Experimental results on Codex show that our proposed approach outperforms baseline by 8.13\% on the APPS dataset and 11.88\% on the CodeContests dataset on average in terms of strict accuracy. Our analysis shows that summaries significantly improve performance for introductory (9.86\%) and interview (11.48\%) programming questions. However, it shows improvement by a small margin ({\textasciitilde} 2\%) for competitive programming questions, implying scope for future research in this direction.},
	publisher = {arXiv},
	author = {Kuznia, Kirby and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2203.08597},
	note = {Issue: arXiv:2203.08597},
}

@article{abolafiaNeuralProgramSynthesis2018,
	title = {Neural program synthesis with priority queue training},
	url = {http://arxiv.org/abs/1801.03526},
	abstract = {We consider the task of program synthesis in the presence of a reward function over the output of programs, where the goal is to find programs with maximal rewards. We employ an iterative optimization scheme, where we train an RNN on a dataset of K best programs from a priority queue of the generated programs so far. Then, we synthesize new programs and add them to the priority queue by sampling from the RNN. We benchmark our algorithm, called priority queue training (or PQT), against genetic algorithm and reinforcement learning baselines on a simple but expressive Turing complete programming language called BF. Our experimental results show that our simple PQT algorithm significantly outperforms the baselines. By adding a program length penalty to the reward function, we are able to synthesize short, human readable programs.},
	journal = {arXiv preprint arXiv:1801.03526},
	author = {Abolafia, Daniel A. and Norouzi, Mohammad and Shen, Jonathan and Zhao, Rui and Le, Quoc V.},
	year = {2018},
	note = {arXiv: 1801.03526
tex.arxivid: 1801.03526
tex.mendeley-groups: ind\_optimization,pirl},
}

@article{alonCode2vecLearningDistributed2019,
	title = {code2vec: {Learning} distributed representations of code},
	volume = {3},
	number = {POPL},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	year = {2019},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--29},
}

@incollection{andrewbagnellReinforcementLearningRobotics2014,
	title = {Reinforcement learning in robotics: {A} survey},
	volume = {97},
	abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our chapter lies on the choice between model-based and model-free as well as between value function-based and policy search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
	booktitle = {Springer tracts in advanced robotics},
	author = {Andrew Bagnell, J.},
	year = {2014},
	doi = {10.1007/978-3-319-03194-1_2},
	note = {ISSN: 1610742X
tex.mendeley-groups: reinforce},
	keywords = {learning control, reinforcement learning, robot, survey},
	pages = {9--67},
}

@article{arelReinforcementLearningbasedMultiagent2009,
	title = {Reinforcement learning-based multi-agent system for network traffic signal control},
	url = {www.ietdl.org},
	doi = {10.1049/iet-its.2009.0070},
	abstract = {A challenging application of artificial intelligence systems involves the scheduling of traffic signals in multi-intersection vehicular networks. This paper introduces a novel use of a multi-agent system and reinforcement learning (RL) framework to obtain an efficient traffic signal control policy. The latter is aimed at minimising the average delay, congestion and likelihood of intersection cross-blocking. A five-intersection traffic network has been studied in which each intersection is governed by an autonomous intelligent agent. Two types of agents, a central agent and an outbound agent, were employed. The outbound agents schedule traffic signals by following the longest-queue-first (LQF) algorithm, which has been proved to guarantee stability and fairness, and collaborate with the central agent by providing it local traffic statistics. The central agent learns a value function driven by its local and neighbours' traffic conditions. The novel methodology proposed here utilises the Q-Learning algorithm with a feedforward neural network for value function approximation. Experimental results clearly demonstrate the advantages of multi-agent RL-based control over LQF governed isolated single-intersection control, thus paving the way for efficient distributed traffic signal control in complex settings.},
	author = {Arel, I and Liu, C and Urbanik, T and Kohls, A G},
	year = {2009},
	note = {tex.mendeley-groups: reinforce},
}

@article{deeprl-survey2,
	title = {Deep reinforcement learning: {A} brief survey},
	volume = {34},
	doi = {10.1109/MSP.2017.2743240},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Arulkumaran, K. and Deisenroth, M. P. and Brundage, M. and Bharath, A. A.},
	year = {2017},
	pages = {26--38},
}

@article{astromOptimalControlMarkov1965,
	title = {Optimal control of {Markov} processes with incomplete state information},
	volume = {10},
	issn = {0022-247X},
	url = {http://www.sciencedirect.com/science/article/pii/0022247X6590154X},
	doi = {https://doi.org/10.1016/0022-247X(65)90154-X},
	number = {1},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Åström, K J},
	year = {1965},
	note = {tex.mendeley-groups: reinforce},
	pages = {174--205},
}

@article{cycle,
	title = {Dimensions of neural-symbolic integration-a structured survey},
	journal = {arXiv preprint cs/0511042},
	author = {Bader, Sebastian and Hitzler, Pascal},
	year = {2005},
}

@article{cartpole,
	title = {Neuronlike adaptive elements that can solve difficult learning control problems},
	volume = {SMC-13},
	issn = {2168-2909},
	doi = {10.1109/TSMC.1983.6313077},
	number = {5},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Barto, A. G. and Sutton, R. S. and Anderson, C. W.},
	month = sep,
	year = {1983},
	keywords = {Adaptive systems, Biological neural networks, Neurons, Pattern recognition, Problem-solving, Supervised learning, Training, adaptive control, adaptive critic element, animal learning studies, associative search element, learning control problem, learning systems, movable cart, neural nets, neuronlike adaptive elements},
	pages = {834--846},
}

@article{bernerDotaLargeScale2019,
	title = {Dota 2 with large scale deep reinforcement learning},
	volume = {abs/1912.06680},
	url = {http://arxiv.org/abs/1912.06680},
	journal = {CoRR},
	author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemyslaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Christopher and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and de Oliveira Pinto, Henrique Pondé and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
	year = {2019},
	note = {arXiv: 1912.06680
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Wed, 03 Jun 2020 10:56:28 +0200},
}

@inproceedings{bavishi2022:neurosymbolic,
	title = {Neurosymbolic {Repair} for {Low}-{Code} {Formula} {Languages}},
	abstract = {Most users of low-code platforms, such as Excel and PowerApps, write programs in domain-specific formula languages to carry out nontrivial tasks. Often users can write most of the program they want, but introduce small mistakes that yield broken formulas. These mistakes, which can be both syntactic and semantic, are hard for low-code users to identify […]},
	booktitle = {{OOPSLA}},
	author = {Bavishi, Rohan and Joshi, Harshit and Cambronero, José and Fariha, Anna and Gulwani, Sumit and Le, Vu and Radicek, Ivan and Tiwari, Ashish},
	month = dec,
	year = {2022},
}

@article{neuralsymbolic,
	title = {Neural-symbolic learning and reasoning: {A} survey and interpretation},
	journal = {arXiv preprint arXiv:1711.03902},
	author = {Besold, Tarek R and Garcez, Artur d'Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kühnberger, Kai-Uwe and Lamb, Luis C and Lowd, Daniel and Lima, Priscila Machado Vieira and {others}},
	year = {2017},
}

@article{taxi,
	title = {Hierarchical reinforcement learning with the {MAXQ} value function decomposition},
	volume = {13},
	journal = {Journal of Artificial Intelligence Research},
	author = {Dietterich, Thomas G.},
	year = {2000},
	pages = {227--303},
}

@article{gersLearningForgetContinual1999,
	title = {Learning to forget: {Continual} prediction with {LSTM}},
	author = {Gers, Felix A and Schmidhuber, Jürgen and Cummins, Fred},
	year = {1999},
	note = {Publisher: IET},
}

@inproceedings{trepan,
	title = {Relational knowledge extraction from neural networks},
	booktitle = {{CoCo}@ {NIPS}},
	author = {França, Manoel Vitor Macedo and Garcez, Artur S d'Avila and Zaverucha, Gerson},
	year = {2015},
}

@inproceedings{hanDeepCompressionCompressing2016,
	title = {Deep compression: {Compressing} deep neural networks with pruning, trained quantization and {Huffman} coding},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce “deep compression”, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.},
	booktitle = {4th international conference on learning representations, {ICLR} 2016 - conference track proceedings},
	author = {Han, Song and Mao, Huizi and Dally, William J},
	year = {2016},
	note = {arXiv: 1510.00149
tex.arxivid: 1510.00149},
}

@misc{huangPrompttunedCodeLanguage2022,
	title = {Prompt-tuned {Code} {Language} {Model} as a {Neural} {Knowledge} {Base} for {Type} {Inference} in {Statically}-{Typed} {Partial} {Code}},
	abstract = {Partial code usually involves non-fully-qualified type names (nonFQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this paper, we formulate type inference as a cloze-style fill-in-blank language task. Built on source code naturalness, our approach fine-tunes a code masked language model (MLM) as a neural knowledge base of code elements with a novel “pre-train, prompt and predict” paradigm from raw source code. Our approach is lightweight and has minimum requirements on code compilation. Unlike existing symbolic name and context matching for type inference, our prompt-tuned code MLM packs FQN syntax and usage in its parameters and supports fuzzy neural type inference. We systematically evaluate our approach on a large amount of source code from GitHub and Stack Overflow. Our results confirm the effectiveness of our approach design and the practicality for partial code type inference. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.},
	publisher = {arXiv},
	author = {Huang, Qing and Yuan, Zhiqiang and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Lu, Qinghua},
	month = aug,
	year = {2022},
	doi = {10.48550/arXiv.2208.05361},
	note = {Issue: arXiv:2208.05361},
}

@article{kantRecentAdvancesNeural2018,
	title = {Recent advances in neural program synthesis},
	url = {http://arxiv.org/abs/1802.02353},
	abstract = {In recent years, deep learning has made tremendous progress in a number of fields that were previously out of reach for artificial intelligence. The successes in these problems has led researchers to consider the possibilities for intelligent systems to tackle a problem that humans have only recently themselves considered: program synthesis. This challenge is unlike others such as object recognition and speech translation, since its abstract nature and demand for rigor make it difficult even for human minds to attempt. While it is still far from being solved or even competitive with most existing methods, neural program synthesis is a rapidly growing discipline which holds great promise if completely realized. In this paper, we start with exploring the problem statement and challenges of program synthesis. Then, we examine the fascinating evolution of program induction models, along with how they have succeeded, failed and been reimagined since. Finally, we conclude with a contrastive look at program synthesis and future research recommendations for the field.},
	author = {Kant, Neel},
	year = {2018},
	note = {arXiv: 1802.02353
tex.arxivid: 1802.02353
tex.mendeley-groups: review},
}

@article{rl-survey,
	title = {Reinforcement learning: {A} survey},
	volume = {4},
	journal = {Journal of artificial intelligence research},
	author = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
	year = {1996},
	pages = {237--285},
}

@article{daqn,
	title = {{DAQN}: {Deep} auto-encoder and {Q}-network},
	journal = {arXiv preprint arXiv:1806.00630},
	author = {Kimura, Daiki},
	year = {2018},
}

@inproceedings{kurachNeuralRandomaccessMachines2016,
	title = {Neural random-access machines},
	abstract = {In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions.},
	booktitle = {4th international conference on learning representations, {ICLR} 2016 - conference track proceedings},
	author = {Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya},
	year = {2016},
	note = {arXiv: 1511.06392
tex.arxivid: 1511.06392
tex.mendeley-groups: nlp},
}

@misc{kramerjdavidrPartiallyObservableMarkov1964,
	title = {Partially observable markov processes.},
	author = {Kramer, J David R, Jr},
	year = {1964},
	note = {tex.mendeley-groups: reinforce},
	keywords = {*LINEAR SYSTEMS, *MATHEMATICAL PREDICTION, *PROBABILITY, *STATISTICAL PROCESSES, AUTOMATA, COMPUTER PROGRAMMING, CONTROL, DECISION THEORY, MATRICES(MATHEMATICS), MOMENTS, OPERATIONS RESEARCH, STATISTICAL FUNCTIONS, VECTOR ANALYSIS},
}

@article{liReinforcementLearningApplications2019,
	title = {Reinforcement learning applications},
	volume = {abs/1908.06973},
	url = {http://arxiv.org/abs/1908.06973},
	journal = {CoRR},
	author = {Li, Yuxi},
	year = {2019},
	note = {arXiv: 1908.06973
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 26 Aug 2019 13:20:40 +0200},
}

@misc{linanderControlFlowBrainfuck2016,
	title = {control flow in brainfuck {\textbar} matslina},
	url = {http://calmerthanyouare.org/2016/01/14/control-flow-in-brainfuck.html},
	urldate = {2020-01-22},
	author = {Linander, Mats},
	year = {2016},
	note = {tex.mendeley-groups: plangs},
}

@inproceedings{lin2017:quixbugs,
	address = {Vancouver BC Canada},
	title = {{QuixBugs}: {A} multi-lingual program repair benchmark set based on the quixey challenge},
	isbn = {978-1-4503-5514-8},
	doi = {10.1145/3135932.3135941},
	abstract = {Recent years have seen an explosion of work in automated program repair. While previous work has focused exclusively on tools for single languages, recent work in multi-language transformation has opened the door for multi-language program repair tools. Evaluating the performance of such a tool requires having a benchmark set of similar buggy programs in different languages. We present QuixBugs, consisting of 40 programs translated to both Python and Java, each with a bug on a single line. The QuixBugs benchmark suite is based on problems from the Quixey Challenge, where programmers were given a short buggy program and 1 minute to fix the bug.},
	booktitle = {Companion of the {SIGPLAN} {International} {Conference} on {Systems}, {Programming}, {Languages}, and {Applications}: {Software} for {Humanity}},
	publisher = {ACM},
	author = {Lin, Derrick and Koppel, James and Chen, Angela and Solar-Lezama, Armando},
	month = oct,
	year = {2017},
	pages = {55--56},
}

@article{petke2018:genetic,
	title = {Genetic {Improvement} of {Software}: {A} {Comprehensive} {Survey}},
	volume = {22},
	issn = {1941-0026},
	shorttitle = {Genetic {Improvement} of {Software}},
	doi = {10.1109/tevc.2017.2693219},
	abstract = {Genetic improvement (GI) uses automated search to find improved versions of existing software. We present a comprehensive survey of this nascent field of research with a focus on the core papers in the area published between 1995 and 2015. We identified core publications including empirical studies, 96\% of which use evolutionary algorithms (genetic programming in particular). Although we can trace the foundations of GI back to the origins of computer science itself, our analysis reveals a significant upsurge in activity since 2012. GI has resulted in dramatic performance improvements for a diverse set of properties such as execution time, energy and memory consumption, as well as results for fixing and extending existing system functionality. Moreover, we present examples of research work that lies on the boundary between GI and other areas, such as program transformation, approximate computing, and software repair, with the intention of encouraging further exchange of ideas between researchers in these fields.},
	number = {3},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Petke, Justyna and Haraldsson, Saemundur O. and Harman, Mark and Langdon, William B. and White, David R. and Woodward, John R.},
	month = jun,
	year = {2018},
	keywords = {Genetic improvement (GI), Genetic programming, History, Software, Software engineering, Software testing, survey},
	pages = {415--432},
}

@article{deduction-review,
	title = {Fundamentals of deductive program synthesis},
	volume = {18},
	issn = {2326-3881},
	doi = {10.1109/32.153379},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Manna, Z. and Waldinger, R.},
	month = aug,
	year = {1992},
	keywords = {Algebra, Artificial intelligence, Computer languages, Computer science, Contracts, Logic functions, Military computing, Programming, Sorting, artificial intelligence, deductive program synthesis, deductive-tableau system, formal specification, induction rule, inference mechanisms, nonclausal resolution rule, program testing, proof, reasoning, specification, theorem proving, theorem-proving framework},
	pages = {674--704},
}

@techreport{mountain_car,
	title = {Efficient memory-based learning for robot control},
	author = {Moore, Andrew William},
	year = {1990},
}

@incollection{mousaviDeepReinforcementLearning2018,
	title = {Deep reinforcement learning: {An} overview},
	volume = {16},
	url = {https://arxiv.org/abs/},
	abstract = {In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This article reviews the recent advances in deep reinforcement learning with focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.},
	booktitle = {Lecture notes in networks and systems},
	author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
	year = {2018},
	doi = {10.1007/978-3-319-56991-8_32},
	note = {arXiv: 1806.08894
ISSN: 23673389
tex.arxivid: 1806.08894
tex.mendeley-groups: reinforce},
	keywords = {Deep leaning, MDPs, Neural networks, Observable MDPs, Reinforcement learning},
	pages = {426--440},
}

@misc{brainfuck,
	title = {Brainfuck – an eight-instruction turing-complete programming language},
	url = {http://en. wikipedia.org/wiki/Brainfuck},
	author = {Muller, U.},
	year = {1993},
}

@inproceedings{flashmeta,
	title = {Flashmeta: {A} framework for inductive program synthesis},
	booktitle = {Proceedings of the 2015 {ACM} {SIGPLAN} international conference on object-oriented programming, systems, languages, and applications},
	author = {Polozov, Oleksandr and Gulwani, Sumit},
	year = {2015},
	keywords = {Inductive program synthesis, deductive inference, domain-specific languages, frameworks, programming by examples, search-based synthesis},
	pages = {107--126},
}

@inproceedings{prenner2022:can,
	title = {Can {OpenAI}'s {Codex} {Fix} {Bugs}?: {An} evaluation on {QuixBugs}},
	doi = {10.1145/3524459.3527351},
	abstract = {OpenAI's Codex, a GPT-3like model trained on a large code corpus, has made headlines in and outside of academia. Given a short user-provided description, it is capable of synthesizing code snippets that are syntactically and semantically valid in most cases. In this work, we want to investigate whether Codex is able to localize and fix bugs, two important tasks in automated program repair. Our initial evaluation uses the multi-language QuixBugs benchmark (40 bugs in both Python and Java). We find that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques. Our results also show that Codex is more successful at repairing Python than Java, fixing 50\% more bugs in Python.},
	booktitle = {2022 {IEEE}/{ACM} {International} {Workshop} on {Automated} {Program} {Repair} ({APR})},
	author = {Prenner, Julian Aron and Babii, Hlib and Robbes, Romain},
	month = may,
	year = {2022},
	pages = {69--75},
}

@misc{renCodeBLEUMethodAutomatic2020,
	title = {{CodeBLEU}: {A} {Method} for {Automatic} {Evaluation} of {Code} {Synthesis}},
	abstract = {Evaluation metrics play a vital role in the growth of an area as it defines the standard of distinguishing between good and bad models. In the area of code synthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but they are not suitable enough to evaluate codes, because BLEU is originally designed to evaluate natural language, neglecting important syntactic and semantic features of codes, and perfect accuracy is too strict thus it underestimates different outputs with the same semantic logic. To remedy this, we introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the strength of BLEU in the n-gram match, and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow. We conduct experiments by evaluating the correlation coefficient between CodeBLEU and quality scores assigned by the programmers on three code synthesis tasks, i.e., text-to-code, code translation, and code refinement. Experimental results show that, our proposed CodeBLEU can achieve a better correlation with programmer assigned scores compared with BLEU and accuracy.},
	publisher = {arXiv},
	author = {Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Sundaresan, Neel and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
	month = sep,
	year = {2020},
	doi = {10.48550/arXiv.2009.10297},
	note = {Issue: arXiv:2009.10297},
}

@article{samekExplainableArtificialIntelligence2017,
	title = {Explainable artificial intelligence: {Understanding}, visualizing and interpreting deep learning models},
	url = {http://arxiv.org/abs/1708.08296},
	abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
	author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.08296
tex.arxivid: 1708.08296
tex.mendeley-groups: readable},
}

@article{shenSelectionIncentivesPerformance2003,
	title = {Selection incentives in a performance‐based contracting system},
	volume = {38},
	number = {2},
	journal = {Health services research},
	author = {Shen, Yujing},
	year = {2003},
	note = {ISBN: 0017-9124
Publisher: Wiley Online Library},
	pages = {535--552},
}

@misc{shrivastavaRepositoryLevelPromptGeneration2022,
	title = {Repository-{Level} {Prompt} {Generation} for {Large} {Language} {Models} of {Code}},
	abstract = {With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a remarkably high relative improvement of 36\% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines. The code for our work can be found at: {\textbackslash}url\{https://github.com/shrivastavadisha/repo\_level\_prompt\_generation\}.},
	publisher = {arXiv},
	author = {Shrivastava, Disha and Larochelle, Hugo and Tarlow, Daniel},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2206.12839},
	note = {Issue: arXiv:2206.12839},
}

@article{silverMasteringGameGo2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {14764687},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.},
	number = {7676},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Van Den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pmid = {29052630},
	note = {Publisher: Nature Publishing Group
tex.mendeley-groups: reinforce},
	pages = {354--359},
}

@book{suttonReinforcementLearningSecond2018,
	series = {Adaptive {Computation} and {Machine} {Learning} series},
	title = {Reinforcement {Learning}, second edition: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	url = {https://books.google.nl/books?id=5s-MEAAAQBAJ},
	publisher = {MIT Press},
	author = {Sutton, R.S. and Barto, A.G.},
	year = {2018},
	keywords = {Reinforcement learning},
}

@misc{tielemanLectureRmsPropDivide2012,
	title = {Lecture 6.5—{RmsProp}: {Divide} the gradient by a running average of its recent magnitude},
	author = {Tieleman, T. and Hinton, G.},
	year = {2012},
	note = {tex.howpublished: COURSERA: Neural Networks for Machine Learning},
}

@inproceedings{jripextr,
	title = {Revisiting neural-symbolic learning cycle},
	url = {https://sites.google.com/view/nesy2019/home},
	booktitle = {14th international workshop on neural-symbolic learning and reasoning},
	author = {Svatoš, Martin and Šourek, Gustav and Železný, Filip},
	year = {2019},
}

@article{turing,
	title = {On computable numbers, with an application to the entscheidungsproblem. a correction},
	volume = {s2-43},
	issn = {1460244X},
	doi = {10.1112/plms/s2-43.6.544},
	number = {1},
	journal = {Proceedings of the London Mathematical Society},
	author = {Turing, A M},
	year = {1938},
	pages = {544--546},
}

@inproceedings{vijayakumarNeuralguidedDeductiveSearch2018,
	title = {Neural-guided deductive search for real-time program synthesis from examples},
	url = {https://microsoft.github.io/prose/impact/},
	abstract = {Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12× speed-up compared to state-of-the-art systems.},
	booktitle = {6th international conference on learning representations, {ICLR} 2018 - conference track proceedings},
	author = {Vijayakumar, Ashwin K and Batra, Dhruv and Mohta, Abhishek and Jain, Prateek and Polozov, Oleksandr and Gulwani, Sumit},
	year = {2018},
	note = {arXiv: 1804.01186
tex.arxivid: 1804.01186
tex.mendeley-groups: sketching},
}

@inproceedings{westonMemoryNetworks2015,
	title = {Memory networks},
	url = {http://arxiv.org/abs/1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	booktitle = {3rd international conference on learning representations, {ICLR} 2015 - conference track proceedings},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	month = oct,
	year = {2015},
	note = {arXiv: 1410.3916
tex.arxivid: 1410.3916
tex.mendeley-groups: ind\_optimization},
}

@misc{bytecode,
	title = {Bytecode — {Wikipedia}, the free encyclopedia},
	url = {https://en.wikipedia.org/w/index.php?title=Bytecode&oldid=995026385},
	author = {{Wikipedia contributors}},
	year = {2020},
}

@article{williamsSimpleStatisticalGradientfollowing1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	number = {3-4},
	journal = {Machine learning},
	author = {Williams, Ronald J},
	year = {1992},
	note = {Publisher: Springer},
	pages = {229--256},
}

@article{xuSQLNetGeneratingStructured2017,
	title = {{SQLNet}: {Generating} structured queries from natural language without reinforcement learning},
	volume = {abs/1711.04436},
	url = {http://arxiv.org/abs/1711.04436},
	journal = {CoRR},
	author = {Xu, Xiaojun and Liu, Chang and Song, Dawn},
	year = {2017},
	note = {arXiv: 1711.04436
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 22 Jul 2019 13:37:30 +0200},
}

@inproceedings{conala,
	series = {{MSR}},
	title = {Learning to mine aligned code and natural language pairs from stack overflow},
	doi = {https://doi.org/10.1145/3196398.3196408},
	booktitle = {International conference on mining software repositories},
	publisher = {ACM},
	author = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
	year = {2018},
	pages = {476--486},
}

@article{healthcare-rl,
	title = {Reinforcement learning in healthcare: a survey},
	journal = {arXiv preprint arXiv:1908.08796},
	author = {Yu, Chao and Liu, Jiming and Nemati, Shamim},
	year = {2019},
}

@article{zarembaReinforcementLearningNeural2015,
	title = {Reinforcement learning neural turing machines},
	volume = {abs/1505.00521},
	url = {http://arxiv.org/abs/1505.00521},
	journal = {CoRR},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	year = {2015},
	note = {arXiv: 1505.00521
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 13 Aug 2018 16:46:53 +0200},
}

@inproceedings{zanLargeLanguageModels2023,
	address = {Toronto, Canada},
	title = {Large {Language} {Models} {Meet} {NL2Code}: {A} {Survey}},
	shorttitle = {Large {Language} {Models} {Meet} {NL2Code}},
	url = {https://aclanthology.org/2023.acl-long.411},
	doi = {10.18653/v1/2023.acl-long.411},
	abstract = {The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are “Large Size, Premium Data, Expert Tuning”. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang},
	year = {2023},
	pages = {7443--7464},
}

@article{zhouOptimizingChemicalReactions2017,
	title = {Optimizing chemical reactions with deep reinforcement learning},
	volume = {3},
	issn = {23747951},
	url = {https://pubs.acs.org/sharingguidelines},
	doi = {10.1021/acscentsci.7b00492},
	abstract = {Deep reinforcement learning was employed to optimize chemical reactions. Our model iteratively records the results of a chemical reaction and chooses new experimental conditions to improve the reaction outcome. This model outperformed a state-of-the-art blackbox optimization algorithm by using 71\% fewer steps on both simulations and real reactions. Furthermore, we introduced an efficient exploration strategy by drawing the reaction conditions from certain probability distributions, which resulted in an improvement on regret from 0.062 to 0.039 compared with a deterministic policy. Combining the efficient exploration policy with accelerated microdroplet reactions, optimal reaction conditions were determined in 30 min for the four reactions considered, and a better understanding of the factors that control microdroplet reactions was reached. Moreover, our model showed a better performance after training on reactions with similar or even dissimilar underlying mechanisms, which demonstrates its learning ability.},
	number = {12},
	journal = {ACS Central Science},
	author = {Zhou, Zhenpeng and Li, Xiaocheng and Zare, Richard N},
	year = {2017},
	note = {tex.mendeley-groups: reinforce},
	pages = {1337--1344},
}

@inproceedings{guptaSynthesizeExecuteDebug2020,
	title = {Synthesize, {Execute} and {Debug}: {Learning} to {Repair} for {Neural} {Program} {Synthesis}},
	volume = {33},
	shorttitle = {Synthesize, {Execute} and {Debug}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/cd0f74b5955dc87fd0605745c4b49ee8-Abstract.html},
	abstract = {The use of deep learning techniques has achieved significant progress for program synthesis from input-output examples. However, when the program semantics become more complex, it still remains a challenge to synthesize programs that are consistent with the specification. In this work, we propose SED, a neural program generation framework that incorporates synthesis, execution, and debugging stages. Instead of purely relying on the neural program synthesizer to generate the final program, SED first produces initial programs using the neural program synthesizer component, then utilizes a neural program debugger to iteratively repair the generated programs. The integration of the debugger component enables SED to modify the programs based on the execution results and specification, which resembles the coding process of human programmers. On Karel, a challenging input-output program synthesis benchmark, SED reduces the error rate of the neural program synthesizer itself by a considerable margin, and outperforms the standard beam search for decoding.},
	urldate = {2024-03-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gupta, Kavi and Christensen, Peter Ebert and Chen, Xinyun and Song, Dawn},
	year = {2020},
	pages = {17685--17695},
}

@techreport{shinSyntheticDatasetsNeural2019,
	title = {Synthetic datasets for neural program synthesis},
	abstract = {The goal of program synthesis is to automatically generate programs in a particular language from corresponding specifications, e.g. input-output behavior. Many current approaches achieve impressive results after training on randomly generated I/O examples in limited domain-specific languages (DSLs), as with string transformations in RobustFill. However, we empirically discover that applying test input generation techniques for languages with control flow and rich input space causes deep networks to generalize poorly to certain data distributions; to correct this, we propose a new methodology for controlling and evaluating the bias of synthetic data distributions over both programs and specifications. We demonstrate, using the Karel DSL and a small Calculator DSL, that training deep networks on these distributions leads to improved cross-distribution generalization performance.},
	author = {Shin, Richard and Kant, Neel and Gupta, Kavi and Bender, Christopher and Trabucco, Brandon and Singh, Rishabh and Song, Dawn},
	year = {2019},
	note = {Publication title: 7th international conference on learning representations, ICLR 2019
tex.mendeley-groups: ind\_optimization},
}

@article{gauntTerpreTProbabilisticProgramming2016,
	title = {{TerpreT}: {A} probabilistic programming language for program induction},
	volume = {abs/1608.04428},
	url = {http://arxiv.org/abs/1608.04428},
	journal = {CoRR},
	author = {Gaunt, Alexander L. and Brockschmidt, Marc and Singh, Rishabh and Kushman, Nate and Kohli, Pushmeet and Taylor, Jonathan and Tarlow, Daniel},
	year = {2016},
	note = {arXiv: 1608.04428
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Thu, 22 Oct 2020 17:01:21 +0200},
}

@article{atari,
	title = {The arcade learning environment: {An} evaluation platform for general agents},
	volume = {47},
	journal = {Journal of Artificial Intelligence Research},
	author = {Bellemare, M. G. and Naddaf, Y. and Veness, J. and Bowling, M.},
	month = jun,
	year = {2013},
	pages = {253--279},
}

@article{richardsonCode2TextChallengeText2017,
	title = {The {Code2Text} challenge: {Text} generation in source code libraries},
	volume = {abs/1708.00098},
	url = {http://arxiv.org/abs/1708.00098},
	journal = {CoRR},
	author = {Richardson, Kyle and Zarrieß, Sina and Kuhn, Jonas},
	year = {2017},
	note = {arXiv: 1708.00098
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 13 Aug 2018 16:47:55 +0200},
}

@article{allenUnicodeStandard2012,
	title = {The unicode standard},
	journal = {Mountain view, CA},
	author = {Allen, Julie D and Anderson, Deborah and Becker, Joe and Cook, Richard and Davis, Mark and Edberg, Peter and Everson, Michael and Freytag, Asmus and Iancu, Laurentiu and Ishida, Richard and {others}},
	year = {2012},
	note = {Publisher: Citeseer},
}

@misc{evestop,
	title = {vadim0x60/evestop: {Early} stopping with exponential variance elmination},
	copyright = {All rights reserved},
	url = {https://github.com/vadim0x60/evestop},
	author = {Liventsev, Vadim},
	year = {2021},
}

@article{aichasm,
	title = {With an eye to {AI} and autonomous diagnosis},
	volume = {1},
	journal = {NPJ digital medicine},
	author = {Keane, P. A. and Topol, E. J.},
	year = {2018},
	pages = {40},
}

@article{adaptivediscretization,
	title = {Zooming for efficient model-free reinforcement learning in metric spaces},
	journal = {arXiv preprint arXiv:2003.04069},
	author = {Touati, Ahmed and Taiga, Adrien Ali and Bellemare, Marc G},
	year = {2020},
}

@article{liventsevTreeVariationalAutoencoder,
	title = {Tree {Variational} {Autoencoder} for {Code}},
	copyright = {All rights reserved},
	language = {en},
	author = {Liventsev, Vadim and de Bruin, Sander and Harma, Aki and Petkovic, Milan},
}

@article{bodikAlgorithmicProgramSynthesis2013,
	title = {Algorithmic program synthesis: introduction},
	volume = {15},
	issn = {1433-2779, 1433-2787},
	shorttitle = {Algorithmic program synthesis},
	url = {http://link.springer.com/10.1007/s10009-013-0287-9},
	doi = {10.1007/s10009-013-0287-9},
	abstract = {Program synthesis is a process of producing an executable program from a specification. Algorithmic synthesis produces the program automatically, without an intervention from an expert. While classical compilation falls under the definition of algorithmic program synthesis, with the source program being the specification, the synthesis literature is typically concerned with producing programs that cannot be (easily) obtained with the deterministic transformations of a compiler. To this end, synthesis algorithms often perform a search, either in a space of candidate programs or in a space of transformations that might be composed to transform the specification into a desired program. In this introduction to the special journal issue, we survey the history of algorithmic program synthesis and introduce the contributed articles. We divide the field into reactive synthesis, which is concerned with automata-theoretic techniques for controllers that handle an infinite stream of requests, and functional synthesis, which produces programs consuming finite input. Contributed articles are divided analogously. We also provide pointers to synthesis work outside these categories and list many applications of synthesis.},
	language = {en},
	number = {5-6},
	urldate = {2023-12-22},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Bodik, Rastislav and Jobstmann, Barbara},
	month = oct,
	year = {2013},
	keywords = {Controller Synthesis, Formal Methods, Program synthesis, Specifications of Program Correctness},
	pages = {397--411},
}

@book{scrum,
	title = {Agile software development with {Scrum}},
	volume = {1},
	publisher = {Prentice Hall Upper Saddle River},
	author = {Schwaber, Ken and Beedle, Mike},
	year = {2002},
}

@inproceedings{rabinovich2017abstract,
	title = {Abstract syntax networks for code generation and semantic parsing},
	booktitle = {Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: {Long} papers)},
	author = {Rabinovich, Maxim and Stern, Mitchell and Klein, Dan},
	year = {2017},
	pages = {1139--1149},
}

@inproceedings{xuSystematicEvaluationLarge2022,
	address = {San Diego CA USA},
	title = {A systematic evaluation of large language models of code},
	isbn = {978-1-4503-9273-0},
	url = {https://dl.acm.org/doi/10.1145/3520312.3534862},
	doi = {10.1145/3520312.3534862},
	language = {en},
	urldate = {2023-03-22},
	booktitle = {Proceedings of the 6th {ACM} {SIGPLAN} {International} {Symposium} on {Machine} {Programming}},
	publisher = {ACM},
	author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
	month = jun,
	year = {2022},
	pages = {1--10},
}

@article{ahvanooeySurveyGeneticProgramming2019,
	title = {A survey of genetic programming and its applications},
	volume = {13},
	number = {4},
	journal = {KSII Transactions on Internet and Information Systems (TIIS)},
	author = {Ahvanooey, Milad Taleby and Li, Qianmu and Wu, Ming and Wang, Shuo},
	year = {2019},
	note = {Publisher: Korean Society for Internet Information},
	pages = {1765--1794},
}

@inproceedings{buReinforcementLearningApproach2009,
	title = {A reinforcement learning approach to online web systems auto-configuration},
	isbn = {978-0-7695-3660-6},
	doi = {10.1109/ICDCS.2009.76},
	abstract = {In a web system, configuration is crucial to the performance and service availability. It is a challenge, not only because of the dynamics of Internet traffic, but also the dynamic virtual machine environment the system tends to be run on. In this paper, we propose a reinforcement learning approach for autonomic configuration and reconfiguration of multi-tier web systems. It is able to adapt performance parameter settings not only to the change of workload, but also to the change of virtual machine configurations. The RL approach is enhanced with an efficient initialization policy to reduce the learning time for online decision. The approach is evaluated using TPC-W benchmark on a three-tier website hosted on a Xen-based virtual machine environment. Experiment results demonstrate that the approach can auto-configure the web system dynamically in response to the change in both workload and VM resource. It can drive the system into a near-optimal configuration setting in less than 25 trial-and-error iterations. © 2009 IEEE.},
	booktitle = {Proceedings - international conference on distributed computing systems},
	author = {Bu, Xiangping and Rao, Jia and Xu, Cheng Zhong},
	year = {2009},
	note = {tex.mendeley-groups: reinforce},
	pages = {2--11},
}

@inproceedings{code2nlg2,
	title = {A neural model for generating natural language summaries of program subroutines},
	doi = {10.1109/ICSE.2019.00087},
	booktitle = {2019 {IEEE}/{ACM} 41st international conference on software engineering ({ICSE})},
	author = {LeClair, A. and Jiang, S. and McMillan, C.},
	year = {2019},
	pages = {795--806},
}

@inproceedings{sobania2022:choose,
	address = {Boston Massachusetts},
	title = {Choose your programming copilot: {A} comparison of the program synthesis performance of github copilot and genetic programming},
	isbn = {978-1-4503-9237-2},
	doi = {10.1145/3512290.3528700},
	abstract = {GitHub Copilot, an extension for the Visual Studio Code development environment powered by the large-scale language model Codex, makes automatic program synthesis available for software developers. This model has been extensively studied in the field of deep learning, however, a comparison to genetic programming, which is also known for its performance in automatic program synthesis, has not yet been carried out. In this paper, we evaluate GitHub Copilot on standard program synthesis benchmark problems and compare the achieved results with those from the genetic programming literature. In addition, we discuss the performance of both approaches. We find that the performance of the two approaches on the benchmark problems is quite similar, however, in comparison to GitHub Copilot, the program synthesis approaches based on genetic programming are not yet mature enough to support programmers in practical software development. Genetic programming usually needs a huge amount of expensive hand-labeled training cases and takes too much time to generate solutions. Furthermore, source code generated by genetic programming approaches is often bloated and difficult to understand. For future work on program synthesis with genetic programming, we suggest researchers to focus on improving the execution time, readability, and usability.},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {ACM},
	author = {Sobania, Dominik and Briesch, Martin and Rothlauf, Franz},
	month = jul,
	year = {2022},
	pages = {1019--1027},
}

@article{helmuth2015:solving,
	title = {Solving {Uncompromising} {Problems} {With} {Lexicase} {Selection}},
	volume = {19},
	issn = {1941-0026},
	doi = {10/f7sv9t},
	abstract = {We describe a broad class of problems, called “uncompromising problems,” which are characterized by the requirement that solutions must perform optimally on each of many test cases. Many of the problems that have long motivated genetic programming research, including the automation of many traditional programming tasks, are uncompromising. We describe and analyze the recently proposed “lexicase” parent selection algorithm and show that it can facilitate the solution of uncompromising problems by genetic programming. Unlike most traditional parent selection techniques, lexicase selection does not base selection on a fitness value that is aggregated over all test cases; rather, it considers test cases one at a time in random order. We present results comparing lexicase selection to more traditional parent selection methods, including standard tournament selection and implicit fitness sharing, on four uncompromising problems: 1) finding terms in finite algebras; 2) designing digital multipliers; 3) counting words in files; and 4) performing symbolic regression of the factorial function. We provide evidence that lexicase selection maintains higher levels of population diversity than other selection methods, which may partially explain its utility as a parent selection algorithm in the context of uncompromising problems.},
	number = {5},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Helmuth, Thomas and Spector, Lee and Matheson, James},
	month = oct,
	year = {2015},
	pages = {630--643},
}

@inproceedings{brown2020:language,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'20},
	title = {Language models are few-shot learners},
	isbn = {978-1-71382-954-6},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	booktitle = {International {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = dec,
	year = {2020},
	pages = {1877--1901},
}

@misc{liventsev2021:bf,
	title = {{BF}++: {A} language for general-purpose program synthesis},
	copyright = {All rights reserved},
	abstract = {Most state of the art decision systems based on Reinforcement Learning (RL) are data-driven black-box neural models, where it is often difficult to incorporate expert knowledge into the models or let experts review and validate the learned decision mechanisms. Knowledge-insertion and model review are important requirements in many applications involving human health and safety. One way to bridge the gap between data and knowledge driven systems is program synthesis: replacing a neural network that outputs decisions with a symbolic program generated by a neural network or by means of genetic programming. We propose a new programming language, BF++, designed specifically for automatic programming of agents in a Partially Observable Markov Decision Process (POMDP) setting and apply neural program synthesis to solve standard OpenAI Gym benchmarks.},
	publisher = {arXiv},
	author = {Liventsev, Vadim and Härmä, Aki and Petković, Milan},
	month = jan,
	year = {2021},
	doi = {10.48550/arXiv.2101.09571},
}

@incollection{bastani2022:interpretable,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interpretable, {Verifiable}, and {Robust} {Reinforcement} {Learning} via {Program} {Synthesis}},
	isbn = {978-3-031-04083-2},
	abstract = {Reinforcement learning is a promising strategy for automatically training policies for challenging control tasks. However, state-of-the-art deep reinforcement learning algorithms focus on training deep neural network (DNN) policies, which are black box models that are hard to interpret and reason about. In this chapter, we describe recent progress towards learning policies in the form of programs. Compared to DNNs, such programmatic policies are significantly more interpretable, easier to formally verify, and more robust. We give an overview of algorithms designed to learn programmatic policies, and describe several case studies demonstrating their various advantages.},
	booktitle = {{xxAI} - {Beyond} {Explainable} {AI}: {International} {Workshop}, {Held} in {Conjunction} with {ICML} 2020, {July} 18, 2020, {Vienna}, {Austria}, {Revised} and {Extended} {Papers}},
	publisher = {Springer},
	author = {Bastani, Osbert and Inala, Jeevana Priya and Solar-Lezama, Armando},
	editor = {Holzinger, Andreas and Goebel, Randy and Fong, Ruth and Moon, Taesup and Müller, Klaus-Robert and Samek, Wojciech},
	year = {2022},
	doi = {10.j5zn},
	pages = {207--228},
}

@article{jia2022:role,
	title = {The {Role} of {Explainability} in {Assuring} {Safety} of {Machine} {Learning} in {Healthcare}},
	volume = {10},
	issn = {2168-6750},
	doi = {10.1109/tetc.2022.3171314},
	abstract = {Established approaches to assuring safety-critical systems and software are difficult to apply to systems employing ML where there is no clear, pre-defined specification against which to assess validity. This problem is exacerbated by the “opaque” nature of ML where the learnt model is not amenable to human scrutiny. Explainable AI (XAI) methods have been proposed to tackle this issue by producing human-interpretable representations of ML models which can help users to gain confidence and build trust in the ML system. However, little work explicitly investigates the role of explainability for safety assurance in the context of ML development. This paper identifies ways in which XAI methods can contribute to safety assurance of ML-based systems. It then uses a concrete ML-based clinical decision support system, concerning weaning of patients from mechanical ventilation, to demonstrate how XAI methods can be employed to produce evidence to support safety assurance. The results are also represented in a safety argument to show where, and in what way, XAI methods can contribute to a safety case. Overall, we conclude that XAI methods have a valuable role in safety assurance of ML-based systems in healthcare but that they are not sufficient in themselves to assure safety.},
	number = {4},
	journal = {IEEE Transactions on Emerging Topics in Computing},
	author = {Jia, Yan and McDermid, John and Lawton, Tom and Habli, Ibrahim},
	month = oct,
	year = {2022},
	pages = {1746--1760},
}

@article{alur2015:syntaxguided,
	title = {Syntax-{Guided} {Synthesis}},
	doi = {10.3233/978-1-61499-495-4-1},
	journal = {Dependable Software Systems Engineering},
	author = {Alur, Rajeev and Bodik, Rastislav and Dallal, Eric and Fisman, Dana and Garg, Pranav and Juniwal, Garvit and Kress-Gazit, Hadas and Madhusudan, P. and Martin, Milo M. K. and Raghothaman, Mukund and Saha, Shamwaditya and Seshia, Sanjit A. and Singh, Rishabh and Solar-Lezama, Armando and Torlak, Emina and Udupa, Abhishek},
	year = {2015},
	pages = {1--25},
}

@incollection{connolly2023:systematic,
	title = {Systematic {Literature} {Review}: {XAI} and {Clinical} {Decision} {Support}},
	isbn = {978-1-66845-092-5},
	abstract = {Machine learning (ML) applications hold significant promise for innovation within healthcare; however, their full potential has not yet been realised, with limited reports of their clinical and cost benefits in clinical practice. This is due to complex clinical, ethical, and legal questions arising...},
	booktitle = {Diverse {Perspectives} and {State}-of-the-{Art} {Approaches} to the {Utilization} of {Data}-{Driven} {Clinical} {Decision} {Support} {Systems}},
	publisher = {IGI Global},
	author = {Connolly, Thomas M. and Soflano, Mario and Papadopoulos, Petros},
	year = {2023},
	doi = {10/j5zp},
	pages = {161--188},
}

@article{dhar2021:survey,
	title = {A survey of on-device machine learning: {An} algorithms and learning theory perspective},
	volume = {2},
	issn = {2577-6207},
	doi = {10.1145/3450494},
	number = {3},
	journal = {ACM Transactions on Internet of Things},
	author = {Dhar, Sauptik and Guo, Junyao and Liu, Jiayi and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
	year = {2021},
	pages = {1--49},
}

@inproceedings{lu2021:codexglue,
	title = {{CodeXGLUE}: {A} {Machine} {Learning} {Benchmark} {Dataset} for {Code} {Understanding} and {Generation}},
	booktitle = {Proceedings of the {Neural} {Information} {Processing} {Systems} {Track} on {Datasets} and {Benchmarks}},
	author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and Gong, Ming and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and Liu, Shujie},
	month = dec,
	year = {2021},
	pages = {1--16},
}

@article{allamanis2018:survey,
	title = {A {Survey} of {Machine} {Learning} for {Big} {Code} and {Naturalness}},
	volume = {51},
	issn = {0360-0300},
	doi = {10.1145/3212695},
	abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	month = jul,
	year = {2018},
	pages = {81:1--81:37},
}

@article{grigorescu2020:survey,
	title = {A survey of deep learning techniques for autonomous driving},
	volume = {37},
	issn = {1556-4967},
	doi = {10.1002/rob.21918},
	abstract = {The last decade witnessed increasingly rapid progress in self-driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence (AI). The objective of this paper is to survey the current state-of-the-art on deep learning technologies used in autonomous driving. We start by presenting AI-based self-driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration, and motion control algorithms. We investigate both the modular perception-planning-action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing AI architectures for autonomous driving, such as their safety, training data sources, and computational hardware. The comparison presented in this survey helps gain insight into the strengths and limitations of deep learning and AI approaches for autonomous driving and assist with design choices.},
	number = {3},
	journal = {Journal of Field Robotics},
	author = {Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel},
	year = {2020},
	pages = {362--386},
}

@inproceedings{chen2021:latent,
	title = {Latent {Execution} for {Neural} {Program} {Synthesis} {Beyond} {Domain}-{Specific} {Languages}},
	volume = {34},
	abstract = {Program synthesis from input-output (IO) examples has been a long-standing challenge. While recent works demonstrated limited success on domain-specific languages (DSL), it remains highly challenging to apply them to real-world programming languages, such as C. Due to complicated syntax and token variation, there are three major challenges: (1) unlike many DSLs, programs in languages like C need to compile first and are not executed via interpreters; (2) the program search space grows exponentially when the syntax and semantics of the programming language become more complex; and (3) collecting a large-scale dataset of real-world programs is non-trivial. As a first step to address these challenges, we propose LaSynth and show its efficacy in a restricted-C domain (i.e., C code with tens of tokens, with sequential, branching, loop and simple arithmetic operations but no library call). More specifically, LaSynth learns the latent representation to approximate the execution of partially generated programs, even if they are incomplete in syntax (addressing (1)). The learned execution significantly improves the performance of next token prediction over existing approaches, facilitating search (addressing (2)). Finally, once trained with randomly generated ground-truth programs and their IO pairs, LaSynth can synthesize more concise programs that resemble human-written code. Furthermore, retraining our model with these synthesized programs yields better performance with fewer samples for both Karel and C program synthesis, indicating the promise of leveraging the learned program synthesizer to improve the dataset quality for input-output program synthesis (addressing (3)). When evaluating on whether the program execution outputs match the IO pairs, LaSynth achieves 55.2\% accuracy on generating simple C code with tens of tokens including loops and branches, outperforming existing approaches without executors by around 20\%.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Xinyun and Song, Dawn and Tian, Yuandong},
	year = {2021},
	pages = {22196--22208},
}

@article{marcano2020:review,
	title = {A review of shared control for automated vehicles: {Theory} and applications},
	volume = {50},
	issn = {2168-2291},
	doi = {10.1109/thms.2020.3017748},
	number = {6},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Marcano, Mauricio and Díaz, Sergio and Pérez, Joshué and Irigoyen, Eloy},
	year = {2020},
	pages = {475--491},
}

@misc{husain2020:codesearchnet,
	title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}},
	abstract = {Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas. To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task. We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.},
	publisher = {arXiv},
	author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
	month = jun,
	year = {2020},
	doi = {10.48550/arXiv.1909.09436},
}

@misc{:tiobe,
	title = {{TIOBE} {Index}},
	url = {https://www.tiobe.com/tiobe-index/},
}

@misc{iyer2018:mapping,
	title = {Mapping {Language} to {Code} in {Programmatic} {Context}},
	abstract = {Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to "return the smallest element" in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.},
	publisher = {arXiv},
	author = {Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1808.09588},
}

@inproceedings{roziere2020:unsupervised,
	address = {Vancouver, Canada},
	title = {Unsupervised {Translation} of {Programming} {Languages}},
	abstract = {A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.},
	booktitle = {34th {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Roziere, Baptiste and Lachaux, Marie-Anne and Lample, Guillaume and Chanussot, Lowik},
	year = {2020},
}

@misc{niu2023:crosscodebench,
	title = {{CrossCodeBench}: {Benchmarking} {Cross}-{Task} {Generalization} of {Source} {Code} {Models}},
	abstract = {Despite the recent advances showing that a model pre-trained on large-scale source code data is able to gain appreciable generalization capability, it still requires a sizeable amount of data on the target task for fine-tuning. And the effectiveness of the model generalization is largely affected by the size and quality of the fine-tuning data, which is detrimental for target tasks with limited or unavailable resources. Therefore, cross-task generalization, with the goal of improving the generalization of the model to unseen tasks that have not been seen before, is of strong research and application value. In this paper, we propose a large-scale benchmark that includes 216 existing code-related tasks. Then, we annotate each task with the corresponding meta information such as task description and instruction, which contains detailed information about the task and a solution guide. This also helps us to easily create a wide variety of “training/evaluation” task splits to evaluate the various cross-task generalization capabilities of the model. Then we perform some preliminary experiments to demonstrate that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross-task learning research on our benchmark. We hope that the collection of the datasets and our benchmark will facilitate future work that is not limited to cross-task generalization.},
	publisher = {arXiv},
	author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Luo, Bin},
	month = feb,
	year = {2023},
	doi = {10.48550/arXiv.2302.04030},
}

@article{chakraborty2021:deep,
	title = {Deep {Learning} {Based} {Vulnerability} {Detection}: {Are} {We} {There} {Yet}?},
	volume = {48},
	issn = {1939-3520},
	doi = {10.1109/tse.2021.3087402},
	abstract = {Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95 percent at detecting vulnerabilities. In this paper, we ask, “how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?” To our surprise, we find that their performance drops by more than 50 percent. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline—up to 33.57 percent boost in precision and 128.38 percent boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Chakraborty, Saikat and Krishna, Rahul and Ding, Yangruibo and Ray, Baishakhi},
	month = sep,
	year = {2022},
	pages = {3280--3296},
}

@techreport{gulwani2016:programming,
	address = {Redmond, WA, USA},
	title = {Programming by {Examples} (and its applications in {Data} {Wrangling})},
	abstract = {Programming by Examples (PBE) has the potential to revolutionize enduser programming by enabling end users, most of whom are non-programmers, to create scripts for automating repetitive tasks. PBE involves synthesizing intended programs in an underlying domain-specific language (DSL) from example based specifications (Ispec). We formalize the notion of Ispec and discuss some principles behind designing useful DSLs for synthesis.},
	institution = {Microsoft Corportation},
	author = {Gulwani, Sumit},
	year = {2016},
	pages = {22},
}

@misc{ouyang2022:training,
	title = {Training language models to follow instructions with human feedback},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.02155},
}

@inproceedings{fernandes2016:reviewbased,
	address = {New York, NY, USA},
	series = {{EASE} '16},
	title = {A review-based comparative study of bad smell detection tools},
	isbn = {978-1-4503-3691-8},
	doi = {10.1145/2915970.2915984},
	abstract = {Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C\#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Fernandes, Eduardo and Oliveira, Johnatan and Vale, Gustavo and Paiva, Thanis and Figueiredo, Eduardo},
	month = jun,
	year = {2016},
	pages = {1--12},
}

@misc{zavershynskyi2018:naps,
	title = {{NAPS}: {Natural} program synthesis dataset},
	publisher = {arXiv},
	author = {Zavershynskyi, Maksym and Skidanov, Alex and Polosukhin, Illia},
	year = {2018},
	doi = {10.48550/arXiv.1807.0316},
}

@book{koza1994:genetic,
	title = {Genetic programming {II}},
	volume = {17},
	publisher = {MIT Press},
	author = {Koza, John R},
	year = {1994},
}

@misc{ahmad2023:fixing,
	title = {Fixing {Hardware} {Security} {Bugs} with {Large} {Language} {Models}},
	abstract = {Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many coding-adjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.},
	publisher = {arXiv},
	author = {Ahmad, Baleegh and Thakur, Shailja and Tan, Benjamin and Karri, Ramesh and Pearce, Hammond},
	month = feb,
	year = {2023},
	doi = {10.48550/arXiv.2302.01215},
}

@book{blain2002:nine,
	title = {Nine {Worlds} of {Seid}-magic: {Ecstasy} and {Neo}-shamanism in {North} {European} {Paganism}},
	isbn = {978-0-415-25651-3},
	abstract = {This accessible study of Northern European shamanistic practice, or seidr, explores the way in which the ancient Norse belief systems evoked in the Icelandic Sagas and Eddas have been rediscovered and reinvented by groups in Europe and North America. The book examines the phenomenon of altered consciousness and the interactions of seid-workers or shamanic practitioners with their spirit worlds. Written by a follower of seidr, it investigates new communities involved in a postmodern quest for spiritual meaning.},
	publisher = {Routledge},
	author = {Blain, Jenny},
	year = {2002},
}

@book{russell2010:artificial,
	title = {Artificial intelligence a modern approach},
	isbn = {0-13-604259-7},
	publisher = {Pearson Education, Inc.},
	author = {Russell, Stuart J},
	year = {2010},
}

@article{jack2000:optimal,
	title = {Optimal {Repair}–{Replace} strategies for a warranted product},
	volume = {67},
	issn = {09255273},
	doi = {10.1016/s0925-5273(00)00012-8},
	abstract = {Semantic Scholar extracted view of "Optimal repair-replace strategies for a warranted product" by N. Jack et al.},
	number = {1},
	journal = {International Journal of Production Economics},
	author = {Jack, Nat and Van der Duyn Schouten, Frank},
	month = aug,
	year = {2000},
	pages = {95--100},
}

@misc{sobania2023:analysis,
	title = {An {Analysis} of the {Automatic} {Bug} {Fixing} {Performance} of {ChatGPT}},
	abstract = {To support software developers in finding and fixing software bugs, several automated program repair techniques have been introduced. Given a test suite, standard methods usually either synthesize a repair, or navigate a search space of software edits to find test-suite passing variants. Recent program repair methods are based on deep learning approaches. One of these novel methods, which is not primarily intended for automated program repair, but is still suitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard bug fixing benchmark set, QuixBugs, and compare the performance with the results of several other approaches reported in the literature. We find that ChatGPT's bug fixing performance is competitive to the common deep learning approaches CoCoNut and Codex and notably better than the results reported for the standard program repair approaches. In contrast to previous approaches, ChatGPT offers a dialogue system through which further information, e.g., the expected output for a certain input or an observed error message, can be entered. By providing such hints to ChatGPT, its success rate can be further increased, fixing 31 out of 40 bugs, outperforming state-of-the-art.},
	publisher = {arXiv},
	author = {Sobania, Dominik and Briesch, Martin and Hanna, Carol and Petke, Justyna},
	month = jan,
	year = {2023},
	doi = {10.48550/arXiv.2301.08653},
}

@misc{chowdhery2022:palm,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2204.02311},
}

@article{helmuth2022:applying,
	title = {Applying genetic programming to {PSB2}: {The} next generation program synthesis benchmark suite},
	volume = {23},
	issn = {1573-7632},
	doi = {10.1007/s10710-022-09434-y},
	abstract = {For the past seven years, researchers in genetic programming and other program synthesis disciplines have used the General Program Synthesis Benchmark Suite (PSB1) to benchmark many aspects of systems that conduct programming by example, where the specifications of the desired program are given as input/output pairs. PSB1 has been used to make notable progress toward the goal of general program synthesis: automatically creating the types of software that human programmers code. Many of the systems that have attempted the problems in PSB1 have used it to demonstrate performance improvements granted through new techniques. Over time, the suite has gradually become outdated, hindering the accurate measurement of further improvements. The field needs a new set of more difficult benchmark problems to move beyond what was previously possible and ensure that systems do not overfit to one benchmark suite. In this paper, we describe the 25 new general program synthesis benchmark problems that make up PSB2, a new benchmark suite. These problems are curated from a variety of sources, including programming katas and college courses. We selected these problems to be more difficult than those in the original suite, and give results using PushGP showing this increase in difficulty. We additionally give an example of benchmarking using a state-of-the-art parent selection method, showing improved performance on PSB2 while still leaving plenty of room for improvement. These new problems will help guide program synthesis research for years to come.},
	number = {3},
	journal = {Genetic Programming and Evolvable Machines},
	author = {Helmuth, Thomas and Kelly, Peter},
	month = sep,
	year = {2022},
	pages = {375--404},
}

@inproceedings{zheng2023:codegeex,
	address = {New York, NY, USA},
	series = {{KDD} '23},
	title = {{CodeGeeX}: {A} {Pre}-{Trained} {Model} for {Code} {Generation} with {Multilingual} {Benchmarking} on {HumanEval}-{X}},
	isbn = {9798400701030},
	doi = {10/gs8hs5},
	abstract = {Large pre-trained code generation models, such as OpenAI Codex, can generate syntax-and function-correct code, making the coding of programmers more productive. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 8 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4\% of its users. Finally, CodeGeeX is publicly accessible since Sep. 2022, we open-sourced its code, model weights, API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Shen, Lei and Wang, Zihan and Wang, Andi and Li, Yang and Su, Teng and Yang, Zhilin and Tang, Jie},
	month = aug,
	year = {2023},
	pages = {5673--5684},
}

@misc{openai2023:gpt4,
	title = {{GPT}-4 {Technical} {Report}},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	publisher = {arXiv},
	author = {{OpenAI}},
	month = mar,
	year = {2023},
	doi = {10.48550/arXiv.2303.08774},
}

@inproceedings{helmuth2015:general,
	address = {Madrid Spain},
	title = {General {Program} {Synthesis} {Benchmark} {Suite}},
	isbn = {978-1-4503-3472-3},
	doi = {10.1145/2739480.2754769},
	abstract = {Recent interest in the development and use of non-trivial benchmark problems for genetic programming research has highlighted the scarcity of general program synthesis (also called “traditional programming”) benchmark problems. We present a suite of 29 general program synthesis benchmark problems systematically selected from sources of introductory computer science programming problems. This suite is suitable for experiments with any program synthesis system driven by input/output examples. We present results from illustrative experiments using our reference implementation of the problems in the PushGP genetic programming system. The results show that the problems in the suite vary in difficulty and can be useful for assessing the capabilities of a program synthesis system.},
	booktitle = {Proceedings of the 2015 {Annual} {Conference} on {Genetic} and {Evolutionary} {Computation}},
	publisher = {ACM},
	author = {Helmuth, Thomas and Spector, Lee},
	month = jul,
	year = {2015},
	pages = {1039--1046},
}

@misc{touvron2023:llama,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	doi = {10.48550/arXiv.2302.13971},
}

@article{helmuth2022:problemsolving,
	title = {Problem-{Solving} {Benefits} of {Down}-{Sampled} {Lexicase} {Selection}},
	volume = {27},
	issn = {1064-5462},
	doi = {10.1162/artl_a_00341},
	abstract = {In genetic programming, an evolutionary method for producing computer programs that solve specified computational problems, parent selection is ordinarily based on aggregate measures of performance across an entire training set. Lexicase selection, by contrast, selects on the basis of performance on random sequences of training cases; this has been shown to enhance problem-solving power in many circumstances. Lexicase selection can also be seen as better reflecting biological evolution, by modeling sequences of challenges that organisms face over their lifetimes. Recent work has demonstrated that the advantages of lexicase selection can be amplified by down-sampling, meaning that only a random subsample of the training cases is used each generation. This can be seen as modeling the fact that individual organisms encounter only subsets of the possible environments and that environments change over time. Here we provide the most extensive benchmarking of down-sampled lexicase selection to date, showing that its benefits hold up to increased scrutiny. The reasons that down-sampling helps, however, are not yet fully understood. Hypotheses include that down-sampling allows for more generations to be processed with the same budget of program evaluations; that the variation of training data across generations acts as a changing environment, encouraging adaptation; or that it reduces overfitting, leading to more general solutions. We systematically evaluate these hypotheses, finding evidence against all three, and instead draw the conclusion that down-sampled lexicase selection's main benefit stems from the fact that it allows the evolutionary process to examine more individuals within the same computational budget, even though each individual is examined less completely.},
	number = {3–4},
	journal = {Artificial Life},
	author = {Helmuth, Thomas and Spector, Lee},
	month = mar,
	year = {2022},
	pages = {183--203},
}

@misc{joshi2022:repair,
	title = {Repair {Is} {Nearly} {Generation}: {Multilingual} {Program} {Repair} with {LLMs}},
	abstract = {Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program - a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are domain-specific and do not easily carry over to new domains. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple domains with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different domains and comparing performance to domain-specific repair engines. We show that RING can outperform domain-specific repair engines in 3 of these domains. We also identify directions for future research using LLMCs for multilingual repair.},
	publisher = {arXiv},
	author = {Joshi, Harshit and Cambronero, José and Gulwani, Sumit and Le, Vu and Radicek, Ivan and Verbruggen, Gust},
	month = sep,
	year = {2022},
	doi = {10.48550/arXiv.2208.11640},
}

@article{renzulloEvolvingSoftwareCombining2023,
	title = {Evolving {Software}: {Combining} {Online} {Learning} with {Mutation}-{Based} {Stochastic} {Search}},
	volume = {3},
	shorttitle = {Evolving {Software}},
	url = {https://dl.acm.org/doi/10.1145/3597617},
	doi = {10.1145/3597617},
	abstract = {Evolutionary algorithms and related mutation-based methods have been used in software engineering, with recent emphasis on the problem of repairing bugs. In this work, programs are typically not synthesized from a random start. Instead, existing solutions—which may be flawed or inefficient—are taken as starting points, with the evolutionary process searching for useful improvements. This approach, however, introduces a challenge for the search algorithm: what is the optimal number of neutral mutations that should be combined? Too much is likely to introduce errors and break the program while too little hampers the search process, inducing the classic tradeoff between exploration and exploitation. In the context of software improvement, this work considers MWRepair, an algorithm for enhancing mutation-based searches, which uses online learning to optimize the tradeoff between exploration and exploitation. The aggressiveness parameter governs how many individual mutations should be applied simultaneously to an individual between fitness evaluations. MWRepair is evaluated in the context of automated program repair problems, where the goal is repairing software bugs with minimal human involvement. The article analyzes the search space for automated program repair induced by neutral mutations, finding that the greatest probability of finding successful repairs often occurs when many neutral mutations are applied to the original program. Moreover, repair probability follows a characteristic, unimodal distribution. MWRepair uses online learning to leverage this property, finding both rare and multi-edit repairs to defects in the popular Defects4J benchmark set of buggy Java programs.},
	number = {4},
	urldate = {2023-12-14},
	journal = {ACM Transactions on Evolutionary Learning and Optimization},
	author = {Renzullo, Joseph and Weimer, Westley and Forrest, Stephanie},
	month = dec,
	year = {2023},
	keywords = {Neutral mutations, automated program repair, multiplicative weights update, software mutational robustness},
	pages = {13:1--13:32},
}

@misc{haluptzokLanguageModelsCan2023,
	title = {Language {Models} {Can} {Teach} {Themselves} to {Program} {Better}},
	url = {http://arxiv.org/abs/2207.14502},
	abstract = {Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM's performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model 'improves itself' using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al., 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Haluptzok, Patrick and Bowers, Matthew and Kalai, Adam Tauman},
	month = apr,
	year = {2023},
	doi = {10.48550/arXiv.2207.14502},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{shinnReflexionLanguageAgents2023,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = oct,
	year = {2023},
	doi = {10.48550/arXiv.2303.11366},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{niLEVERLearningVerify2023,
	title = {{LEVER}: {Learning} to {Verify} {Language}-to-{Code} {Generation} with {Execution}},
	shorttitle = {{LEVER}},
	url = {https://proceedings.mlr.press/v202/ni23b.html},
	abstract = {The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs (4.6\% to 10.9\% with code-davinci-002) and achieves new state-of-the-art results on all of them.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Veselin and Yih, Wen-Tau and Wang, Sida and Lin, Xi Victoria},
	month = jul,
	year = {2023},
	pages = {26106--26128},
}

@inproceedings{linSolvingLinguisticOlympiad2023,
	address = {Taipei City, Taiwan},
	title = {Solving {Linguistic} {Olympiad} {Problems} with {Tree}-of-{Thought} {Prompting}},
	url = {https://aclanthology.org/2023.rocling-1.33},
	urldate = {2023-12-14},
	booktitle = {Proceedings of the 35th {Conference} on {Computational} {Linguistics} and {Speech} {Processing} ({ROCLING} 2023)},
	publisher = {The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)},
	author = {Lin, Zheng-Lin and Yen, Chiao-Han and Xu, Jia-Cheng and Watty, Deborah and Hsieh, Shu-Kai},
	editor = {Wu, Jheng-Long and Su, Ming-Hsiang},
	month = oct,
	year = {2023},
	pages = {262--269},
}

@misc{wangCompilableNeuralCode2022,
	title = {Compilable {Neural} {Code} {Generation} with {Compiler} {Feedback}},
	url = {http://arxiv.org/abs/2203.05132},
	abstract = {Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Wang, Xin and Wang, Yasheng and Wan, Yao and Mi, Fei and Li, Yitong and Zhou, Pingyi and Liu, Jin and Wu, Hao and Jiang, Xin and Liu, Qun},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.05132},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages},
}

@misc{DiffModelsNew2023,
	title = {Diff {Models} – {A} {New} {Way} to {Edit} {Code}},
	url = {https://carper.ai/diff-models-a-new-way-to-edit-code/},
	abstract = {Visit the post for more.},
	language = {en-GB},
	urldate = {2023-12-14},
	month = jan,
	year = {2023},
}

@misc{Defects4JDatabaseExisting,
	title = {{Defects4J}: a database of existing faults to enable controlled testing studies for {Java} programs {\textbar} {Proceedings} of the 2014 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	url = {https://dl.acm.org/doi/abs/10.1145/2610384.2628055},
	urldate = {2023-12-14},
}

@book{stanleyWhyGreatnessCannot2015a,
	title = {Why greatness cannot be planned: {The} myth of the objective},
	isbn = {3-319-15524-5},
	publisher = {Springer},
	author = {Stanley, Kenneth O. and Lehman, Joel},
	year = {2015},
}

@inproceedings{zanCERTContinualPretraining2022,
	address = {Vienna, Austria},
	title = {{CERT}: {Continual} {Pre}-training on {Sketches} for {Library}-oriented {Code} {Generation}},
	isbn = {978-1-956792-00-3},
	shorttitle = {{CERT}},
	url = {https://www.ijcai.org/proceedings/2022/329},
	doi = {10.24963/ijcai.2022/329},
	abstract = {Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive textcode paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large-scale unlabelled code corpora and perform well in code generation. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present CERT with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and the generator are continually pre-trained upon a base model using unlabelled data. Furthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate library-oriented code generation. Experimental results demonstrate the impressive performance of CERT. For example, it surpasses the base model by an absolute 15.67\% improvement in terms of pass@1 on PandasEval. Our work is available at https://github.com/microsoft/PyCodeGPT.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},
	month = jul,
	year = {2022},
	pages = {2369--2375},
}

@misc{yuBetterChainofThoughtPrompting2023,
	title = {Towards {Better} {Chain}-of-{Thought} {Prompting} {Strategies}: {A} {Survey}},
	shorttitle = {Towards {Better} {Chain}-of-{Thought} {Prompting} {Strategies}},
	url = {http://arxiv.org/abs/2310.04959},
	abstract = {Chain-of-Thought (CoT), a step-wise and coherent reasoning chain, shows its impressive strength when used as a prompting strategy for large language models (LLM). Recent years, the prominent effect of CoT prompting has attracted emerging research. However, there still lacks of a systematic summary about key factors of CoT prompting and comprehensive guide for prompts utilizing. For a deeper understanding about CoT prompting, we survey on a wide range of current research, presenting a systematic and comprehensive analysis on several factors that may influence the effect of CoT prompting, and introduce how to better apply it in different applications under these discussions. We further analyze the challenges and propose some future directions about CoT prompting. This survey could provide an overall reference on related research.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Yu, Zihan and He, Liang and Wu, Zhen and Dai, Xinyu and Chen, Jiajun},
	month = oct,
	year = {2023},
	doi = {10.48550/arXiv.2310.04959},
	keywords = {Computer Science - Computation and Language},
}

@misc{roziereCodeLlamaOpen2023,
	title = {Code {Llama}: {Open} {Foundation} {Models} for {Code}},
	shorttitle = {Code {Llama}},
	url = {http://arxiv.org/abs/2308.12950},
	abstract = {We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53\% and 55\% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
	month = aug,
	year = {2023},
	doi = {10.48550/arXiv.2308.12950},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{pearceAsleepKeyboardAssessing2022,
	address = {San Francisco, CA, USA},
	title = {Asleep at the {Keyboard}? {Assessing} the {Security} of {GitHub} {Copilot}’s {Code} {Contributions}},
	isbn = {978-1-66541-316-9},
	shorttitle = {Asleep at the {Keyboard}?},
	url = {https://ieeexplore.ieee.org/document/9833571/},
	doi = {10.1109/SP46214.2022.9833571},
	abstract = {There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the ﬁrst self-described ‘AI pair programmer’, GitHub Copilot, which is a language model trained over open-source GitHub code. However, code often contains bugs—and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot’s code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk cybersecurity weaknesses, e.g. those from MITRE’s “Top 25” Common Weakness Enumeration (CWE) list. We explore Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40 \% to be vulnerable.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
	month = may,
	year = {2022},
	pages = {754--768},
}

@article{andreStateAbstractionProgrammable,
	title = {State {Abstraction} for {Programmable} {Reinforcement} {Learning} {Agents}},
	abstract = {Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield signiﬁcant state abstraction while maintaining hierarchical optimality, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich’s taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.},
	language = {en},
	author = {Andre, David and Russell, Stuart J},
}

@article{ellisAlgorithmsLearningInduce,
	title = {Algorithms for {Learning} to {Induce} {Programs}},
	abstract = {The future of machine learning should have a knowledge representation that supports, at a minimum, several features: Expressivity, interpretability, the potential for reuse by both humans and machines, while also enabling sample-efﬁcient generalization. Here we argue that programs–i.e., source code–are a knowledge representation which can contribute to the project of capturing these elements of intelligence. This research direction however requires new program synthesis algorithms which can induce programs solving a range of AI tasks. This program induction challenge confronts two primary obstacles: the space of all programs is inﬁnite, so we need a strong inductive bias or prior to steer us toward the correct programs; and even if we have that prior, effectively searching through the vast combinatorial space of all programs is generally intractable. We introduce algorithms that learn to induce programs, with the goal of addressing these two primary obstacles. Focusing on case studies in vision, computational linguistics, and learning-to-learn, we develop an algorithmic toolkit for learning inductive biases over programs as well as learning to search for programs, drawing on probabilistic, neural, and symbolic methods. Together this toolkit suggests ways in which program induction can contribute to AI, and how we can use learning to improve program synthesis technologies.},
	language = {en},
	author = {Ellis, Kevin},
}

@misc{StripeLoginSign,
	title = {Stripe {Login} {\textbar} {Sign} in to the {Stripe} {Dashboard}},
	url = {https://dashboard.stripe.com/login},
	urldate = {2024-02-22},
}

@misc{DrVadim,
	title = {Dr {Vadim}},
	url = {https://www.overleaf.com/project/6582f56c6dfb6890ded16524},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2024-02-22},
}

@article{mannaAutomaticProgramSynthesis1971,
	title = {Toward automatic program synthesis},
	volume = {14},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/362566.362568},
	doi = {10.1145/362566.362568},
	abstract = {An elementary outline of the theorem-proving approach to automatic program synthesis is given, without dwelling on technical details. The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers, lists, and trees. In order to construct a program satisfying certain specifications, a theorem induced by those specifications is proved, and the desired program is extracted from the proof. The same technique is applied to transform recursively defined functions into iterative programs, frequently with a major gain in efficiency. It is emphasized that in order to construct a program with loops or with recursion, the principle of mathematical induction must be applied. The relation between the version of the induction rule used and the form of the program constructed is explored in some detail.},
	number = {3},
	urldate = {2024-02-22},
	journal = {Communications of the ACM},
	author = {Manna, Zohar and Waldinger, Richard J.},
	month = mar,
	year = {1971},
	keywords = {answer extraction, artificial intelligence, automatic program synthesis, mathematical induction principle, problem solving, theorem proving},
	pages = {151--165},
}

@misc{NumpyVersionsGoogle,
	title = {numpy versions - {Google} {Search}},
	url = {https://www.google.com/search?q=numpy+versions&oq=numpy+versions&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIHCAEQABiABDIHCAIQABiABDIHCAMQABiABDIHCAQQABiABDIHCAUQABiABDIHCAYQABiABDIHCAcQABiABDIHCAgQABiABDIHCAkQABiABNIBCDI0MDRqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8},
	urldate = {2024-02-22},
}

@misc{giannouLoopedTransformersProgrammable2023,
	title = {Looped {Transformers} as {Programmable} {Computers}},
	url = {http://arxiv.org/abs/2301.13196},
	abstract = {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{kidgerNeuralControlledDifferential2020,
	title = {Neural {Controlled} {Differential} {Equations} for {Irregular} {Time} {Series}},
	url = {http://arxiv.org/abs/2005.08926},
	abstract = {Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations. Here, we demonstrate how this may be resolved through the well-understood mathematics of {\textbackslash}emph\{controlled differential equations\}. The resulting {\textbackslash}emph\{neural controlled differential equation\} model is directly applicable to the general setting of partially-observed irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efficient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models.},
	urldate = {2024-01-27},
	publisher = {arXiv},
	author = {Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},
	month = nov,
	year = {2020},
	note = {arXiv:2005.08926 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kuznetsovaImportanceStepwiseEmbeddings2023,
	title = {On the {Importance} of {Step}-wise {Embeddings} for {Heterogeneous} {Clinical} {Time}-{Series}},
	url = {http://arxiv.org/abs/2311.08902},
	abstract = {Recent advances in deep learning architectures for sequence modeling have not fully transferred to tasks handling time-series from electronic health records. In particular, in problems related to the Intensive Care Unit (ICU), the state-of-the-art remains to tackle sequence classification in a tabular manner with tree-based methods. Recent findings in deep learning for tabular data are now surpassing these classical methods by better handling the severe heterogeneity of data input features. Given the similar level of feature heterogeneity exhibited by ICU time-series and motivated by these findings, we explore these novel methods' impact on clinical sequence modeling tasks. By jointly using such advances in deep learning for tabular data, our primary objective is to underscore the importance of step-wise embeddings in time-series modeling, which remain unexplored in machine learning methods for clinical data. On a variety of clinically relevant tasks from two large-scale ICU datasets, MIMIC-III and HiRID, our work provides an exhaustive analysis of state-of-the-art methods for tabular time-series as time-step embedding models, showing overall performance improvement. In particular, we evidence the importance of feature grouping in clinical time-series, with significant performance gains when considering features within predefined semantic groups in the step-wise embedding module.},
	urldate = {2024-01-26},
	publisher = {arXiv},
	author = {Kuznetsova, Rita and Pace, Alizée and Burger, Manuel and Yèche, Hugo and Rätsch, Gunnar},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08902 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning},
}

@article{agarapDeepLearningUsing2018,
	title = {Deep learning using rectified linear units (relu)},
	journal = {arXiv preprint arXiv:1803.08375},
	author = {Agarap, Abien Fred},
	year = {2018},
}

@misc{PrivacyPolicy,
	title = {Privacy policy},
	url = {https://openai.com/policies/privacy-policy},
	language = {en-US},
	urldate = {2024-01-23},
}

@article{achiamGpt4TechnicalReport2023,
	title = {Gpt-4 technical report},
	journal = {arXiv preprint arXiv:2303.08774},
	author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal},
	year = {2023},
}

@article{truscottDetectingShadowEconomy2011,
	title = {Detecting shadow economy sizes with symbolic regression},
	journal = {Genetic Programming Theory and Practice IX},
	author = {Truscott, Philip D. and Korns, Michael F.},
	year = {2011},
	note = {ISBN: 1461417694
Publisher: Springer},
	pages = {195--210},
}

@article{yamashitaCustomizedPredictionAttendance2022,
	title = {Customized prediction of attendance to soccer matches based on symbolic regression and genetic programming},
	volume = {187},
	journal = {Expert Systems with Applications},
	author = {Yamashita, Gabrielli H. and Fogliatto, Flavio S. and Anzanello, Michel J. and Tortorella, Guilherme L.},
	year = {2022},
	note = {ISBN: 0957-4174
Publisher: Elsevier},
	pages = {115912},
}

@article{panInfluentialFactorsCarbon2019,
	title = {Influential factors of carbon emissions intensity in {OECD} countries: evidence from symbolic regression},
	volume = {220},
	journal = {Journal of Cleaner Production},
	author = {Pan, Xiongfeng and Uddin, Md Kamal and Ai, Bowei and Pan, Xianyou and Saima, Umme},
	year = {2019},
	note = {ISBN: 0959-6526
Publisher: Elsevier},
	pages = {1194--1201},
}

@article{lianModelingForecastingPassenger2018,
	title = {Modeling and forecasting passenger car ownership based on symbolic regression},
	volume = {10},
	number = {7},
	journal = {Sustainability},
	author = {Lian, Lian and Tian, Wen and Xu, Hongfeng and Zheng, Menglan},
	year = {2018},
	note = {ISBN: 2071-1050
Publisher: MDPI},
	pages = {2275},
}

@article{claveriaAssessmentEffectFinancial2017,
	title = {Assessment of the effect of the financial crisis on agents’ expectations through symbolic regression},
	volume = {24},
	number = {9},
	journal = {Applied Economics Letters},
	author = {Claveria, Oscar and Monte, Enric and Torra, Salvador},
	year = {2017},
	note = {ISBN: 1350-4851
Publisher: Taylor \& Francis},
	pages = {648--652},
}

@article{truscottExplainingUnemploymentRates2014,
	title = {Explaining unemployment rates with symbolic regression},
	journal = {Genetic Programming Theory and Practice XI},
	author = {Truscott, Philip and Korns, Michael F.},
	year = {2014},
	note = {ISBN: 1493903748
Publisher: Springer},
	pages = {119--135},
}

@incollection{duffyUsingSymbolicRegression2002,
	title = {Using symbolic regression to infer strategies from experimental data},
	booktitle = {Evolutionary computation in {Economics} and {Finance}},
	publisher = {Springer},
	author = {Duffy, John and Engle-Warnick, Jim},
	year = {2002},
	pages = {61--82},
}

@article{chenRevealingComplexEcological2019,
	title = {Revealing complex ecological dynamics via symbolic regression},
	volume = {41},
	number = {12},
	journal = {BioEssays},
	author = {Chen, Yize and Angulo, Marco Tulio and Liu, Yang-Yu},
	year = {2019},
	note = {ISBN: 0265-9247
Publisher: Wiley Online Library},
	pages = {1900069},
}

@article{udrescuAIFeynmanPhysicsinspired2020,
	title = {{AI} {Feynman}: {A} physics-inspired method for symbolic regression},
	volume = {6},
	shorttitle = {{AI} {Feynman}},
	url = {https://www.science.org/doi/10.1126/sciadv.aay2631},
	doi = {10.1126/sciadv.aay2631},
	abstract = {A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90\%.},
	number = {16},
	urldate = {2024-01-23},
	journal = {Science Advances},
	author = {Udrescu, Silviu-Marian and Tegmark, Max},
	month = apr,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaay2631},
}

@misc{AIFeynmanPhysicsinspired,
	title = {{AI} {Feynman}: {A} physics-inspired method for symbolic regression {\textbar} {Science} {Advances}},
	url = {https://www.science.org/doi/10.1126/sciadv.aay2631},
	urldate = {2024-01-23},
}

@article{angelisArtificialIntelligencePhysical2023,
	title = {Artificial {Intelligence} in {Physical} {Sciences}: {Symbolic} {Regression} {Trends} and {Perspectives}},
	volume = {30},
	issn = {1886-1784},
	shorttitle = {Artificial {Intelligence} in {Physical} {Sciences}},
	url = {https://doi.org/10.1007/s11831-023-09922-z},
	doi = {10.1007/s11831-023-09922-z},
	abstract = {Symbolic regression (SR) is a machine learning-based regression method based on genetic programming principles that integrates techniques and processes from heterogeneous scientific fields and is capable of providing analytical equations purely from data. This remarkable characteristic diminishes the need to incorporate prior knowledge about the investigated system. SR can spot profound and elucidate ambiguous relations that can be generalizable, applicable, explainable and span over most scientific, technological, economical, and social principles. In this review, current state of the art is documented, technical and physical characteristics of SR are presented, the available programming techniques are investigated, fields of application are explored, and future perspectives are discussed.},
	language = {en},
	number = {6},
	urldate = {2024-01-23},
	journal = {Archives of Computational Methods in Engineering},
	author = {Angelis, Dimitrios and Sofos, Filippos and Karakasidis, Theodoros E.},
	month = jul,
	year = {2023},
	pages = {3845--3865},
}

@article{tenachiDeepSymbolicRegression2023,
	title = {Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws},
	volume = {959},
	issn = {0004-637X, 1538-4357},
	shorttitle = {Deep symbolic regression for physics guided by units constraints},
	url = {http://arxiv.org/abs/2303.03192},
	doi = {10.3847/1538-4357/ad014c},
	abstract = {Symbolic Regression is the study of algorithms that automate the search for analytic expressions that fit data. While recent advances in deep learning have generated renewed interest in such approaches, the development of symbolic regression methods has not been focused on physics, where we have important additional constraints due to the units associated with our data. Here we present \${\textbackslash}Phi\$-SO, a Physical Symbolic Optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints. Our system is built, from the ground up, to propose solutions where the physical units are consistent by construction. This is useful not only in eliminating physically impossible solutions, but because the "grammatical" rules of dimensional analysis restrict enormously the freedom of the equation generator, thus vastly improving performance. The algorithm can be used to fit noiseless data, which can be useful for instance when attempting to derive an analytical property of a physical model, and it can also be used to obtain analytical approximations to noisy data. We test our machinery on a standard benchmark of equations from the Feynman Lectures on Physics and other physics textbooks, achieving state-of-the-art performance in the presence of noise (exceeding 0.1\%) and show that it is robust even in the presence of substantial (10\%) noise. We showcase its abilities on a panel of examples from astrophysics.},
	number = {2},
	urldate = {2024-01-23},
	journal = {The Astrophysical Journal},
	author = {Tenachi, Wassim and Ibata, Rodrigo and Diakogiannis, Foivos I.},
	month = dec,
	year = {2023},
	note = {arXiv:2303.03192 [astro-ph, physics:physics]},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Physics - Computational Physics},
	pages = {99},
}

@article{linardatosExplainableAiReview2020,
	title = {Explainable ai: {A} review of machine learning interpretability methods},
	volume = {23},
	number = {1},
	journal = {Entropy},
	author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
	year = {2020},
	note = {ISBN: 1099-4300
Publisher: MDPI},
	pages = {18},
}

@inproceedings{skalseInvariancePolicyOptimisation2023,
	title = {Invariance in policy optimisation and partial identifiability in reward learning},
	isbn = {2640-3498},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Skalse, Joar Max Viktor and Farrugia-Roberts, Matthew and Russell, Stuart and Abate, Alessandro and Gleave, Adam},
	year = {2023},
	pages = {32033--32058},
}

@article{skalseDefiningCharacterizingReward2022,
	title = {Defining and {Characterizing} {Reward} {Gaming}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/3d719fee332caa23d5038b8a90e81796-Abstract-Conference.html},
	language = {en},
	urldate = {2024-01-23},
	journal = {Advances in Neural Information Processing Systems},
	author = {Skalse, Joar and Howe, Nikolaus and Krasheninnikov, Dmitrii and Krueger, David},
	month = dec,
	year = {2022},
	pages = {9460--9471},
}

@article{skalseDefiningCharacterizingReward,
	title = {Defining and {Characterizing} {Reward} {Hacking}},
	abstract = {We provide the first formal definition of reward hacking, a phenomenon where optimizing an imperfect proxy reward function, R˜, leads to poor performance according to the true reward function, R. We say that a proxy is unhackable if increasing the expected proxy return can never decrease the expected true return. Intuitively, it might be possible to create an unhackable proxy by leaving some terms out of the reward function (making it “narrower”) or overlooking fine-grained distinctions between roughly equivalent outcomes, but we show this is usually not the case. A key insight is that the linearity of reward (in state-action visit counts) makes unhackability a very strong condition. In particular, for the set of all stochastic policies, two reward functions can only be unhackable if one of them is constant. We thus turn our attention to deterministic policies and finite sets of stochastic policies, where non-trivial unhackable pairs always exist, and establish necessary and sufficient conditions for the existence of simplifications, an important special case of unhackability. Our results reveal a tension between using reward functions to specify narrow tasks and aligning AI systems with human values.},
	language = {en},
	author = {Skalse, Joar and Howe, Nikolaus H R and Krueger, David and Krasheninnikov, Dmitrii},
}

@misc{ObstacleDetectionSafe,
	title = {Obstacle {Detection} and {Safe} {Navigation} in {Unexpected} {Situations} of {Intelligent} {Vehicle} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Obstacle-Detection-and-Safe-Navigation-in-of-Sheebajoice-Reddy/f9628992f3b42bdd12391d219b8acd738d079d99},
	urldate = {2024-01-23},
}

@misc{ImpactPhysicianBonuses,
	title = {The impact of physician bonuses, enhanced fees, and feedback on childhood immunization coverage rates. {\textbar} {AJPH} {\textbar} {Vol}. 89 {Issue} 2},
	url = {https://ajph.aphapublications.org/doi/abs/10.2105/AJPH.89.2.171},
	urldate = {2024-01-23},
}

@article{wintersDiagnosticErrorsIntensive2012,
	title = {Diagnostic errors in the intensive care unit: a systematic review of autopsy studies},
	volume = {21},
	issn = {2044-5415, 2044-5423},
	shorttitle = {Diagnostic errors in the intensive care unit},
	url = {https://qualitysafety.bmj.com/lookup/doi/10.1136/bmjqs-2012-000803},
	doi = {10.1136/bmjqs-2012-000803},
	language = {en},
	number = {11},
	urldate = {2024-01-23},
	journal = {BMJ Quality \& Safety},
	author = {Winters, Bradford and Custer, Jason and Galvagno, Samuel M and Colantuoni, Elizabeth and Kapoor, Shruti G and Lee, HeeWon and Goode, Victoria and Robinson, Karen and Nakhasi, Atul and Pronovost, Peter and Newman-Toker, David},
	month = nov,
	year = {2012},
	pages = {894--902},
}

@article{sheebajoiceObstacleDetectionSafe2023,
	title = {Obstacle {Detection} and {Safe} {Navigation} in {Unexpected} {Situations} of {Intelligent} {Vehicle}},
	url = {https://ieeexplore.ieee.org/document/10100210/},
	doi = {10.1109/ICIDCA56705.2023.10100210},
	abstract = {Transportation technology now relies on autonomous, intelligent cars. They use WSN technology in the service of improving traffic flow and motorist security (Wireless Sensor Network). Over the years, several approaches have been used regarding crucial issues including the various levels of autonomous driving and its associated safety, obstacle detection, and obstacle avoidance tactics. Intelligent transportation system deployment requires a multi-step process. This research proposes a solution in the form of an obstacle detection sensor system that can avoid collisions while continuing on a predetermined direction. Intelligent vehicle safety is greatly influenced by obstacle detection and avoidance technologies. Calculating and redefining the minimal distance between three potentials is used in obstacle identification to distinguish between obstacles and non-obstacles in a hazardous setting. In this study, a number of different strategies and methodologies for autonomous vehicle obstacle detection and avoidance systems, in addition to vehicle formation control strategies are covered. Methods from several important studies have been evaluated systematically. This article provides a concise overview of obstacle detection techniques and avoidance strategies for autonomous vehicles. The outcome represents practical use and demonstrates the effectiveness of the implemented system.},
	urldate = {2024-01-23},
	journal = {2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA)},
	author = {Sheebajoice, C. and Krishna Reddy, Rendla Suresh and Varun Kumar, Theetla Raj and V, Tharun},
	month = mar,
	year = {2023},
	note = {Conference Name: 2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA)
ISBN: 9798350397208
Place: Uttarakhand, India
Publisher: IEEE},
	pages = {620--625},
}

@article{mcleanRisksAssociatedArtificial2023,
	title = {The risks associated with {Artificial} {General} {Intelligence}: {A} systematic review},
	volume = {35},
	issn = {0952-813X},
	shorttitle = {The risks associated with {Artificial} {General} {Intelligence}},
	url = {https://doi.org/10.1080/0952813X.2021.1964003},
	doi = {10.1080/0952813X.2021.1964003},
	abstract = {Artificial General intelligence (AGI) offers enormous benefits for humanity, yet it also poses great risk. The aim of this systematic review was to summarise the peer reviewed literature on the risks associated with AGI. The review followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed eligible for inclusion. Article types included in the review were classified as philosophical discussions, applications of modelling techniques, and assessment of current frameworks and processes in relation to AGI. The review identified a range of risks associated with AGI, including AGI removing itself from the control of human owners/managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks. Several limitations of the AGI literature base were also identified, including a limited number of peer reviewed articles and modelling techniques focused on AGI risk, a lack of specific risk research in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology. Recommendations to address the identified issues with AGI risk research are required to guide AGI design, implementation, and management.},
	number = {5},
	urldate = {2024-01-23},
	journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
	author = {McLean, Scott and Read, Gemma J. M. and Thompson, Jason and Baber, Chris and Stanton, Neville A. and Salmon, Paul M.},
	month = jul,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0952813X.2021.1964003},
	keywords = {Artificial General Intelligence, artificial intelligence, existential threat, risk, safety},
	pages = {649--663},
}

@misc{RisksAssociatedArtificial,
	title = {The risks associated with {Artificial} {General} {Intelligence}: {A} systematic review},
	shorttitle = {The risks associated with {Artificial} {General} {Intelligence}},
	url = {https://www.tandfonline.com/doi/epdf/10.1080/0952813X.2021.1964003?needAccess=true},
	language = {en},
	urldate = {2024-01-23},
	note = {ISSN: 0952-813X},
}

@article{fairbrotherImpactFinancialIncentives2001,
	title = {Impact of {Financial} {Incentives} on {Documented} {Immunization} {Rates} in the {Inner} {City}: {Results} of a {Randomized} {Controlled} {Trial}},
	volume = {1},
	issn = {1530-1567},
	shorttitle = {Impact of {Financial} {Incentives} on {Documented} {Immunization} {Rates} in the {Inner} {City}},
	url = {https://www.sciencedirect.com/science/article/pii/S1530156705600470},
	doi = {10.1367/1539-4409(2001)001<0206:IOFIOD>2.0.CO;2},
	abstract = {Objective.—This study determined the effect of 2 financial incentives—bonus and enhanced fee-for-service—on documented immunization rates during a second period of observation. Methods.—Incentives were given to 57 randomly selected inner-city physicians 4 times at 4-month intervals based on the performance of 50 randomly selected children. Coverage from linked records from all sources was determined for a subsample of children within physician offices. Results.—Up-to-date coverage rates documented in the charts increased significantly for children in the bonus group (49.7\% to 55.6\%; P {\textless} .05) and the enhanced fee-for-service group (50.8\% to 58.2\%; P {\textless} .01) compared with the control group. The number of immunizations given by these physicians did not change significantly, although the number of immunizations given by others and documented by physicians in the bonus group did increase (P {\textless} .05). Up-to-date coverage for all groups increased from 20 to 40 percentage points when immunizations from physician charts were combined with other sources. Conclusions.—Both financial incentives produced a significant increase in coverage levels. Increases were primarily due to better documentation not to better immunizing practices. The financial incentives appeared to provide motivation to physicians but were not sufficient to overcome entrenched behavior patterns. However, true immunization coverage was substantially higher than that documented in the charts.},
	number = {4},
	urldate = {2024-01-23},
	journal = {Ambulatory Pediatrics},
	author = {Fairbrother, Gerry and Siegel, Michele J. and Friedman, Stephen and Kory, Pierre D. and Butts, Gary C.},
	month = jul,
	year = {2001},
	keywords = {bonus, financial incentives, immunization, incentives},
	pages = {206--212},
}

@article{roskiImpactFinancialIncentives2003,
	title = {The impact of financial incentives and a patient registry on preventive care quality: increasing provider adherence to evidence-based smoking cessation practice guidelines☆☆{Surveys} available upon request from corresponding author.},
	volume = {36},
	issn = {0091-7435},
	shorttitle = {The impact of financial incentives and a patient registry on preventive care quality},
	url = {https://www.sciencedirect.com/science/article/pii/S009174350200052X},
	doi = {10.1016/S0091-7435(02)00052-X},
	abstract = {Background
This study tested the effects of two organizational support processes, the provision of financial incentives for superior clinical performance and the availability of a patient (smoker) registry and proactive telephone support system for smoking cessation, on provider adherence to accepted practice guidelines and associated patient outcomes.
Methods
Forty clinics of a large multispecialty medical group practice providing primary care services were randomly allocated to study conditions. Fifteen clinics each were assigned to the experimental conditions “control” (distribution of printed versions of smoking cessation guidelines) and “incentive” (financial incentive pay-out for reaching preset clinical performance targets). Ten clinics were randomized to receive financial incentives combined with access to a centralized patient registry and intervention system (″registry″). Main outcome measures were adherence to smoking cessation clinical practice guidelines and patients’ smoking cessation behaviors.
Results
Patients’ tobacco use status was statistically significant (P {\textless} 0.01) more frequently identified in clinics with the opportunity for incentives and access to a registry than in clinics in the control condition. Patients visiting registry clinics accessed counseling programs statistically significantly more often (P {\textless} 0.001) than patients receiving care in the control condition. Other endpoints did not statistically significantly differ between the experimental conditions.
Conclusions
The impact of financial incentives and a patient registry/intervention system in improving smoking cessation clinical practices and patient behaviors was mixed. Additional research is needed to identify conditions under which such organizational support processes result in significant health care quality improvement and warrant the investment.},
	number = {3},
	urldate = {2024-01-23},
	journal = {Preventive Medicine},
	author = {Roski, Joachim and Jeddeloh, Robert and An, Larry and Lando, Harry and Hannan, Peter and Hall, Carmen and Zhu, Shu-Hong},
	month = mar,
	year = {2003},
	keywords = {Guideline adherence, Practice guidelines, Practice management, Preventive medicine, Quality of health care, Smoking cessation},
	pages = {291--299},
}

@article{hurdalComparisonRuleBasedNeural,
	title = {Comparison of {Rule}-{Based} and {Neural} {Network} {Solutions} for a {Structured} {Selection} {Problem}},
	language = {en},
	journal = {TRANSPORTATION RESEARCH RECORD},
	author = {HuRDAL, BRIAN},
}

@misc{PDFCOMPARISONRULEBASED,
	title = {[{PDF}] {COMPARISON} {OF} {RULE}-{BASED} {AND} {NEURAL} {NETWORK} {SOLUTIONS} {FOR} {A} {STRUCTURED} {SELECTION} {PROBLEM} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/COMPARISON-OF-RULE-BASED-AND-NEURAL-NETWORK-FOR-A-Hajek-Hurdal/ecf88611a9583cf2addbbae432160f2e61e040fd},
	urldate = {2024-01-17},
}

@article{precheltEmpiricalComparisonSeven2000,
	title = {An empirical comparison of seven programming languages},
	volume = {33},
	issn = {00189162},
	url = {http://ieeexplore.ieee.org/document/876288/},
	doi = {10.1109/2.876288},
	language = {en},
	number = {10},
	urldate = {2024-01-17},
	journal = {Computer},
	author = {Prechelt, L.},
	month = oct,
	year = {2000},
	pages = {23--29},
}

@article{benson-tilsenFormalizingConvergentInstrumental,
	title = {Formalizing {Convergent} {Instrumental} {Goals}},
	abstract = {Omohundro has argued that suﬃciently advanced AI systems of any design would, by default, have incentives to pursue a number of instrumentally useful subgoals, such as acquiring more computing power and amassing many resources. Omohundro refers to these as “basic AI drives,” and he, along with Bostrom and others, has argued that this means great care must be taken when designing powerful autonomous systems, because even if they have harmless goals, the side eﬀects of pursuing those goals may be quite harmful. These arguments, while intuitively compelling, are primarily philosophical. In this paper, we provide formal models that demonstrate Omohundro’s thesis, thereby putting mathematical weight behind those intuitive claims.},
	language = {en},
	author = {Benson-Tilsen, Tsvi and Soares, Nate},
}

@misc{NaturalLanguageCode,
	title = {Natural {Language} to {Code}: {How} {Far} {Are} {We}? {\textbar} {Proceedings} of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	url = {https://dl.acm.org/doi/abs/10.1145/3611643.3616323?casa_token=yKkDNvwbUjMAAAAA:gSQSDQ6YJPF6mejJib5uAL4Xox1Qn5IPB0KN-nAe4I13aO7x-rt7q4u3WX_wfRvVxF6v-qxjFy8T7w},
	urldate = {2023-12-22},
}

@misc{ElicitCompilerProgram,
	title = {Elicit: {Is} a compiler a program synthesis system?},
	url = {https://elicit.com/?workflow=table-of-papers&run=afebf81c-e1bd-4ec2-b526-4ef6c417d5b8},
	urldate = {2023-12-22},
}

@misc{AccountDetails,
	title = {Account details},
	url = {https://pay.google.com/gp/w/home/accountdetail?ebaid=AJ9oCCxIGFMT1B6G3ARyjmDPeS3ECbsZLAq0YpMbo4RfsGVWO8T%2BBDMcCEN9e%2BW6AhSlQv9l6mFO&hl=en_GB},
	urldate = {2023-12-09},
}

@article{chelbaOneBillionWord2013,
	title = {One billion word benchmark for measuring progress in statistical language modeling},
	journal = {arXiv preprint arXiv:1312.3005},
	author = {Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
	year = {2013},
}

@inproceedings{nanzComparativeStudyProgramming2015,
	address = {Florence, Italy},
	title = {A {Comparative} {Study} of {Programming} {Languages} in {Rosetta} {Code}},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194625/},
	doi = {10.1109/ICSE.2015.90},
	abstract = {Sometimes debates on programming languages are more religious than scientiﬁc. Questions about which language is more succinct or efﬁcient, or makes developers more productive are discussed with fervor, and their answers are too often based on anecdotes and unsubstantiated beliefs. In this study, we use the largely untapped research potential of Rosetta Code, a code repository of solutions to common programming tasks in various languages, which offers a large data set for analysis. Our study is based on 7’087 solution programs corresponding to 745 tasks in 8 widely used languages representing the major programming paradigms (procedural: C and Go; object-oriented: C\# and Java; functional: F\# and Haskell; scripting: Python and Ruby). Our statistical analysis reveals, most notably, that: functional and scripting languages are more concise than procedural and objectoriented languages; C is hard to beat when it comes to raw speed on large inputs, but performance differences over inputs of moderate size are less pronounced and allow even interpreted languages to be competitive; compiled strongly-typed languages, where more defects can be caught at compile time, are less prone to runtime failures than interpreted or weakly-typed languages. We discuss implications of these results for developers, language designers, and educators.},
	language = {en},
	urldate = {2024-01-17},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Nanz, Sebastian and Furia, Carlo A.},
	month = may,
	year = {2015},
	pages = {778--788},
}

@article{evansLearningExplanatoryRules2018,
	title = {Learning {Explanatory} {Rules} from {Noisy} {Data}},
	volume = {61},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/11172},
	doi = {10.1613/jair.5714},
	abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data--which is not necessarily easily obtained--that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
	language = {en},
	urldate = {2024-01-17},
	journal = {Journal of Artificial Intelligence Research},
	author = {Evans, Richard and Grefenstette, Edward},
	month = jan,
	year = {2018},
	pages = {1--64},
}

@misc{amodeiConcreteProblemsAI2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jul,
	year = {2016},
	note = {arXiv:1606.06565 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{hadfield-menellIncompleteContractingAI2019,
	address = {Honolulu HI USA},
	title = {Incomplete {Contracting} and {AI} {Alignment}},
	isbn = {978-1-4503-6324-2},
	url = {https://dl.acm.org/doi/10.1145/3306618.3314250},
	doi = {10.1145/3306618.3314250},
	language = {en},
	urldate = {2024-01-16},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Hadfield-Menell, Dylan and Hadfield, Gillian K.},
	month = jan,
	year = {2019},
	pages = {417--422},
}

@inproceedings{nestianPerverseIncentiveGeneral2017,
	title = {The {Perverse} {Incentive} – {A} {General} {Concern} {In} {Managerial} {Systems}},
	url = {https://www.semanticscholar.org/paper/The-Perverse-Incentive-%E2%80%93-A-General-Concern-In-Ne%C5%9Ftian/73a3ac9756cc7866e66dcd603d25520074dada13},
	abstract = {The perverse incentive is a wide spread phenomenon reported in various contexts and organizations. Despite this, there are no theoretical approaches from a management perspective. The study aims to make the necessary theoretical conceptual delimitations for clarifying the perverse incentive concept and to describe, for the first time, the main contexts and causes for occurrence of perverse incentive in organisations. This article presents the first general theoretical framework about the determinant mechanisms of perverse incentives in organisations.},
	urldate = {2024-01-15},
	author = {Neştian, Ștefan Andrei},
	year = {2017},
}

@article{pandaAgencyTheoryReview2017,
	title = {Agency theory: {Review} of {Theory} and {Evidence} on {Problems} and {Perspectives}},
	volume = {10},
	issn = {0974-6862},
	shorttitle = {Agency theory},
	url = {https://doi.org/10.1177/0974686217701467},
	doi = {10.1177/0974686217701467},
	abstract = {This article intends to review the theoretical aspects and empirical evidences made on agency theory. It is aimed to explore the main ideas, perspectives, problems and issues related to the agency theory through a literature survey. It discusses the theoretical aspects of agency theory and the various concepts and issues related to it and documents empirical evidences on the mechanisms that diminish the agency cost. The conflict of interest and agency cost arises due to the separation of ownership from control, different risk preferences, information asymmetry and moral hazards. The literatures have cited many solutions like strong ownership control, managerial ownership, independent board members and different committees can be useful in controlling the agency conflict and its cost. This literature survey will enlighten the practitioners and researchers in understanding, analysing the agency problem and will be helpful in mitigating the agency problem.},
	language = {en},
	number = {1},
	urldate = {2024-01-15},
	journal = {Indian Journal of Corporate Governance},
	author = {Panda, Brahmadev and Leepsa, N. M.},
	month = jun,
	year = {2017},
	note = {Publisher: SAGE Publications India},
	pages = {74--95},
}

@article{purushothamBenchmarkingDeepLearning2018,
	title = {Benchmarking deep learning models on large healthcare datasets},
	volume = {83},
	journal = {Journal of Biomedical Informatics},
	author = {Purushotham, Sanjay and Meng, Chuizheng and Che, Zhengping and Liu, Yan},
	month = jul,
	year = {2018},
	note = {Publisher: Elsevier BV},
	pages = {112--134},
}

@article{harutyunyanMultitaskLearningBenchmarking2019,
	title = {Multitask learning and benchmarking with clinical time series data},
	volume = {6},
	number = {1},
	journal = {Scientific Data},
	author = {Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg, Greg and Galstyan, Aram},
	month = jun,
	year = {2019},
	note = {Publisher: Springer Science and Business Media LLC},
}

@misc{shinnReflexionAutonomousAgent2023,
	title = {Reflexion: an autonomous agent with dynamic memory and self-reflection},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	doi = {10.48550/arXiv.2303.11366},
	abstract = {Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective heuristic that enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment. To assess our approach, we evaluate the agent's ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97\% and 51\%, respectively, and provide a discussion on the emergent property of self-reflection.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11366 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chenTeachingLargeLanguage2023,
	title = {Teaching {Large} {Language} {Models} to {Self}-{Debug}},
	url = {http://arxiv.org/abs/2304.05128},
	doi = {10.48550/arXiv.2304.05128},
	abstract = {Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest label by 9\%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Chen, Xinyun and Lin, Maxwell and Schärli, Nathanael and Zhou, Denny},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05128 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{lehmanEvolutionLargeModels2022,
	title = {Evolution through {Large} {Models}},
	url = {http://arxiv.org/abs/2206.08896},
	doi = {10.48550/arXiv.2206.08896},
	abstract = {This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.},
	urldate = {2023-09-06},
	publisher = {arXiv},
	author = {Lehman, Joel and Gordon, Jonathan and Jain, Shawn and Ndousse, Kamal and Yeh, Cathy and Stanley, Kenneth O.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08896 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@misc{fanAutomatedRepairPrograms2023,
	title = {Automated {Repair} of {Programs} from {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2205.10583},
	doi = {10.48550/arXiv.2205.10583},
	abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
	month = jan,
	year = {2023},
	note = {arXiv:2205.10583 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{sabaterEventTransformerSparseaware2022,
	address = {New Orleans, LA, USA},
	title = {Event {Transformer}. {A} sparse-aware solution for efficient event data processing},
	isbn = {978-1-66548-739-9},
	url = {https://ieeexplore.ieee.org/document/9857382/},
	doi = {10.1109/CVPRW56347.2022.00301},
	abstract = {Event cameras are sensors of great interest for many applications that run in low-resource and challenging environments. They log sparse illumination changes with high temporal resolution and high dynamic range, while they present minimal power consumption. However, topperforming methods often ignore specific event-data properties, leading to the development of generic but computationally expensive algorithms. Efforts toward efficient solutions usually do not achieve top-accuracy results for complex tasks. This work proposes a novel framework, Event Transformer (EvT)1, that effectively takes advantage of event-data properties to be highly efficient and accurate. We introduce a new patch-based event representation and a compact transformer-like architecture to process it. EvT is evaluated on different event-based benchmarks for action and gesture recognition. Evaluation results show better or comparable accuracy to the state-of-the-art while requiring significantly less computation resources, which makes EvT able to work with minimal latency both on GPU and CPU.},
	language = {en},
	urldate = {2023-10-31},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Sabater, Alberto and Montesano, Luis and Murillo, Ana C.},
	month = jun,
	year = {2022},
	pages = {2676--2685},
}

@misc{guEfficientlyModelingLong2022,
	title = {Efficiently {Modeling} {Long} {Sequences} with {Structured} {State} {Spaces}},
	url = {http://arxiv.org/abs/2111.00396},
	abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) {\textbackslash}( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) {\textbackslash}), and showed that for appropriate choices of the state matrix {\textbackslash}( A {\textbackslash}), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning {\textbackslash}( A {\textbackslash}) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91{\textbackslash}\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60{\textbackslash}times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Gu, Albert and Goel, Karan and Ré, Christopher},
	month = aug,
	year = {2022},
	note = {arXiv:2111.00396 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{dengExplainableTimeseriesDeep2022,
	title = {Explainable time-series deep learning models for the prediction of mortality, prolonged length of stay and 30-day readmission in intensive care patients},
	volume = {9},
	issn = {2296-858X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9554013/},
	doi = {10.3389/fmed.2022.933037},
	abstract = {Background
In-hospital mortality, prolonged length of stay (LOS), and 30-day readmission are common outcomes in the intensive care unit (ICU). Traditional scoring systems and machine learning models for predicting these outcomes usually ignore the characteristics of ICU data, which are time-series forms. We aimed to use time-series deep learning models with the selective combination of three widely used scoring systems to predict these outcomes.

Materials and methods
A retrospective cohort study was conducted on 40,083 patients in ICU from the Medical Information Mart for Intensive Care-IV (MIMIC-IV) database. Three deep learning models, namely, recurrent neural network (RNN), gated recurrent unit (GRU), and long short-term memory (LSTM) with attention mechanisms, were trained for the prediction of in-hospital mortality, prolonged LOS, and 30-day readmission with variables collected during the initial 24 h after ICU admission or the last 24 h before discharge. The inclusion of variables was based on three widely used scoring systems, namely, APACHE II, SOFA, and SAPS II, and the predictors consisted of time-series vital signs, laboratory tests, medication, and procedures. The patients were randomly divided into a training set (80\%) and a test set (20\%), which were used for model development and model evaluation, respectively. The area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and Brier scores were used to evaluate model performance. Variable significance was identified through attention mechanisms.

Results
A total of 33 variables for 40,083 patients were enrolled for mortality and prolonged LOS prediction and 36,180 for readmission prediction. The rates of occurrence of the three outcomes were 9.74\%, 27.54\%, and 11.79\%, respectively. In each of the three outcomes, the performance of RNN, GRU, and LSTM did not differ greatly. Mortality prediction models, prolonged LOS prediction models, and readmission prediction models achieved AUCs of 0.870 ± 0.001, 0.765 ± 0.003, and 0.635 ± 0.018, respectively. The top significant variables co-selected by the three deep learning models were Glasgow Coma Scale (GCS), age, blood urea nitrogen, and norepinephrine for mortality; GCS, invasive ventilation, and blood urea nitrogen for prolonged LOS; and blood urea nitrogen, GCS, and ethnicity for readmission.

Conclusion
The prognostic prediction models established in our study achieved good performance in predicting common outcomes of patients in ICU, especially in mortality prediction. In addition, GCS and blood urea nitrogen were identified as the most important factors strongly associated with adverse ICU events.},
	urldate = {2023-11-23},
	journal = {Frontiers in Medicine},
	author = {Deng, Yuhan and Liu, Shuang and Wang, Ziyao and Wang, Yuxin and Jiang, Yong and Liu, Baohua},
	month = sep,
	year = {2022},
	pmid = {36250092},
	pmcid = {PMC9554013},
	pages = {933037},
}

@article{saunshiMathematicalExplorationWhy2020,
	title = {A {Mathematical} {Exploration} of {Why} {Language} {Models} {Help} {Solve} {Downstream} {Tasks}},
	url = {https://www.semanticscholar.org/paper/01400290c7db96c4d665d1c29519c42ba47401e0},
	abstract = {Autoregressive language models pretrained on large corpora have been successful at solving downstream tasks, even with zero-shot usage. However, there is little theoretical justification for their success. This paper considers the following questions: (1) Why should learning the distribution of natural language help with downstream classification tasks? (2) Why do features learned using language modeling help solve downstream tasks with linear classifiers? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as next word prediction tasks, thus making language modeling a meaningful pretraining task. For (2), we analyze properties of the cross-entropy objective to show that \${\textbackslash}epsilon\$-optimal language models in cross-entropy (log-perplexity) learn features that are \${\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{{\textbackslash}epsilon\})\$-good on natural linear classification tasks, thus demonstrating mathematically that doing well on language modeling can be beneficial for downstream tasks. We perform experiments to verify assumptions and validate theoretical results. Our theoretical insights motivate a simple alternative to the cross-entropy objective that performs well on some linear classification tasks.},
	urldate = {2023-11-13},
	journal = {ArXiv},
	author = {Saunshi, Nikunj and Malladi, Sadhika and Arora, Sanjeev},
	month = oct,
	year = {2020},
}

@article{mcdermottReproducibilityMachineLearning2021,
	title = {Reproducibility in machine learning for health research: {Still} a ways to go},
	volume = {13},
	number = {586},
	journal = {Science Translational Medicine},
	author = {McDermott, Matthew B. A. and Wang, Shirly and Marinsek, Nikki and Ranganath, Rajesh and Foschini, Luca and Ghassemi, Marzyeh},
	month = mar,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science (AAAS)},
}

@article{guSupervisedLearningPervasive2023,
	title = {Beyond {Supervised} {Learning} for {Pervasive} {Healthcare}},
	journal = {IEEE Reviews in Biomedical Engineering},
	author = {Gu, Xiao and Deligianni, Fani and Han, Jinpei and Liu, Xiangyu and Chen, Wei and Yang, Guang-Zhong and Lo, Benny},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {1--21},
}

@misc{sunEnhancingChainofThoughtsPrompting2023,
	title = {Enhancing {Chain}-of-{Thoughts} {Prompting} with {Iterative} {Bootstrapping} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.11657},
	doi = {10.48550/arXiv.2304.11657},
	abstract = {Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability across varying levels of difficulty. Experimental results indicate that Iter-CoT exhibits superiority, achieving competitive performance across three distinct reasoning tasks on ten datasets.},
	urldate = {2023-12-12},
	publisher = {arXiv},
	author = {Sun, Jiashuo and Luo, Yi and Gong, Yeyun and Lin, Chen and Shen, Yelong and Guo, Jian and Duan, Nan},
	month = oct,
	year = {2023},
	note = {arXiv:2304.11657 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhouLanguageAgentTree2023,
	title = {Language {Agent} {Tree} {Search} {Unifies} {Reasoning} {Acting} and {Planning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2310.04406},
	abstract = {While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4{\textbackslash}\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04406 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{roland31415NeurIPS2023Institutions2023,
	type = {Reddit {Post}},
	title = {[{D}] {NeurIPS} 2023 {Institutions} {Ranking}},
	url = {www.reddit.com/r/MachineLearning/comments/185pdax/d_neurips_2023_institutions_ranking/},
	urldate = {2023-11-28},
	journal = {r/MachineLearning},
	author = {Roland31415},
	month = nov,
	year = {2023},
}

@misc{tuGeneralistBiomedicalAI2023,
	title = {Towards {Generalist} {Biomedical} {AI}},
	url = {http://arxiv.org/abs/2307.14334},
	abstract = {Medicine is inherently multimodal, with rich data modalities spanning text, imaging, genomics, and more. Generalist biomedical artificial intelligence (AI) systems that flexibly encode, integrate, and interpret this data at scale can potentially enable impactful applications ranging from scientific discovery to care delivery. To enable the development of these models, we first curate MultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses 14 diverse tasks such as medical question answering, mammography and dermatology image interpretation, radiology report generation and summarization, and genomic variant calling. We then introduce Med-PaLM Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights. Med-PaLM M reaches performance competitive with or exceeding the state of the art on all MultiMedBench tasks, often surpassing specialist models by a wide margin. We also report examples of zero-shot generalization to novel medical concepts and tasks, positive transfer learning across tasks, and emergent zero-shot medical reasoning. To further probe the capabilities and limitations of Med-PaLM M, we conduct a radiologist evaluation of model-generated (and human) chest X-ray reports and observe encouraging performance across model scales. In a side-by-side ranking on 246 retrospective chest X-rays, clinicians express a pairwise preference for Med-PaLM M reports over those produced by radiologists in up to 40.50\% of cases, suggesting potential clinical utility. While considerable work is needed to validate these models in real-world use cases, our results represent a milestone towards the development of generalist biomedical AI systems.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Tu, Tao and Azizi, Shekoofeh and Driess, Danny and Schaekermann, Mike and Amin, Mohamed and Chang, Pi-Chuan and Carroll, Andrew and Lau, Chuck and Tanno, Ryutaro and Ktena, Ira and Mustafa, Basil and Chowdhery, Aakanksha and Liu, Yun and Kornblith, Simon and Fleet, David and Mansfield, Philip and Prakash, Sushant and Wong, Renee and Virmani, Sunny and Semturs, Christopher and Mahdavi, S. Sara and Green, Bradley and Dominowska, Ewa and Arcas, Blaise Aguera y and Barral, Joelle and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Singhal, Karan and Florence, Pete and Karthikesalingam, Alan and Natarajan, Vivek},
	month = jul,
	year = {2023},
	note = {arXiv:2307.14334 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bubeckSparksArtificialGeneral2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhouComprehensiveSurveyPretrained2023,
	title = {A {Comprehensive} {Survey} on {Pretrained} {Foundation} {Models}: {A} {History} from {BERT} to {ChatGPT}},
	shorttitle = {A {Comprehensive} {Survey} on {Pretrained} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2302.09419},
	abstract = {Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and Peng, Hao and Li, Jianxin and Wu, Jia and Liu, Ziwei and Xie, Pengtao and Xiong, Caiming and Pei, Jian and Yu, Philip S. and Sun, Lichao},
	month = may,
	year = {2023},
	note = {arXiv:2302.09419 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{jiAIAlignmentComprehensive2024,
	title = {{AI} {Alignment}: {A} {Comprehensive} {Survey}},
	shorttitle = {{AI} {Alignment}},
	url = {http://arxiv.org/abs/2310.19852},
	doi = {10.48550/arXiv.2310.19852},
	abstract = {AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices. We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Ng, Kwan Yee and Dai, Juntao and Pan, Xuehai and O'Gara, Aidan and Lei, Yingshan and Xu, Hua and Tse, Brian and Fu, Jie and McAleer, Stephen and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
	month = jan,
	year = {2024},
	note = {arXiv:2310.19852 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{longFairnessMachineLearning2021,
	title = {Fairness in machine learning: against false positive rate equality as a measure of fairness},
	volume = {19},
	issn = {1740-4681, 1745-5243},
	shorttitle = {Fairness in machine learning},
	url = {http://arxiv.org/abs/2007.02890},
	doi = {10.1163/17455243-20213439},
	abstract = {As machine learning informs increasingly consequential decisions, different metrics have been proposed for measuring algorithmic bias or unfairness. Two popular fairness measures are calibration and equality of false positive rate. Each measure seems intuitively important, but notably, it is usually impossible to satisfy both measures. For this reason, a large literature in machine learning speaks of a fairness tradeoff between these two measures. This framing assumes that both measures are, in fact, capturing something important. To date, philosophers have not examined this crucial assumption, and examined to what extent each measure actually tracks a normatively important property. This makes this inevitable statistical conflict, between calibration and false positive rate equality, an important topic for ethics. In this paper, I give an ethical framework for thinking about these measures and argue that, contrary to initial appearances, false positive rate equality does not track anything about fairness, and thus sets an incoherent standard for evaluating the fairness of algorithms.},
	number = {1},
	urldate = {2024-01-15},
	journal = {Journal of Moral Philosophy},
	author = {Long, Robert},
	month = nov,
	year = {2021},
	note = {arXiv:2007.02890 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	pages = {49--78},
}

@misc{JoelGrusFizz,
	title = {Joel {Grus} – {Fizz} {Buzz} in {Tensorflow}},
	url = {https://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/},
	urldate = {2024-01-12},
}

@incollection{samarinLinguaFranca1987,
	title = {Lingua {Franca}},
	isbn = {978-3-11-009694-1},
	url = {https://tspace.library.utoronto.ca/handle/1807/70765},
	abstract = {It is on the basis of function alone that a language is considered to be a lingua franca, by which term is designated any lingual medium of communication between people of different mother tongues, for whom it is a second language. Applicable to all situations where linguistic communication is difficult or impossible, it applies as well to areas characterized by extreme dialect differences as to those with different languages in the normal sense. Any form of language can be used with this purpose. Natural languages spoken beyond their native boundaries are the best known examples, but dialects have spread in the same manner. Examples of the latter are Fijian, based on the Bauan dialect, and Yawelmani, the latter used amongst speakers of Yokuts on the Tule River Reservation in California. Such languages of common intercourse become established informally, as in any instance of second-language acquisition, or formally in some context of education. In the latter case the languages are usually written, exemplified by Latin, a vital lingua franca up to the end of the Middle Ages, and Arabic throughout the Islamicized world to this very day. Writing as well as specialized function may also have been responsible for the longevity for Aramaic as a common medium of intercommunication in the Near East, from at least the 6th century B. C.},
	language = {en\_ca},
	urldate = {2024-01-11},
	publisher = {Walter de Gruyter},
	author = {Samarin, William J.},
	year = {1987},
	note = {Accepted: 2016-01-06T17:46:37Z},
}

@misc{radfordRobustSpeechRecognition2022,
	title = {Robust {Speech} {Recognition} via {Large}-{Scale} {Weak} {Supervision}},
	url = {http://arxiv.org/abs/2212.04356},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04356 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{makkeInterpretableScientificDiscovery2022,
	title = {Interpretable {Scientific} {Discovery} with {Symbolic} {Regression}: {A} {Review}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Interpretable {Scientific} {Discovery} with {Symbolic} {Regression}},
	url = {https://arxiv.org/abs/2211.10873},
	doi = {10.48550/ARXIV.2211.10873},
	abstract = {Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery method, achieving significant advances in various application domains ranging from fundamental to applied sciences. This survey presents a structured and comprehensive overview of symbolic regression methods and discusses their strengths and limitations.},
	urldate = {2023-12-26},
	author = {Makke, Nour and Chawla, Sanjay},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Physical sciences, High Energy Physics - Phenomenology (hep-ph), Machine Learning (cs.LG)},
}

@book{clarke2001SpaceOdyssey2016,
	title = {2001: a space odyssey},
	isbn = {0-14-311157-4},
	publisher = {Penguin},
	author = {Clarke, Arthur C.},
	year = {2016},
}

@article{ellisonHaveNoMouth1967,
	title = {I {Have} {No} {Mouth} and {I} {Must} {Scream}." {I} {Have} {No} {Mouth} and {I} {Must} {Scream}},
	journal = {New York: Pyramid},
	author = {Ellison, Harlan},
	year = {1967},
	pages = {22--42},
}

@book{cocksMaking2001Space2010,
	title = {The {Making} of 2001: {A} {Space} {Odyssey}},
	isbn = {0-307-75760-9},
	publisher = {Modern library},
	author = {Cocks, Jay},
	year = {2010},
}

@book{jonesColossus2019,
	title = {Colossus},
	isbn = {1-4732-2630-9},
	publisher = {Hachette UK},
	author = {Jones, Dennis Feltham},
	year = {2019},
}

@inproceedings{gulwaniDimensionsProgramSynthesis2010,
	address = {Hagenberg Austria},
	title = {Dimensions in program synthesis},
	isbn = {978-1-4503-0132-9},
	url = {https://dl.acm.org/doi/10.1145/1836089.1836091},
	doi = {10.1145/1836089.1836091},
	language = {en},
	urldate = {2023-12-26},
	booktitle = {Proceedings of the 12th international {ACM} {SIGPLAN} symposium on {Principles} and practice of declarative programming},
	publisher = {ACM},
	author = {Gulwani, Sumit},
	month = jul,
	year = {2010},
	pages = {13--24},
}

@article{jenkinsLogicNamedJoe1946,
	title = {A {Logic} {Named} {Joe}},
	volume = {37},
	number = {1},
	journal = {Astounding},
	author = {Jenkins, Will F.},
	year = {1946},
	pages = {139--155},
}

@incollection{cunninghamSupervisedLearning2008,
	address = {Berlin, Heidelberg},
	series = {Cognitive {Technologies}},
	title = {Supervised {Learning}},
	isbn = {978-3-540-75171-7},
	url = {https://doi.org/10.1007/978-3-540-75171-7_2},
	abstract = {Supervised learning accounts for a lot of research activity in machine learning and many supervised learning techniques have found application in the processing of multimedia content. The defining characteristic of supervised learning is the availability of annotated training data. The name invokes the idea of a ‘supervisor’ that instructs the learning system on the labels to associate with training examples. Typically these labels are class labels in classification problems. Supervised learning algorithms induce models from these training data and these models can be used to classify other unlabelled data. In this chapter we ground or analysis of supervised learning on the theory of risk minimization. We provide an overview of support vector machines and nearest neighbour classifiers{\textasciitilde}– probably the two most popular supervised learning techniques employed in multimedia research.},
	language = {en},
	urldate = {2023-12-25},
	booktitle = {Machine {Learning} {Techniques} for {Multimedia}: {Case} {Studies} on {Organization} and {Retrieval}},
	publisher = {Springer},
	author = {Cunningham, Pádraig and Cord, Matthieu and Delany, Sarah Jane},
	editor = {Cord, Matthieu and Cunningham, Pádraig},
	year = {2008},
	doi = {10.1007/978-3-540-75171-7_2},
	keywords = {Competence Model, Ensemble Member, Neural Network Ensemble, Random Forest, Support Vector Machine},
	pages = {21--49},
}

@book{gulwaniProgrammingExamplesandIts2016,
	title = {Programming by {Examples}-and its applications in {Data} {Wrangling}. {Dependable} {Software} {Systems} {Engineering} 45 (2016), 137–158},
	author = {Gulwani, Sumit},
	year = {2016},
}

@book{hofstadterGodelEscherBach1999,
	title = {Gödel, {Escher}, {Bach}: an eternal golden braid},
	isbn = {0-465-02656-7},
	publisher = {Basic books},
	author = {Hofstadter, Douglas R.},
	year = {1999},
}

@misc{QuinePageSelfreproducing,
	title = {The {Quine} {Page} (self-reproducing code)},
	url = {https://www.nyx.net/~gthompso/quine.htm},
	urldate = {2023-12-22},
}

@misc{MichaelWeharBrief,
	title = {Michael {Wehar} - {A} {Brief} {Guide} to {Self}-{Referential} {Programs}},
	url = {http://michaelwehar.com/quines/},
	urldate = {2023-12-22},
}

@misc{koziolekLLMbasedControlCode2023,
	title = {{LLM}-based {Control} {Code} {Generation} using {Image} {Recognition}},
	url = {http://arxiv.org/abs/2311.10401},
	abstract = {LLM-based code generation could save significant manual efforts in industrial automation, where control engineers manually produce control logic for sophisticated production processes. Previous attempts in control logic code generation lacked methods to interpret schematic drawings from process engineers. Recent LLMs now combine image recognition, trained domain knowledge, and coding skills. We propose a novel LLM-based code generation method that generates IEC 61131-3 Structure Text control logic source code from Piping-and-Instrumentation Diagrams (P\&IDs) using image recognition. We have evaluated the method in three case study with industrial P\&IDs and provide first evidence on the feasibility of such a code generation besides experiences on image recognition glitches.},
	urldate = {2023-12-22},
	publisher = {arXiv},
	author = {Koziolek, Heiko and Koziolek, Anne},
	month = nov,
	year = {2023},
	note = {arXiv:2311.10401 [cs]},
	keywords = {Computer Science - Software Engineering, D.2.2},
}

@misc{chatgptmodderChatgptCanNow2023,
	type = {Reddit {Post}},
	title = {Chatgpt can now code from a whiteboard drawing. {Wow}},
	url = {www.reddit.com/r/ChatGPT/comments/16tso7j/chatgpt_can_now_code_from_a_whiteboard_drawing_wow/},
	urldate = {2023-12-22},
	journal = {r/ChatGPT},
	author = {ChatgptModder},
	month = sep,
	year = {2023},
}

@book{halbertProgrammingExample1984,
	title = {Programming by example},
	isbn = {9798662044210},
	publisher = {University of California, Berkeley},
	author = {Halbert, Daniel Conrad},
	year = {1984},
}

@article{leDeepLearningSource2020,
	title = {Deep {Learning} for {Source} {Code} {Modeling} and {Generation}: {Models}, {Applications}, and {Challenges}},
	volume = {53},
	issn = {0360-0300},
	shorttitle = {Deep {Learning} for {Source} {Code} {Modeling} and {Generation}},
	url = {https://dl.acm.org/doi/10.1145/3383458},
	doi = {10.1145/3383458},
	abstract = {Deep Learning (DL) techniques for Natural Language Processing have been evolving remarkably fast. Recently, the DL advances in language modeling, machine translation, and paragraph understanding are so prominent that the potential of DL in Software Engineering cannot be overlooked, especially in the field of program learning. To facilitate further research and applications of DL in this field, we provide a comprehensive review to categorize and investigate existing DL methods for source code modeling and generation. To address the limitations of the traditional source code models, we formulate common program learning tasks under an encoder-decoder framework. After that, we introduce recent DL mechanisms suitable to solve such problems. Then, we present the state-of-the-art practices and discuss their challenges with some recommendations for practitioners and researchers as well.},
	number = {3},
	urldate = {2023-12-22},
	journal = {ACM Computing Surveys},
	author = {Le, Triet H. M. and Chen, Hao and Babar, Muhammad Ali},
	month = jun,
	year = {2020},
	keywords = {Deep learning, big code, source code generation, source code modeling},
	pages = {62:1--62:38},
}

@inproceedings{wangNaturalLanguageCode2023,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {Natural {Language} to {Code}: {How} {Far} {Are} {We}?},
	isbn = {9798400703270},
	shorttitle = {Natural {Language} to {Code}},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616323},
	doi = {10.1145/3611643.3616323},
	abstract = {A longstanding dream in software engineering research is to devise effective approaches for automating development tasks based on developers' informally-specified intentions. Such intentions are generally in the form of natural language descriptions. In recent literature, a number of approaches have been proposed to automate tasks such as code search and even code generation based on natural language inputs. While these approaches vary in terms of technical designs, their objective is the same: transforming a developer's intention into source code. The literature, however, lacks a comprehensive understanding towards the effectiveness of existing techniques as well as their complementarity to each other. We propose to fill this gap through a large-scale empirical study where we systematically evaluate natural language to code techniques. Specifically, we consider six state-of-the-art techniques targeting code search, and four targeting code generation. Through extensive evaluations on a dataset of 22K+ natural language queries, our study reveals the following major findings: (1) code search techniques based on model pre-training are so far the most effective while code generation techniques can also provide promising results; (2) complementarity widely exists among the existing techniques; and (3) combining the ten techniques together can enhance the performance for 35\% compared with the most effective standalone technique. Finally, we propose a post-processing strategy to automatically integrate different techniques based on their generated code. Experimental results show that our devised strategy is both effective and extensible.},
	urldate = {2023-12-22},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Shangwen and Geng, Mingyang and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Li, Li and Bissyandé, Tegawendé F. and Mao, Xiaoguang},
	month = nov,
	year = {2023},
	keywords = {Code Generation, Code Search, Pre-Training Technique},
	pages = {375--387},
}

@article{campbellAutomatedCodingQuest2020,
	title = {Automated {Coding}: {The} {Quest} to {Develop} {Programs} {That} {Write} {Programs}},
	volume = {53},
	issn = {0018-9162, 1558-0814},
	shorttitle = {Automated {Coding}},
	url = {https://ieeexplore.ieee.org/document/8996112/},
	doi = {10.1109/MC.2019.2957958},
	abstract = {Computer scientists have long pondered the possibility of crafting systems capable of creating programs directly from human intent. Recent developments in neural networks are closing the gap between fuzzy user objectives and concrete automated code generation.},
	number = {2},
	urldate = {2023-12-22},
	journal = {Computer},
	author = {Campbell, Mark},
	month = feb,
	year = {2020},
	pages = {80--82},
}

@inproceedings{penjamDeductiveInductiveMethods2003,
	title = {Deductive and {Inductive} {Methods} for {Program} {Synthesis}},
	url = {https://www.semanticscholar.org/paper/Deductive-and-Inductive-Methods-for-Program-Penjam-Sanko/16eb051512dbf5e99d2900afefe4f4ce51559475},
	abstract = {The paper discusses simple functional constraint networks and a value propagation method for program construction. Structural synthesis of programs is described as an example of deductive approach to program construction. An inductive method for program synthesis utilizing stochastic optimization algorithms is introduced to complement value propagation techniques.},
	urldate = {2023-12-22},
	author = {Penjam, J. and Sanko, Jelena},
	year = {2003},
}

@misc{patrickmckenzie[@patio11]GlibLineHave2023,
	type = {Tweet},
	title = {A glib line {I} have said for years: the first {AI} designed to disemploy programmers was called a compiler. {It} worked very well and caused explosive growth in programmer employment.},
	url = {https://twitter.com/patio11/status/1639787447667048449},
	language = {en},
	urldate = {2023-12-22},
	journal = {Twitter},
	author = {{Patrick McKenzie [@patio11]}},
	month = mar,
	year = {2023},
}

@misc{liventsevTaxonomyAutomaticProgramming2023,
	title = {Taxonomy of {Automatic} {Programming}},
	copyright = {All rights reserved},
	url = {https://vadim.me/publications/taxonomy/},
	abstract = {Automatic programming is a pursuit of a seemingly clear goal: let’s make computers program themselves! As usual, that nasty pal of ours is in the details: are compilers automatic programming? They do generate code automatically, and modern compilers tend to utilize sophisticated machine learning to do so [Leather, Cummins]. So, in the interest of unambiguous nomenclature, here’s a brief summary of various tasks that tend to get lumped under the fuzzy umbrella terms of automatic programming and program synthesis.},
	language = {en},
	urldate = {2023-12-22},
	author = {Liventsev, Vadim},
	month = may,
	year = {2023},
	note = {Section: publications},
}

@article{zhu2019grammarcnn,
	title = {A grammar-based structural {CNN} decoder for code generation},
	volume = {33},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4686},
	doi = {10.1609/aaai.v33i01.33017055},
	number = {01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sun, Zeyu and Zhu, Qihao and Mou, Lili and Xiong, Yingfei and Li, Ge and Zhang, Lu},
	month = jul,
	year = {2019},
	note = {Abstract Note: \&lt;p\&gt;Code generation maps a program description to executable source code in a programming language. Existing approaches mainly rely on a recurrent neural network (RNN) as the decoder. However, we find that a program contains significantly more tokens than a natural language sentence, and thus it may be inappropriate for RNN to capture such a long sequence. In this paper, we propose a grammar-based structural convolutional neural network (CNN) for code generation. Our model generates a program by predicting the grammar rules of the programming language; we design several CNN modules, including the tree-based convolution and pre-order convolution, whose information is further aggregated by dedicated attentive pooling layers. Experimental results on the HearthStone benchmark dataset show that our CNN code generator significantly outperforms the previous state-of-the-art method by 5 percentage points; additional experiments on several semantic parsing tasks demonstrate the robustness of our model. We also conduct in-depth ablation test to better understand each component of our model.\&lt;/p\&gt;},
	pages = {7055--7062},
}

@article{kingma2014adam,
	title = {Adam: {A} method for stochastic optimization},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

@inproceedings{psb2,
	title = {{PSB2}: the second program synthesis benchmark suite},
	booktitle = {Proceedings of the genetic and evolutionary computation conference},
	author = {Helmuth, Thomas and Kelly, Peter},
	year = {2021},
	pages = {785--794},
}

@article{jiang2021ast,
	title = {An {AST} structure enhanced decoder for code generation},
	volume = {30},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Jiang, Hui and Song, Linfeng and Ge, Yubin and Meng, Fandong and Yao, Junfeng and Su, Jinsong},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {468--476},
}

@article{zhang2015tree,
	title = {Tree recurrent neural networks with application to language modeling},
	journal = {CoRR, abs/1511.00060},
	author = {Zhang, Xingxing and Lu, Liang and Lapata, Mirella},
	year = {2015},
}

@inproceedings{latentspaceopt,
	title = {Program synthesis as latent continuous optimization: evolutionary search in neural embeddings},
	booktitle = {Proceedings of the 2020 genetic and evolutionary computation conference},
	author = {Liskowski, Paweł and Krawiec, Krzysztof and Toklu, Nihat Engin and Swan, Jerry},
	year = {2020},
	pages = {359--367},
}

@inproceedings{autoenc-gp,
	title = {Why is auto-encoding difficult for genetic programming?},
	booktitle = {European conference on genetic programming},
	publisher = {Springer},
	author = {McDermott, James},
	year = {2019},
	pages = {131--145},
}

@article{autoencoders,
	title = {Autoencoders},
	journal = {arXiv preprint arXiv:2003.05991},
	author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
	year = {2020},
}

@inproceedings{denoising-autoenc-gp,
	title = {Using denoising autoencoder genetic programming to control exploration and exploitation in search},
	booktitle = {European conference on genetic programming (part of {EvoStar})},
	publisher = {Springer},
	author = {Wittenberg, David},
	year = {2022},
	pages = {102--117},
}

@article{gpt-neo,
	title = {Gpt-neox-20b: {An} open-source autoregressive language model},
	journal = {arXiv preprint arXiv:2204.06745},
	author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and {others}},
	year = {2022},
}

@article{codebert,
	title = {Codebert: {A} pre-trained model for programming and natural languages},
	journal = {arXiv preprint arXiv:2002.08155},
	author = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and {others}},
	year = {2020},
}

@article{codegen,
	title = {Codegen: {An} open large language model for code with multi-turn program synthesis},
	journal = {ArXiv preprint, abs/2203.13474},
	author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
	year = {2022},
}

@article{foundation-models,
	title = {On the opportunities and risks of foundation models},
	journal = {arXiv preprint arXiv:2108.07258},
	author = {Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and {others}},
	year = {2021},
}

@phdthesis{grammar-vae,
	type = {phd},
	title = {Program synthesis and vulnerability injection using a {Grammar} {VAE}},
	school = {Boston University},
	author = {Kosta Jr, Leonard R},
	year = {2019},
}

@article{nevergrad,
	title = {Black-box optimization revisited: {Improving} algorithm selection wizards through massive benchmarking},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Meunier, Laurent and Rakotoarison, Herilalaina and Wong, Pak Kan and Roziere, Baptiste and Rapin, Jeremy and Teytaud, Olivier and Moreau, Antoine and Doerr, Carola},
	year = {2021},
	note = {Publisher: IEEE},
}

@inproceedings{ast,
	title = {Understanding source code evolution using abstract syntax tree matching},
	booktitle = {Proceedings of the 2005 international workshop on {Mining} software repositories},
	author = {Neamtiu, Iulian and Foster, Jeffrey S and Hicks, Michael},
	year = {2005},
	pages = {1--5},
}

@article{genimp2,
	title = {Automatic software repair: a bibliography},
	volume = {abs/1807.00515},
	url = {http://arxiv.org/abs/1807.00515},
	journal = {CoRR},
	author = {Monperrus, Martin},
	year = {2018},
	note = {arXiv: 1807.00515
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 13 Aug 2018 16:48:52 +0200},
}

@article{tiobe2017tiobe,
	title = {Tiobe index},
	journal = {Retrieved from Tiobe Index: https://www. tiobe. com/tiobe-index},
	author = {TIOBE, I},
	year = {2017},
}

@inproceedings{kaszuba2021automated,
	title = {Automated development of latent representations for optimization of sequences using autoencoders},
	booktitle = {2021 {IEEE} congress on evolutionary computation ({CEC})},
	publisher = {IEEE},
	author = {Kaszuba, Piotr and Komosinski, Maciej and Mensfelt, Agnieszka},
	year = {2021},
	pages = {1123--1130},
}

@article{lozoya2021commit2vec,
	title = {Commit2vec: {Learning} distributed representations of code changes},
	volume = {2},
	number = {3},
	journal = {SN Computer Science},
	author = {Lozoya, Rocío Cabrera and Baumann, Arnaud and Sabetta, Antonino and Bezzi, Michele},
	year = {2021},
	note = {Publisher: Springer},
	pages = {1--16},
}

@article{fredman1986pairing,
	title = {The pairing heap: {A} new form of self-adjusting heap},
	volume = {1},
	number = {1-4},
	journal = {Algorithmica. An International Journal in Computer Science},
	author = {Fredman, Michael L and Sedgewick, Robert and Sleator, Daniel D and Tarjan, Robert E},
	year = {1986},
	note = {Publisher: Springer},
	pages = {111--129},
}

@inproceedings{henkel2018code,
	title = {Code vectors: {Understanding} programs through embedded abstracted symbolic traces},
	booktitle = {Proceedings of the 2018 26th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering},
	author = {Henkel, Jordan and Lahiri, Shuvendu K and Liblit, Ben and Reps, Thomas},
	year = {2018},
	pages = {163--174},
}

@inproceedings{lynch2020grammarsvaes,
	address = {Cham},
	title = {Program synthesis in a continuous space using grammars and variational autoencoders},
	isbn = {978-3-030-58115-2},
	abstract = {An important but elusive goal of computer scientists is the automatic creation of computer programs given only input and output examples. We present a novel approach to program synthesis based on the combination of grammars, generative neural models, and evolutionary algorithms. Programs are described by sequences of productions sampled from a Backus-Naur form grammar. A sequence-to-sequence Variational Autoencoder (VAE) is trained to embed randomly sampled programs in a continuous space – the VAE's encoder maps a sequence of productions (a program) to a point z in the latent space, and the VAE's decoder reconstructs the program given z. After the VAE has converged, we can engage the decoder as a generative model that maps locations in the latent space to executable programs. Hence, an Evolutionary Algorithm can be employed to search for a vector z (and its corresponding program) that solves the synthesis task. Experiments on the program synthesis benchmark suite suggest that the proposed approach is competitive with tree-based GP and PushGP. Crucially, code can be synthesised in any programming language.},
	booktitle = {Parallel problem solving from nature – {PPSN} {XVI}},
	publisher = {Springer International Publishing},
	author = {Lynch, David and McDermott, James and O'Neill, Michael},
	editor = {Bäck, Thomas and Preuss, Mike and Deutz, André and Wang, Hao and Doerr, Carola and Emmerich, Michael and Trautmann, Heike},
	year = {2020},
	pages = {33--47},
}

@article{chung2014empirical,
	title = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
	journal = {arXiv preprint arXiv:1412.3555},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	year = {2014},
}

@article{alon2018general,
	title = {A general path-based representation for predicting program properties},
	volume = {53},
	number = {4},
	journal = {ACM SIGPLAN Notices},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	year = {2018},
	note = {Publisher: ACM New York, NY, USA},
	pages = {404--419},
}

@article{chen2018tree,
	title = {Tree-to-tree neural networks for program translation},
	journal = {arXiv preprint arXiv:1802.03691},
	author = {Chen, Xinyun and Liu, Chang and Song, Dawn},
	year = {2018},
}

@inproceedings{kramer2000gaps,
	title = {Gaps: a genetic programming system},
	booktitle = {Proceedings 24th annual international computer software and applications conference. {COMPSAC2000}},
	publisher = {IEEE},
	author = {Kramer, Michael D and Zhang, Du},
	year = {2000},
	pages = {614--619},
}

@book{turing1950computing,
	title = {Computing machinery and intelligence},
	publisher = {MIT Press Cambridge, MA},
	author = {Turing, Alan M and Haugeland, J},
	year = {1950},
}

@article{spector2002genetic,
	title = {Genetic programming and autoconstructive evolution with the push programming language},
	volume = {3},
	number = {1},
	journal = {Genetic Programming and Evolvable Machines},
	author = {Spector, Lee and Robinson, Alan},
	year = {2002},
	note = {Publisher: Springer},
	pages = {7--40},
}

@misc{shin2019program,
	title = {Program synthesis and semantic parsing with learned code idioms},
	author = {Shin, Richard and Allamanis, Miltiadis and Brockschmidt, Marc and Polozov, Oleksandr},
	year = {2019},
	note = {arXiv: 1906.10816 [cs.LG]},
}

@misc{hajipour2019samplefix,
	title = {{SampleFix}: {Learning} to correct programs by sampling diverse fixes},
	author = {Hajipour, Hossein and Bhattacharya, Apratim and Fritz, Mario},
	year = {2019},
	note = {arXiv: 1906.10502 [cs.SE]},
}

@article{Luan_2019,
	title = {Aroma: code recommendation via structural code search},
	volume = {3},
	issn = {2475-1421},
	url = {http://dx.doi.org/10.1145/3360578},
	doi = {10.1145/3360578},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
	month = oct,
	year = {2019},
	note = {Publisher: Association for Computing Machinery (ACM)},
	pages = {1--28},
}

@misc{li2018code,
	title = {Code completion with neural attention and pointer networks},
	author = {Li, Jian and Wang, Yue and Lyu, Michael R. and King, Irwin},
	year = {2018},
	note = {arXiv: 1711.09573 [cs.CL]},
}

@inproceedings{collobert2008unified,
	title = {A unified architecture for natural language processing: {Deep} neural networks with multitask learning},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	author = {Collobert, Ronan and Weston, Jason},
	year = {2008},
	pages = {160--167},
}

@article{lecun1998gradient,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	year = {1998},
	note = {Publisher: Ieee},
	pages = {2278--2324},
}

@misc{bunel2017learning,
	title = {Learning to superoptimize programs},
	author = {Bunel, Rudy and Desmaison, Alban and Kumar, M. Pawan and Torr, Philip H. S. and Kohli, Pushmeet},
	year = {2017},
	note = {arXiv: 1611.01787 [cs.LG]},
}

@inproceedings{le2018maximal,
	title = {Maximal divergence sequential autoencoder for binary software vulnerability detection},
	url = {https://openreview.net/forum?id=ByloIiCqYQ},
	booktitle = {International conference on learning representations},
	author = {Le, Tue and Nguyen, Tuan and Le, Trung and Phung, Dinh and Montague, Paul and Vel, Olivier De and Qu, Lizhen},
	year = {2019},
}

@misc{dauphin2017language,
	title = {Language modeling with gated convolutional networks},
	author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
	year = {2017},
	note = {arXiv: 1612.08083 [cs.CL]},
}

@inproceedings{kayhan2020translation,
	title = {On translation invariance in cnns: {Convolutional} layers can exploit absolute spatial location},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Kayhan, Osman Semih and Gemert, Jan C van},
	year = {2020},
	pages = {14274--14285},
}

@article{ravanelli2018light,
	title = {Light gated recurrent units for speech recognition},
	volume = {2},
	number = {2},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Ravanelli, Mirco and Brakel, Philemon and Omologo, Maurizio and Bengio, Yoshua},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {92--102},
}

@inproceedings{tang2015document,
	title = {Document modeling with gated recurrent neural network for sentiment classification},
	booktitle = {Proceedings of the 2015 conference on empirical methods in natural language processing},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	year = {2015},
	pages = {1422--1432},
}

@incollection{NEURIPS2019_9015,
	title = {{PyTorch}: {An} imperative style, high-performance deep learning library},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in neural information processing systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@article{kitaev2020reformer,
	title = {Reformer: {The} efficient transformer},
	journal = {arXiv preprint arXiv:2001.04451},
	author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
	year = {2020},
}

@article{gruber2020gru,
	title = {Are {GRU} cells more specific and {LSTM} cells more sensitive in motive classification of text?},
	volume = {3},
	journal = {Frontiers in artificial intelligence},
	author = {Gruber, Nicole and Jockisch, Alfred},
	year = {2020},
	note = {Publisher: Frontiers},
	pages = {40},
}

@article{chan2015listen,
	title = {Listen, attend and spell},
	journal = {arXiv preprint arXiv:1508.01211},
	author = {Chan, William and Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol},
	year = {2015},
}

@article{shen2016empirical,
	title = {Empirical evaluation of {RNN} architectures on sentence classification task},
	journal = {arXiv preprint arXiv:1609.09171},
	author = {Shen, Lei and Zhang, Junlin},
	year = {2016},
}

@misc{fabius2015variational,
	title = {Variational recurrent auto-encoders},
	author = {Fabius, Otto and van Amersfoort, Joost R.},
	year = {2015},
	note = {arXiv: 1412.6581 [stat.ML]},
}

@article{long2019preventing,
	title = {Preventing posterior collapse in sequence {VAEs} with pooling},
	journal = {arXiv preprint arXiv:1911.03976},
	author = {Long, Teng and Cao, Yanshuai and Cheung, Jackie Chi Kit},
	year = {2019},
}

@article{chen2016latent,
	title = {Latent attention for if-then program synthesis},
	journal = {arXiv preprint arXiv:1611.01867},
	author = {Chen, Xinyun and Liu, Chang and Shin, Richard and Song, Dawn and Chen, Mingcheng},
	year = {2016},
}

@inproceedings{pennington2014glove,
	title = {Glove: {Global} vectors for word representation},
	booktitle = {Proceedings of the 2014 conference on empirical methods in natural language processing ({EMNLP})},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
	year = {2014},
	pages = {1532--1543},
}

@article{chechik2010large,
	title = {Large scale online learning of image similarity through ranking.},
	volume = {11},
	number = {3},
	journal = {Journal of Machine Learning Research},
	author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
	year = {2010},
}

@misc{kao2020comparison,
	title = {A comparison of pooling methods on {LSTM} models for rare acoustic event classification},
	author = {Kao, Chieh-Chi and Sun, Ming and Wang, Weiran and Wang, Chao},
	year = {2020},
	note = {arXiv: 2002.06279 [eess.AS]},
}

@inproceedings{winata2018attention,
	title = {Attention-based lstm for psychological stress detection from spoken language using distant supervision},
	booktitle = {2018 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	publisher = {IEEE},
	author = {Winata, Genta Indra and Kampman, Onno Pepijn and Fung, Pascale},
	year = {2018},
	pages = {6204--6208},
}

@inproceedings{graves2013hybrid,
	title = {Hybrid speech recognition with deep bidirectional {LSTM}},
	booktitle = {2013 {IEEE} workshop on automatic speech recognition and understanding},
	publisher = {IEEE},
	author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
	year = {2013},
	pages = {273--278},
}

@inproceedings{wang2021mulcode,
	title = {{MulCode}: {A} multi-task learning approach for source code understanding},
	booktitle = {2021 {IEEE} international conference on software analysis, evolution and reengineering ({SANER})},
	publisher = {IEEE},
	author = {Wang, Deze and Yu, Yue and Li, Shanshan and Dong, Wei and Wang, Ji and Qing, Liao},
	year = {2021},
	pages = {48--59},
}

@misc{deng2018latent,
	title = {Latent alignment and variational attention},
	author = {Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander M.},
	year = {2018},
	note = {arXiv: 1807.03756 [stat.ML]},
}

@article{mou2016transferable,
	title = {How transferable are neural networks in nlp applications?},
	journal = {arXiv preprint arXiv:1603.06111},
	author = {Mou, Lili and Meng, Zhao and Yan, Rui and Li, Ge and Xu, Yan and Zhang, Lu and Jin, Zhi},
	year = {2016},
}

@misc{trevett2019the,
	title = {The effectiveness of pre-trained code embeddings},
	url = {https://openreview.net/forum?id=H1glKiCqtm},
	author = {Trevett, Ben and Reay, Donald and Taylor, N. K.},
	year = {2019},
}

@article{holtzman2019curious,
	title = {The curious case of neural text degeneration},
	journal = {arXiv preprint arXiv:1904.09751},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	year = {2019},
}

@article{hinton2015distilling,
	title = {Distilling the knowledge in a neural network},
	journal = {arXiv preprint arXiv:1503.02531},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	year = {2015},
}

@inproceedings{furcy2005limited,
	title = {Limited discrepancy beam search},
	booktitle = {{IJCAI}},
	author = {Furcy, David and Koenig, Sven},
	year = {2005},
}

@article{bengio2015scheduled,
	title = {Scheduled sampling for sequence prediction with recurrent neural networks},
	journal = {arXiv preprint arXiv:1506.03099},
	author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
	year = {2015},
}

@article{bengio1994learning,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	number = {2},
	journal = {IEEE transactions on neural networks},
	author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
	year = {1994},
	note = {Publisher: IEEE},
	pages = {157--166},
}

@misc{kim2014convolutional,
	title = {Convolutional neural networks for sentence classification},
	author = {Kim, Yoon},
	year = {2014},
	note = {arXiv: 1408.5882 [cs.CL]},
}

@article{holtzman2018learning,
	title = {Learning to write with cooperative discriminators},
	journal = {arXiv preprint arXiv:1805.06087},
	author = {Holtzman, Ari and Buys, Jan and Forbes, Maxwell and Bosselut, Antoine and Golub, David and Choi, Yejin},
	year = {2018},
}

@article{fan2018hierarchical,
	title = {Hierarchical neural story generation},
	journal = {arXiv preprint arXiv:1805.04833},
	author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
	year = {2018},
}

@inproceedings{tufano2019learning,
	title = {On learning meaningful code changes via neural machine translation},
	booktitle = {2019 {IEEE}/{ACM} 41st international conference on software engineering ({ICSE})},
	publisher = {IEEE},
	author = {Tufano, Michele and Pantiuchina, Jevgenija and Watson, Cody and Bavota, Gabriele and Poshyvanyk, Denys},
	year = {2019},
	pages = {25--36},
}

@misc{he2019lagging,
	title = {Lagging inference networks and posterior collapse in variational autoencoders},
	author = {He, Junxian and Spokoyny, Daniel and Neubig, Graham and Berg-Kirkpatrick, Taylor},
	year = {2019},
	note = {arXiv: 1901.05534 [cs.LG]},
}

@article{fu2019cyclical,
	title = {Cyclical annealing schedule: {A} simple approach to mitigating kl vanishing},
	journal = {arXiv preprint arXiv:1903.10145},
	author = {Fu, Hao and Li, Chunyuan and Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
	year = {2019},
}

@article{bowman2015generating,
	title = {Generating sentences from a continuous space},
	journal = {arXiv preprint arXiv:1511.06349},
	author = {Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
	year = {2015},
}

@inproceedings{papineni2002bleu,
	title = {Bleu: a method for automatic evaluation of machine translation},
	booktitle = {Proceedings of the 40th annual meeting of the {Association} for {Computational} {Linguistics}},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2002},
	pages = {311--318},
}

@article{elman1990finding,
	title = {Finding structure in time},
	volume = {14},
	number = {2},
	journal = {Cognitive science},
	author = {Elman, Jeffrey L},
	year = {1990},
	note = {Publisher: Wiley Online Library},
	pages = {179--211},
}

@misc{zhao2018infovae,
	title = {{InfoVAE}: {Information} maximizing variational autoencoders},
	author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
	year = {2018},
	note = {arXiv: 1706.02262 [cs.LG]},
}

@article{higgins2016beta,
	title = {beta-vae: {Learning} basic visual concepts with a constrained variational framework},
	journal = {-},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	year = {2016},
}

@misc{grave2017efficient,
	title = {Efficient softmax approximation for {GPUs}},
	author = {Grave, Edouard and Joulin, Armand and Cissé, Moustapha and Grangier, David and Jégou, Hervé},
	year = {2017},
	note = {arXiv: 1609.04309 [cs.CL]},
}

@misc{tokui2016reparameterization,
	title = {Reparameterization trick for discrete variables},
	author = {Tokui, Seiya and sato, Issei},
	year = {2016},
	note = {arXiv: 1611.01239 [stat.ML]},
}

@article{jensen1906fonctions,
	title = {Sur les fonctions convexes et les inégalités entre les valeurs moyennes},
	volume = {30},
	journal = {Acta mathematica},
	author = {Jensen, Johan Ludwig William Valdemar and {others}},
	year = {1906},
	note = {Publisher: Institut Mittag-Leffler},
	pages = {175--193},
}

@article{kullback1951information,
	title = {On information and sufficiency},
	volume = {22},
	number = {1},
	journal = {The annals of mathematical statistics},
	author = {Kullback, Solomon and Leibler, Richard A},
	year = {1951},
	note = {Publisher: JSTOR},
	pages = {79--86},
}

@article{kingma2019introduction,
	title = {An introduction to variational autoencoders},
	journal = {arXiv preprint arXiv:1906.02691},
	author = {Kingma, Diederik P and Welling, Max},
	year = {2019},
}

@article{bengio2013generalized,
	title = {Generalized denoising auto-encoders as generative models},
	journal = {arXiv preprint arXiv:1305.6663},
	author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
	year = {2013},
}

@inproceedings{rifai2011higher,
	title = {Higher order contractive auto-encoder},
	booktitle = {Joint {European} conference on machine learning and knowledge discovery in databases},
	publisher = {Springer},
	author = {Rifai, Salah and Mesnil, Grégoire and Vincent, Pascal and Muller, Xavier and Bengio, Yoshua and Dauphin, Yann and Glorot, Xavier},
	year = {2011},
	pages = {645--660},
}

@inproceedings{deng2013sparse,
	title = {Sparse autoencoder-based feature transfer learning for speech emotion recognition},
	booktitle = {2013 humaine association conference on affective computing and intelligent interaction},
	publisher = {IEEE},
	author = {Deng, Jun and Zhang, Zixing and Marchi, Erik and Schuller, Björn},
	year = {2013},
	pages = {511--516},
}

@article{jean2014using,
	title = {On using very large target vocabulary for neural machine translation},
	journal = {arXiv preprint arXiv:1412.2007},
	author = {Jean, Sébastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
	year = {2014},
}

@inproceedings{takahashi2019variational,
	title = {Variational autoencoder with implicit optimal priors},
	volume = {33},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Takahashi, Hiroshi and Iwata, Tomoharu and Yamanaka, Yuki and Yamada, Masanori and Yagi, Satoshi},
	year = {2019},
	note = {Issue: 01},
	pages = {5066--5073},
}

@inproceedings{vincent2008extracting,
	title = {Extracting and composing robust features with denoising autoencoders},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year = {2008},
	pages = {1096--1103},
}

@techreport{rumelhart1985learning,
	title = {Learning internal representations by error propagation},
	institution = {California Univ San Diego La Jolla Inst for Cognitive Science},
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year = {1985},
}

@incollection{bottou2010large,
	title = {Large-scale machine learning with stochastic gradient descent},
	booktitle = {Proceedings of {COMPSTAT}'2010},
	publisher = {Springer},
	author = {Bottou, Léon},
	year = {2010},
	pages = {177--186},
}

@article{kingma2013auto,
	title = {Auto-encoding variational bayes},
	journal = {arXiv preprint arXiv:1312.6114},
	author = {Kingma, Diederik P and Welling, Max},
	year = {2013},
}

@article{shin2016tree,
	title = {Tree-structured variational autoencoder},
	journal = {-},
	author = {Shin, Richard and Alemi, Alexander A and Irving, Geoffrey and Vinyals, Oriol},
	year = {2016},
}

@article{gu2018top,
	title = {Top-down tree structured decoding with syntactic connections for neural machine translation and parsing},
	journal = {arXiv preprint arXiv:1809.01854},
	author = {Gū, Jetic and Shavarani, Hassan S and Sarkar, Anoop},
	year = {2018},
}

@article{Schmidhuber_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {0893-6080},
	url = {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
	doi = {10.1016/j.neunet.2014.09.003},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	note = {Publisher: Elsevier BV},
	pages = {85--117},
}

@article{yin2018structvae,
	title = {Structvae: {Tree}-structured latent variable models for semi-supervised semantic parsing},
	journal = {arXiv preprint arXiv:1806.07832},
	author = {Yin, Pengcheng and Zhou, Chunting and He, Junxian and Neubig, Graham},
	year = {2018},
}

@inproceedings{goller1996learning,
	title = {Learning task-dependent distributed representations by backpropagation through structure},
	volume = {1},
	booktitle = {Proceedings of international conference on neural networks ({ICNN}'96)},
	publisher = {IEEE},
	author = {Goller, Christoph and Kuchler, Andreas},
	year = {1996},
	pages = {347--352},
}

@article{sutskever2014sequence,
	title = {Sequence to sequence learning with neural networks},
	journal = {arXiv preprint arXiv:1409.3215},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	year = {2014},
}

@inproceedings{alvarezmelis2017tree,
	title = {Tree-structured decoding with doubly-recurrent neural networks},
	booktitle = {Proceedings of the international conference on learning representations ({ICLR})},
	author = {Alvarez-Melis, David and Jaakkola, Tommi S},
	year = {2017},
}

@inproceedings{kusner2017grammar,
	title = {Grammar variational autoencoder},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Kusner, Matt J and Paige, Brooks and Hernández-Lobato, José Miguel},
	year = {2017},
	pages = {1945--1954},
}

@article{tai2015improved,
	title = {Improved semantic representations from tree-structured long short-term memory networks},
	journal = {arXiv preprint arXiv:1503.00075},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
	year = {2015},
}

@inproceedings{kovalenko2019pathminer,
	title = {{PathMiner}: a library for mining of path-based representations of code},
	booktitle = {Proceedings of the 16th international conference on mining software repositories},
	publisher = {IEEE Press},
	author = {Kovalenko, Vladimir and Bogomolov, Egor and Bryksin, Timofey and Bacchelli, Alberto},
	year = {2019},
	pages = {13--17},
}

@misc{dong2016language,
	title = {Language to logical form with neural attention},
	author = {Dong, Li and Lapata, Mirella},
	year = {2016},
	note = {arXiv: 1601.01280 [cs.CL]},
}

@article{kiperwasser2016easy,
	title = {Easy-first dependency parsing with hierarchical tree {LSTMs}},
	volume = {4},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kiperwasser, Eliyahu and Goldberg, Yoav},
	year = {2016},
	note = {Publisher: MIT Press},
	pages = {445--461},
}

@inproceedings{white2015toward,
	title = {Toward deep learning software repositories},
	booktitle = {2015 {IEEE}/{ACM} 12th working conference on mining software repositories},
	publisher = {IEEE},
	author = {White, Martin and Vendome, Christopher and Linares-Vásquez, Mario and Poshyvanyk, Denys},
	year = {2015},
	pages = {334--345},
}

@article{li2015hierarchical,
	title = {A hierarchical neural autoencoder for paragraphs and documents},
	journal = {arXiv preprint arXiv:1506.01057},
	author = {Li, Jiwei and Luong, Minh-Thang and Jurafsky, Dan},
	year = {2015},
}

@article{alon2019structural,
	title = {Structural language models for any-code generation},
	journal = {arXiv preprint arXiv:1910.00577},
	author = {Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
	year = {2019},
}

@article{alon2018code2seq,
	title = {code2seq: {Generating} sequences from structured representations of code},
	journal = {arXiv preprint arXiv:1808.01400},
	author = {Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
	year = {2018},
}

@inproceedings{hindle2012naturalness,
	title = {On the naturalness of software},
	booktitle = {2012 34th international conference on software engineering ({ICSE})},
	publisher = {IEEE},
	author = {Hindle, Abram and Barr, Earl T and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
	year = {2012},
	pages = {837--847},
}

@inproceedings{eggermont2004genetic,
	title = {Genetic programming for data classification: {Partitioning} the search space},
	booktitle = {Proceedings of the 2004 {ACM} symposium on {Applied} computing},
	author = {Eggermont, Jeroen and Kok, Joost N and Kosters, Walter A},
	year = {2004},
	pages = {1001--1005},
}

@article{nguyen2019hybrid,
	title = {A hybrid genetic programming algorithm for automated design of dispatching rules},
	volume = {27},
	number = {3},
	journal = {Evolutionary computation},
	author = {Nguyen, Su and Mei, Yi and Xue, Bing and Zhang, Mengjie},
	year = {2019},
	note = {Publisher: MIT Press},
	pages = {467--496},
}

@misc{heartpole,
	title = {{HeartPole}: {A} transparent task for reinforcement learning in {Healthcare}},
	copyright = {All rights reserved},
	url = {https://github.com/vadim0x60/heartpole/blob/master/HeartPole_abstract.pdf},
	author = {Liventsev, Vadim},
}

@article{banditsexplo,
	title = {Bandit problems and the exploration/exploitation tradeoff},
	volume = {2},
	number = {1},
	journal = {IEEE Transactions on evolutionary computation},
	author = {Macready, William G and Wolpert, David H},
	year = {1998},
	note = {Publisher: IEEE},
	pages = {2--22},
}

@article{banditsolutions,
	title = {Algorithms for multi-armed bandit problems},
	volume = {abs/1402.6028},
	url = {http://arxiv.org/abs/1402.6028},
	journal = {CoRR},
	author = {Kuleshov, Volodymyr and Precup, Doina},
	year = {2014},
	note = {arXiv: 1402.6028
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 13 Aug 2018 16:48:13 +0200},
}

@article{banditproblem,
	title = {Some aspects of the sequential design of experiments},
	volume = {58},
	number = {5},
	journal = {Bulletin of the American Mathematical Society},
	author = {Robbins, Herbert},
	year = {1952},
	note = {Publisher: Citeseer},
	pages = {527--535},
}

@article{neuroevolution,
	title = {Neuroevolution: from architectures to learning},
	volume = {1},
	number = {1},
	journal = {Evolutionary intelligence},
	author = {Floreano, Dario and Dürr, Peter and Mattiussi, Claudio},
	year = {2008},
	note = {Publisher: Springer},
	pages = {47--62},
}

@article{exploration,
	title = {Exploring parameter space in reinforcement learning},
	volume = {1},
	number = {1},
	journal = {Paladyn : journal of behavioral robotics},
	author = {Rückstiess, Thomas and Sehnke, Frank and Schaul, Tom and Wierstra, Daan and Sun, Yi and Schmidhuber, Jürgen},
	year = {2010},
	note = {Publisher: Springer},
	pages = {14--24},
}

@incollection{rlevolution,
	address = {Berlin, Heidelberg},
	title = {Evolutionary computation for reinforcement learning},
	isbn = {978-3-642-27645-3},
	url = {https://doi.org/10.1007/978-3-642-27645-3_10},
	abstract = {Algorithms for evolutionary computation, which simulate the process of natural selection to solve optimization problems, are an effective tool for discovering high-performing reinforcement-learning policies. Because they can automatically find good representations, handle continuous action spaces, and cope with partial observability, evolutionary reinforcement-learning approaches have a strong empirical track record, sometimes significantly outperforming temporal-difference methods. This chapter surveys research on the application of evolutionary computation to reinforcement learning, overviewing methods for evolving neural-network topologies and weights, hybrid methods that also use temporal-difference methods, coevolutionary methods for multi-agent settings, generative and developmental systems, and methods for on-line evolutionary reinforcement learning.},
	booktitle = {Reinforcement learning: {State}-of-the-art},
	publisher = {Springer Berlin Heidelberg},
	author = {Whiteson, Shimon},
	editor = {Wiering, Marco and van Otterlo, Martijn},
	year = {2012},
	doi = {10.1007/978-3-642-27645-3_10},
	pages = {325--355},
}

@inproceedings{bugfixing,
	title = {A systematic study of automated program repair: {Fixing} 55 out of 105 bugs for \$8 each},
	booktitle = {2012 34th international conference on software engineering ({ICSE})},
	publisher = {IEEE},
	author = {Le Goues, Claire and Dewey-Vogt, Michael and Forrest, Stephanie and Weimer, Westley},
	year = {2012},
	pages = {3--13},
}

@phdthesis{votingsystems,
	type = {phd},
	title = {Voting systems: {From} method to algorithm},
	school = {California State University Channel Islands},
	author = {Devlin, Christopher R},
}

@inproceedings{grammargp,
	title = {Grammatically-based genetic programming},
	author = {Whigham, Peter A and {others}},
}

@inproceedings{semparsing1,
	title = {A syntactic neural model for general-purpose code generation},
	booktitle = {Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: {Long} papers)},
	author = {Yin, Pengcheng and Neubig, Graham},
	year = {2017},
	pages = {440--450},
}

@inproceedings{structural,
	title = {Structural language models of code},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
	year = {2020},
	pages = {245--256},
}

@misc{geneticvsneural,
	title = {Algorithm synthesis: {Deep} learning and genetic programming},
	url = {http://iao.hfuu.edu.cn/blogs/33-algorithm-synthesis-deep-learning-and-genetic-programming},
}

@inproceedings{flash2,
	title = {Flashextract: {A} framework for data extraction by examples},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} conference on programming language design and implementation},
	author = {Le, Vu and Gulwani, Sumit},
	year = {2014},
	pages = {542--553},
}

@book{ptolemy,
	title = {The almagest},
	publisher = {Encyclopaedia Britannica},
	author = {Ptolemaeus, Claudius and Copernicus, Nicolaus and Kepler, Johannes and tr Wallis, Charles Glenn},
	year = {1952},
}

@article{deepcoder,
	title = {{DeepCoder}: {Learning} to write programs},
	volume = {abs/1611.01989},
	url = {http://arxiv.org/abs/1611.01989},
	journal = {CoRR},
	author = {Balog, Matej and Gaunt, Alexander L. and Brockschmidt, Marc and Nowozin, Sebastian and Tarlow, Daniel},
	year = {2016},
	note = {arXiv: 1611.01989
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 13 Aug 2018 16:47:48 +0200},
}

@inproceedings{flash1,
	title = {Robustfill: {Neural} program learning under noisy i/o},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-rahman and Kohli, Pushmeet},
	year = {2017},
	pages = {990--998},
}

@article{occam,
	title = {The myth of occam's razor},
	volume = {27},
	number = {107},
	journal = {Mind; a quarterly review of psychology and philosophy},
	author = {Thorburn, William M},
	year = {1918},
	note = {Publisher: JSTOR},
	pages = {345--353},
}

@article{prooftheoretic,
	title = {From program verification to program synthesis},
	volume = {45},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/1707801.1706337},
	doi = {10.1145/1707801.1706337},
	abstract = {This paper describes a novel technique for the synthesis of imperative programs. Automated program synthesis has the potential to make programming and the design of systems easier by allowing programs to be specified at a higher-level than executable code. In our approach, which we call proof-theoretic synthesis, the user provides an input-output functional specification, a description of the atomic operations in the programming language, and a specification of the synthesized program's looping structure, allowed stack space, and bound on usage of certain operations. Our technique synthesizes a program, if there exists one, that meets the input-output specification and uses only the given resources.The insight behind our approach is to interpret program synthesis as generalized program verification, which allows us to bring verification tools and techniques to program synthesis. Our synthesis algorithm works by creating a program with unknown statements, guards, inductive invariants, and ranking functions. It then generates constraints that relate the unknowns and enforces three kinds of requirements: partial correctness, loop termination, and well-formedness conditions on program guards. We formalize the requirements that program verification tools must meet to solve these constraint and use tools from prior work as our synthesizers.We demonstrate the feasibility of the proposed approach by synthesizing programs in three different domains: arithmetic, sorting, and dynamic programming. Using verification tools that we previously built in the VS3 project we are able to synthesize programs for complicated arithmetic algorithms including Strassen's matrix multiplication and Bresenham's line drawing; several sorting algorithms; and several dynamic programming algorithms. For these programs, the median time for synthesis is 14 seconds, and the ratio of synthesis to verification time ranges between 1x to 92x (with an median of 7x), illustrating the potential of the approach.},
	number = {1},
	journal = {SIGPLAN Not.},
	author = {Srivastava, Saurabh and Gulwani, Sumit and Foster, Jeffrey S.},
	month = jan,
	year = {2010},
	note = {Number of pages: 14
Place: New York, NY, USA
Publisher: Association for Computing Machinery
tex.issue\_date: January 2010},
	keywords = {proof-theoretic program synthesis, verification},
	pages = {313--326},
}

@misc{gym-sepsis,
	title = {{GYMIC}: {An} {OpenAI} gym environment for simulating sepsis treatment for {ICU} patients},
	url = {https://github.com/akiani/rlsepsis234/blob/master/writeup.pdf},
	author = {Kiani, Amirhossein and Ding, Tianli and Henderson, Peter},
}

@article{robotrl,
	title = {Reinforcement learning in robotics: {A} survey},
	volume = {32},
	url = {https://doi.org/10.1177/0278364913495721},
	doi = {10.1177/0278364913495721},
	abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
	number = {11},
	journal = {The International Journal of Robotics Research},
	author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
	year = {2013},
	note = {tex.eprint: https://doi.org/10.1177/0278364913495721},
	pages = {1238--1274},
}

@inproceedings{hearthstone,
	title = {Latent predictor networks for code generation},
	booktitle = {Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: {Long} papers)},
	author = {Ling, Wang and Blunsom, Phil and Grefenstette, Edward and Hermann, Karl Moritz and Kočiskỳ, Tomáš and Wang, Fumin and Senior, Andrew},
	year = {2016},
	pages = {599--609},
}

@article{flashfill,
	title = {Automating string processing in spreadsheets using input-output examples},
	volume = {46},
	number = {1},
	journal = {ACM Sigplan Notices},
	author = {Gulwani, Sumit},
	year = {2011},
	note = {Publisher: ACM New York, NY, USA},
	pages = {317--330},
}

@article{random2,
	title = {Liveness-driven random program generation},
	volume = {abs/1709.04421},
	url = {http://arxiv.org/abs/1709.04421},
	journal = {CoRR},
	author = {Barany, Gergö},
	year = {2017},
	note = {arXiv: 1709.04421
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 13 Aug 2018 16:47:16 +0200},
}

@article{random1,
	title = {Random testing for {C} and {C}++ compilers with {YARPGen}},
	volume = {4},
	url = {https://doi.org/10.1145/3428264},
	doi = {10.1145/3428264},
	abstract = {Compilers should not crash and they should not miscompile applications. Random testing is an effective method for finding compiler bugs that have escaped other kinds of testing. This paper presents Yet Another Random Program Generator (YARPGen), a random test-case generator for C and C++ that we used to find and report more than 220 bugs in GCC, LLVM, and the Intel® C++ Compiler. Our research contributions include a method for generating expressive programs that avoid undefined behavior without using dynamic checks, and generation policies, a mechanism for increasing diversity of generated code and for triggering more optimizations. Generation policies decrease the testing time to find hard-to-trigger compiler bugs and, for the kinds of scalar optimizations YARPGen was designed to stress-test, increase the number of times these optimizations are applied by the compiler by an average of 20\% for LLVM and 40\% for GCC. We also created tools for automating most of the common tasks related to compiler fuzzing; these tools are also useful for fuzzers other than ours.},
	number = {OOPSLA},
	journal = {Proc. ACM Program. Lang.},
	author = {Livinskii, Vsevolod and Babokin, Dmitry and Regehr, John},
	month = nov,
	year = {2020},
	note = {Number of pages: 25
Place: New York, NY, USA
Publisher: Association for Computing Machinery
tex.articleno: 196
tex.issue\_date: November 2020},
	keywords = {automated testing, compiler defect, compiler testing, random program generation, random testing},
}

@article{metainduction,
	title = {Neural program meta-induction},
	volume = {abs/1710.04157},
	url = {http://arxiv.org/abs/1710.04157},
	journal = {CoRR},
	author = {Devlin, Jacob and Bunel, Rudy and Singh, Rishabh and Hausknecht, Matthew J. and Kohli, Pushmeet},
	year = {2017},
	note = {arXiv: 1710.04157
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Mon, 13 Aug 2018 16:48:46 +0200},
}

@article{kolmogorov,
	title = {Three approaches to the quantitative definition ofinformation'},
	volume = {1},
	number = {1},
	journal = {Problems of information transmission},
	author = {Kolmogorov, Andrei N},
	year = {1965},
	pages = {1--7},
}

@book{massiveparallelism,
	edition = {1},
	title = {Models of massive parallelism: {Analysis} of cellular automata and neural networks},
	isbn = {3-642-77907-7},
	abstract = {This textbook provides an introduction to the fundamental models of massively parallel computation, the most important technique for high-performance computing. It presents a coherent exposition of analytic methods and results for the exploration and understanding of cellular automata and discrete neural networks as computational and dynamical systems. The book will be useful also as a reference manual to the scattered literature in the field. Each chapter includes a separate bibliography, as well as pointers to historically relevant papers, and gives exercise problems for the reader.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Garzon, Max},
	year = {2012},
}

@book{mythicalmanmonth,
	title = {The mythical man-month: essays on software engineering},
	publisher = {Pearson Education},
	author = {Brooks Jr, Frederick P},
	year = {1995},
}

@article{deap,
	title = {{DEAP}: {Evolutionary} algorithms made easy},
	volume = {13},
	journal = {Journal of Machine Learning Research},
	author = {Fortin, Félix-Antoine and De Rainville, François-Michel and Gardner, Marc-André and Parizeau, Marc and {Christian Gagné}},
	month = jul,
	year = {2012},
	pages = {2171--2175},
}

@book{evocritique,
	title = {A critique of the theory of evolution},
	publisher = {Princeton University Press},
	author = {Morgan, Thomas Hunt},
	year = {1916},
}

@book{computation,
	title = {Theory of computation},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Brainerd, Walter S and Landweber, Lawrence H},
	year = {1974},
}

@article{David2020Evaluating,
	title = {Evaluating {Progress} on {Machine} {Learning} for {Longitudinal} {Electronic} {Healthcare} {Data}},
	journal = {ArXiv},
	author = {{David Bellamy} and {Leo Celi} and {Andrew L. Beam}},
	year = {2020},
}

@article{perezAttentionTuringComplete,
	title = {Attention is {Turing} {Complete}},
	abstract = {Alternatives to recurrent neural networks, in particular, architectures based on self-attention, are gaining momentum for processing input sequences. In spite of their relevance, the computational properties of such networks have not yet been fully explored. We study the computational power of the Transformer, one of the most paradigmatic architectures exemplifying self-attention. We show that the Transformer with hard-attention is Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. Our study also reveals some minimal sets of elements needed to obtain this completeness result.},
	language = {en},
	author = {Perez, Jorge and Barcelo, Pablo and Marinkovic, Javier},
	keywords = {arbitrary precision, neural networks, self-attention, transformers, turing completeness},
}

@article{Kathrin2022Benchmark,
	title = {Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals},
	journal = {ArXiv},
	author = {{Kathrin Blagec} and {Jakob Kraiger} and {Wolfgang Frühwirt} and {Matthias Samwald}},
	year = {2022},
}

@misc{zelikmanSelfTaughtOptimizerSTOP2023,
	title = {Self-{Taught} {Optimizer} ({STOP}): {Recursively} {Self}-{Improving} {Code} {Generation}},
	shorttitle = {Self-{Taught} {Optimizer} ({STOP})},
	url = {http://arxiv.org/abs/2310.02304},
	abstract = {Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our proof-of-concept experiments, is capable of writing code that can call itself to improve itself. We critically consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zelikman, Eric and Lorch, Eliana and Mackey, Lester and Kalai, Adam Tauman},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02304 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xiaConversationalAutomatedProgram2023,
	title = {Conversational {Automated} {Program} {Repair}},
	url = {http://arxiv.org/abs/2301.13246},
	doi = {10.48550/arXiv.2301.13246},
	abstract = {Automated Program Repair (APR) can help developers automatically generate patches for bugs. Due to the impressive performance obtained using Large Pre-Trained Language Models (LLMs) on many code related tasks, researchers have started to directly use LLMs for APR. However, prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases. To address these limitations, we propose conversational APR, a new paradigm for program repair that alternates between patch generation and validation in a conversational manner. In conversational APR, we iteratively build the input to the model by combining previously generated patches with validation feedback. As such, we leverage the long-term context window of LLMs to not only avoid generating previously incorrect patches but also incorporate validation feedback to help the model understand the semantic meaning of the program under test. We evaluate 10 different LLM including the newly developed ChatGPT model to demonstrate the improvement of conversational APR over the prior LLM for APR approach.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Xia, Chunqiu Steven and Zhang, Lingming},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13246 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{jiangSelfEvolveCodeEvolution2023,
	title = {{SelfEvolve}: {A} {Code} {Evolution} {Framework} via {Large} {Language} {Models}},
	shorttitle = {{SelfEvolve}},
	url = {http://arxiv.org/abs/2306.02907},
	doi = {10.48550/arXiv.2306.02907},
	abstract = {Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called {\textbackslash}autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, {\textbackslash}autoknow{\textasciitilde}obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, {\textbackslash}autoknow{\textasciitilde}asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate {\textbackslash}autoknow{\textasciitilde}on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that {\textbackslash}autoknow{\textasciitilde}outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of {\textbackslash}autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that {\textbackslash}autoknow{\textasciitilde}can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Jiang, Shuyang and Wang, Yuhao and Wang, Yu},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02907 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{dongSelfcollaborationCodeGeneration2023,
	title = {Self-collaboration {Code} {Generation} via {ChatGPT}},
	url = {http://arxiv.org/abs/2304.07590},
	doi = {10.48550/arXiv.2304.07590},
	abstract = {Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLMs act as distinct ``experts'', each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three ChatGPT roles (i.e., analyst, coder, and tester) responsible for software development's analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9\%-47.1\% Pass@1 compared to direct code generation, achieving state-of-the-art performance and even surpassing GPT-4. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex real-world tasks that are not readily solved by direct code generation, as evidenced in case study.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge},
	month = may,
	year = {2023},
	note = {arXiv:2304.07590 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{zhangAutomaticChainThought2022,
	title = {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03493},
	doi = {10.48550/arXiv.2210.03493},
	abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{wangSelfInstructAligningLanguage2023,
	title = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	doi = {10.48550/arXiv.2212.10560},
	abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = may,
	year = {2023},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{prasadGrIPSGradientfreeEditbased2023,
	title = {{GrIPS}: {Gradient}-free, {Edit}-based {Instruction} {Search} for {Prompting} {Large} {Language} {Models}},
	shorttitle = {{GrIPS}},
	url = {http://arxiv.org/abs/2203.07281},
	doi = {10.48550/arXiv.2203.07281},
	abstract = {Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instruction-only prompts and instruction + k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GrIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy. Our code is available at: https://github.com/archiki/GrIPS},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Prasad, Archiki and Hase, Peter and Zhou, Xiang and Bansal, Mohit},
	month = apr,
	year = {2023},
	note = {arXiv:2203.07281 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{gouCRITICLargeLanguage2023,
	title = {{CRITIC}: {Large} {Language} {Models} {Can} {Self}-{Correct} with {Tool}-{Interactive} {Critiquing}},
	shorttitle = {{CRITIC}},
	url = {http://arxiv.org/abs/2305.11738},
	doi = {10.48550/arXiv.2305.11738},
	abstract = {Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Yang, Yujiu and Duan, Nan and Chen, Weizhu},
	month = sep,
	year = {2023},
	note = {arXiv:2305.11738 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{lightmanLetVerifyStep2023,
	title = {Let's {Verify} {Step} by {Step}},
	url = {http://arxiv.org/abs/2305.20050},
	doi = {10.48550/arXiv.2305.20050},
	abstract = {In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78\% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
	month = may,
	year = {2023},
	note = {arXiv:2305.20050 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{saparovLanguageModelsAre2023,
	title = {Language {Models} {Are} {Greedy} {Reasoners}: {A} {Systematic} {Formal} {Analysis} of {Chain}-of-{Thought}},
	shorttitle = {Language {Models} {Are} {Greedy} {Reasoners}},
	url = {http://arxiv.org/abs/2210.01240},
	doi = {10.48550/arXiv.2210.01240},
	abstract = {Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Saparov, Abulhair and He, He},
	month = mar,
	year = {2023},
	note = {arXiv:2210.01240 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yaoTreeThoughtsDeliberate2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	doi = {10.48550/arXiv.2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = dec,
	year = {2023},
	note = {arXiv:2305.10601 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhouLargeLanguageModels2023,
	title = {Large {Language} {Models} {Are} {Human}-{Level} {Prompt} {Engineers}},
	url = {http://arxiv.org/abs/2211.01910},
	doi = {10.48550/arXiv.2211.01910},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = mar,
	year = {2023},
	note = {arXiv:2211.01910 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liThinkOutsideCode2023,
	title = {Think {Outside} the {Code}: {Brainstorming} {Boosts} {Large} {Language} {Models} in {Code} {Generation}},
	shorttitle = {Think {Outside} the {Code}},
	url = {http://arxiv.org/abs/2305.10679},
	doi = {10.48550/arXiv.2305.10679},
	abstract = {Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50\% increase in the pass@\$k\$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that our framework boosts the ability of ChatGPT to a level comparable to that of human programmers.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Li, Xin-Ye and Xue, Jiang-Tian and Xie, Zheng and Li, Ming},
	month = may,
	year = {2023},
	note = {arXiv:2305.10679 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{zhangSelfEditFaultAwareCode2023,
	title = {Self-{Edit}: {Fault}-{Aware} {Code} {Editor} for {Code} {Generation}},
	shorttitle = {Self-{Edit}},
	url = {http://arxiv.org/abs/2305.04087},
	doi = {10.48550/arXiv.2305.04087},
	abstract = {Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89{\textbackslash}\% on APPS-dev, 31{\textbackslash}\% on APPS-test, and 48{\textbackslash}\% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhang, Kechi and Li, Zhuo and Li, Jia and Li, Ge and Jin, Zhi},
	month = sep,
	year = {2023},
	note = {arXiv:2305.04087 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{liuYourCodeGenerated2023,
	title = {Is {Your} {Code} {Generated} by {ChatGPT} {Really} {Correct}? {Rigorous} {Evaluation} of {Large} {Language} {Models} for {Code} {Generation}},
	shorttitle = {Is {Your} {Code} {Generated} by {ChatGPT} {Really} {Correct}?},
	url = {http://arxiv.org/abs/2305.01210},
	doi = {10.48550/arXiv.2305.01210},
	abstract = {Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9\%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
	month = oct,
	year = {2023},
	note = {arXiv:2305.01210 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{manUVAPADOVAType2014,
	title = {The {UVA}/{PADOVA} type 1 diabetes simulator: new features},
	volume = {8},
	number = {1},
	journal = {Journal of diabetes science and technology},
	author = {Man, Chiara Dalla and Micheletto, Francesco and Lv, Dayu and Breton, Marc and Kovatchev, Boris and Cobelli, Claudio},
	year = {2014},
	note = {ISBN: 1932-2968
Publisher: SAGE Publications Sage CA: Los Angeles, CA},
	pages = {26--34},
}

@article{herrettDataResourceProfile2015,
	title = {Data resource profile: clinical practice research datalink ({CPRD})},
	volume = {44},
	number = {3},
	journal = {International journal of epidemiology},
	author = {Herrett, Emily and Gallagher, Arlene M. and Bhaskaran, Krishnan and Forbes, Harriet and Mathur, Rohini and Van Staa, Tjeerd and Smeeth, Liam},
	year = {2015},
	note = {ISBN: 1464-3685
Publisher: Oxford University Press},
	pages = {827--836},
}

@misc{RedDotDesign,
	title = {Red {Dot} {Design} {Award}: {CPRD} {Training} {Kit}},
	url = {https://www.red-dot.org/project/cprd-training-kit-55269},
	urldate = {2023-11-21},
}

@article{siegelmannComputationTuringLimit1995,
	title = {Computation {Beyond} the {Turing} {Limit}},
	volume = {268},
	language = {en},
	author = {Siegelmann, Hava T},
	year = {1995},
}

@article{schlichtingRecognizingManagingSevere2007,
	title = {Recognizing and managing severe sepsis: a common and deadly threat},
	volume = {100},
	issn = {0038-4348},
	shorttitle = {Recognizing and managing severe sepsis},
	doi = {10.1097/SMJ.0b013e31804aa29f},
	abstract = {Through a literature review, the epidemiology and pathophysiology, including alterations in inflammation, coagulation, and impaired fibrinolysis that occur in the course of severe sepsis, is presented. Treatment guidelines that are evidence-based and endorsed by 11 professional societies representing multispecialty groups are described. Severe sepsis is common; 750,000 cases are estimated to occur annually in the United States. The mortality rate for severe sepsis still ranges from 30 to 50\%, and is as high as 80 to 90\% for septic shock and multiple organ dysfunction. Severe sepsis exists along a continuum initiated by a localized infection that triggers a systemic response. A cascade of inflammation and activation of the coagulation system associated with impaired fibrinolysis leads to alterations in microvascular circulation associated with organ dysfunction, severe sepsis, multiple organ dysfunction syndrome, and death. In an attempt to improve care and reduce mortality, the Surviving Sepsis Campaign and The Institute for Healthcare Improvement (IHI) have created two sepsis treatment bundles.},
	language = {eng},
	number = {6},
	journal = {Southern Medical Journal},
	author = {Schlichting, Douglas and McCollam, Jill Shwed},
	month = jun,
	year = {2007},
	pmid = {17591313},
	keywords = {Clinical Protocols, Europe, Evidence-Based Medicine, Humans, International Cooperation, Practice Guidelines as Topic, Shock, Septic, Systemic Inflammatory Response Syndrome, United States},
	pages = {594--600},
}

@inproceedings{hyotyniemiTuringMachinesAre1996,
	title = {Turing {Machines} {Are} {Recurrent} {Neural} {Networks}},
	url = {https://www.semanticscholar.org/paper/Turing-Machines-Are-Recurrent-Neural-Networks-Hyotyniemi/1c8c87c9009f4e435ce3e079c22e2d66c73d96e6},
	abstract = {Any algebraically computable function can be expressed as a recurrent neural network structure consisting of identical computing elements (or, equivalently, as a nonlinear discrete-time system of the form x(k + 1) = f(Ax(k)), where f() is a simplècut' function). A constructive proof is presented in this paper.},
	urldate = {2023-11-10},
	author = {Hyotyniemi, H.},
	year = {1996},
}

@article{L1991aging,
	title = {The aging population. {A} critical issue in medicine for the 21st century.},
	journal = {The Journal of the Florida Medical Association},
	author = {{L. Wecker} and {J. Krzanowski} and {J. Polson} and {D. Fitzpatrick} and {R. Coffey} and {J. Williams} and {J. Hackney}},
	year = {1991},
}

@article{Suzman2015Health,
	title = {Health in an ageing world—what do we know?},
	volume = {385},
	number = {9967},
	journal = {The Lancet},
	author = {Suzman, Richard and Beard, John R and Boerma, Ties and Chatterji, Somnath},
	month = feb,
	year = {2015},
	note = {Publisher: Elsevier BV},
	pages = {484--486},
}

@article{Sammy2019global,
	title = {The global challenge of ageing population – {Part} {II}: the impact on health services, and the optimum healthcare strategy for older patients},
	journal = {Caribbean Medical Journal},
	author = {Sammy, Ian and Paul, Joanne and Ramnarine, Arvind and Ramdhanie, Joseph},
	month = apr,
	year = {2019},
	note = {Publisher: Trinidad and Tobago Medical Association},
}

@article{2012health,
	title = {The health-care challenges posed by population ageing},
	volume = {90},
	number = {2},
	journal = {Bulletin of the World Health Organization},
	month = feb,
	year = {2012},
	note = {Publisher: WHO Press},
	pages = {82--83},
}

@article{Lloyd2012Population,
	title = {Population ageing and health},
	volume = {379},
	number = {9823},
	journal = {The Lancet},
	author = {Lloyd-Sherlock, Peter and McKee, Martin and Ebrahim, Shah and Gorman, Mark and Greengross, Sally and Prince, Martin and Pruchno, Rachel and Gutman, Gloria and Kirkwood, Tom and O'Neill, Desmond and Ferrucci, Luigi and Kritchevsky, Stephen B and Vellas, Bruno},
	month = apr,
	year = {2012},
	note = {Publisher: Elsevier BV},
	pages = {1295--1296},
}

@article{Mahishale2015Ageing,
	title = {Ageing world: {Health} care challenges},
	volume = {42},
	number = {3},
	journal = {Journal of the Scientific Society},
	author = {Mahishale, Vinay},
	year = {2015},
	note = {Publisher: Medknow},
	pages = {138},
}

@article{Aslam2021Ageing,
	title = {The {Ageing} {Population} in {Healthcare}; {Role} of {Leadership} {Theories} and {Policies} in the {Modern} {Era}},
	journal = {SSRN Electronic Journal},
	author = {Aslam, Fahim},
	year = {2021},
	note = {Publisher: Elsevier BV},
}

@article{Mann2004aging,
	title = {The aging population and its needs},
	volume = {3},
	number = {2},
	journal = {IEEE Pervasive Computing},
	author = {Mann, W.C.},
	month = apr,
	year = {2004},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {12--14},
}

@article{pollard2018a,
	title = {The {eICU} collaborative research database, a freely available multi-center database for critical care research},
	volume = {5},
	language = {it},
	number = {1},
	journal = {Scientific data},
	author = {Pollard, Tom J. and Johnson, Alistair E.W. and Raffa, Jesse D. and Celi, Leo A. and Mark, Roger G. and Badawi, Omar},
	year = {2018},
	pages = {1--13},
}

@misc{amsterdamumcdb-a,
	url = {https://amsterdammedicaldatascience.nl/.},
	author = {{AmsterdamUMCdb}},
}

@article{nunezreizBigDataAnalysis2019,
	title = {Big {Data} {Analysis} and {Machine} {Learning} in {Intensive} {Care} {Units}},
	volume = {43},
	issn = {2173-5727},
	url = {https://www.sciencedirect.com/science/article/pii/S2173572719301420},
	doi = {10.1016/j.medine.2019.06.012},
	abstract = {Intensive care is an ideal environment for the use of Big Data Analysis (BDA) and Machine Learning (ML), due to the huge amount of information processed and stored in electronic format in relation to such care. These tools can improve our clinical research capabilities and clinical decision making in the future. The present study reviews the foundations of BDA and ML, and explores possible applications in our field from a clinical viewpoint. We also suggest potential strategies to optimize these new technologies and describe a new kind of hybrid healthcare-data science professional with a linking role between clinicians and data.
Resumen
La gran cantidad de información que se procesa informáticamente en el entorno de la medicina intensiva la convierte en un campo ideal para el empleo de técnicas conocidas como Big Data Analysis (BDA) y Machine Learning (ML), que pueden permitir en el futuro mejorar nuestra capacidad de investigación clínica y dirigir de manera más precisa las terapias que proporcionamos a nuestros pacientes. En este artículo se revisan los conceptos fundamentales sobre BDA y ML, y se estudian sus posibles aplicaciones al ámbito de la medicina intensiva, desde un punto de vista del clínico. También se plantean potenciales estrategias para sacar el máximo partido a estas tecnologías emergentes, incluyendo la aparición de un nuevo tipo de profesional sanitario encargado de actuar como enlace entre la parte clínica y la ingeniería de datos.},
	number = {7},
	urldate = {2023-11-07},
	journal = {Medicina Intensiva (English Edition)},
	author = {Núñez Reiz, A. and Armengol de la Hoz, M. A. and Sánchez García, M.},
	month = oct,
	year = {2019},
	keywords = {Análisis secundario de datos clínicos electrónicos, Artificial intelligence, Big Data Analysis, Inteligencia artificial, Machine Learning, Secondary electronic health record data analysis},
	pages = {416--426},
}

@article{Prgomet2016Vital,
	title = {Vital signs monitoring on general wards: clinical staff perceptions of current practices and the planned introduction of continuous monitoring technology},
	volume = {28},
	number = {4},
	journal = {International Journal for Quality in Health Care},
	author = {Prgomet, Mirela and Cardona-Morrell, Magnolia and Nicholson, Margaret and Lake, Rebecca and Long, Janet and Westbrook, Johanna and Braithwaite, Jeffrey and Hillman, Ken},
	month = jun,
	year = {2016},
	note = {Publisher: Oxford University Press (OUP)},
	pages = {515--521},
}

@article{Vincent2018Improving,
	title = {Improving detection of patient deterioration in the general hospital ward environment},
	volume = {35},
	number = {5},
	journal = {European Journal of Anaesthesiology},
	author = {Vincent, Jean-Louis and Einav, Sharon and Pearse, Rupert and Jaber, Samir and Kranke, Peter and Overdyk, Frank J. and Whitaker, David K. and Gordo, Federico and Dahan, Albert and Hoeft, Andreas},
	month = may,
	year = {2018},
	note = {Publisher: Ovid Technologies (Wolters Kluwer Health)},
	pages = {325--333},
}

@article{Bockholt2022Real,
	title = {Real-{Time} {Monitoring} of {Blood} {Parameters} in the {Intensive} {Care} {Unit}: {State}-of-the-{Art} and {Perspectives}},
	volume = {11},
	number = {9},
	journal = {Journal of Clinical Medicine},
	author = {Bockholt, Rebecca and Paschke, Shaleen and Heubner, Lars and Ibarlucea, Bergoi and Laupp, Alexander and Janićijević, Željko and Klinghammer, Stephanie and Balakin, Sascha and Maitz, Manfred F. and Werner, Carsten and Cuniberti, Gianaurelio and Baraban, Larysa and Spieth, Peter Markus},
	month = apr,
	year = {2022},
	note = {Publisher: MDPI AG},
	pages = {2408},
}

@incollection{Fried2000Some,
	title = {Some {Statistical} {Methods} in {Intensive} {Care} {Online} {Monitoring} — {A} {Review}},
	booktitle = {Medical {Data} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fried, Roland and Gather, Ursula and Imhoff, Michael and Bauer, Marcus},
	year = {2000},
	pages = {67--77},
}

@article{Bailey2013trial,
	title = {A trial of a realtime {Alert} for clinical deterioration in {Patients} hospitalized on general medical wards},
	volume = {8},
	number = {5},
	journal = {Journal of Hospital Medicine},
	author = {Bailey, Thomas C. and Chen, Yixin and Mao, Yi and Lu, Chenyang and Hackmann, Gregory and Micek, Scott T. and Heard, Kevin M. and Faulkner, Kelly M. and Kollef, Marin H.},
	month = feb,
	year = {2013},
	note = {Publisher: Wiley},
	pages = {236--242},
}

@article{Blount2010Real,
	title = {Real-{Time} {Analysis} for {Intensive} {Care}: {Development} and {Deployment} of the {Artemis} {Analytic} {System}},
	volume = {29},
	number = {2},
	journal = {IEEE Engineering in Medicine and Biology Magazine},
	author = {Blount, Marion and Ebling, Maria R. and Eklund, J. Mikael. and James, Andrew G. and McGregor, Carolyn and Percival, Nathan and Smith, Kathleen and Sow, Daby},
	month = mar,
	year = {2010},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {110--118},
}

@article{Dimitrios1999Distributed,
	title = {A {Distributed}, {Agent}-{Based} {Architecture} for the {Acquisition}, {Management}, {Archiving} and {Display} of {Real}-{Time} {Monitor}- ing {Data} in the {Intensive} {Care} {Unit} *1},
	author = {{Dimitrios G. Katehakis} and {G. Chalkiadakis} and {M. Tsiknakis} and {S. Orphanoudakis}},
	year = {1999},
}

@inproceedings{Mao2012integrated,
	title = {An integrated data mining approach to real-time clinical monitoring and deterioration warning},
	booktitle = {Proceedings of the 18th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Mao, Yi and Chen, Wenlin and Chen, Yixin and Lu, Chenyang and Kollef, Marin and Bailey, Thomas},
	month = aug,
	year = {2012},
}

@misc{WhatIntensiveCare,
	title = {What is {Intensive} {Care}?},
	url = {https://www.esicm.org/patient-and-family/what-is-intensive-care/},
	abstract = {[...]Read More...},
	language = {en-US},
	urldate = {2023-11-07},
	journal = {ESICM},
}

@article{yecheHiRIDICUBenchmarkComprehensiveMachine,
	title = {{HiRID}-{ICU}-{Benchmark} — {A} {Comprehensive} {Machine} {Learning} {Benchmark} on {High}-resolution {ICU} {Data}},
	abstract = {The recent success of machine learning methods applied to time series collected from Intensive Care Units (ICU) exposes the lack of standardized machine learning benchmarks for developing and comparing such methods. While raw datasets, such as MIMIC-IV or eICU, can be freely accessed on Physionet, the choice of tasks and pre-processing is often chosen ad-hoc for each publication, limiting comparability across publications. In this work, we aim to improve this situation by providing a benchmark covering a large spectrum of ICU-related tasks. Using the HiRID dataset, we deﬁne multiple clinically relevant tasks in collaboration with clinicians. In addition, we provide a reproducible end-to-end pipeline to construct both data and labels. Finally, we provide an in-depth analysis of current state-of-the-art sequence modeling methods, highlighting some limitations of deep learning approaches for this type of data. With this benchmark, we hope to give the research community the possibility of a fair comparison of their work.},
	language = {en},
	author = {Yèche, Hugo and Kuznetsova, Rita and Zimmermann, Marc and Hüser, Matthias and Lyu, Xinrui and Faltys, Martin and Rätsch, Gunnar},
}

@misc{zhangChallengesPerspectivesFoundation2023,
	title = {On the {Challenges} and {Perspectives} of {Foundation} {Models} for {Medical} {Image} {Analysis}},
	url = {http://arxiv.org/abs/2306.05705},
	abstract = {This article discusses the opportunities, applications and future directions of large-scale pre-trained models, i.e., foundation models, for analyzing medical images. Medical foundation models have immense potential in solving a wide range of downstream tasks, as they can help to accelerate the development of accurate and robust models, reduce the large amounts of required labeled data, preserve the privacy and confidentiality of patient data. Specifically, we illustrate the "spectrum" of medical foundation models, ranging from general vision models, modality-specific models, to organ/task-specific models, highlighting their challenges, opportunities and applications. We also discuss how foundation models can be leveraged in downstream medical tasks to enhance the accuracy and efficiency of medical image analysis, leading to more precise diagnosis and treatment decisions.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Zhang, Shaoting and Metaxas, Dimitris},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05705 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yuanPowerFoundationModels2023,
	title = {On the {Power} of {Foundation} {Models}},
	url = {https://proceedings.mlr.press/v202/yuan23b.html},
	abstract = {With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be seen as a new type of generalization theorem, showing that the foundation model can generate unseen objects from the target category (e.g., images) using the structural information from the source category (e.g., texts). Along the way, we provide a categorical framework for supervised and self-supervised learning, which might be of independent interest.},
	language = {en},
	urldate = {2023-11-06},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yuan, Yang},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {40519--40530},
}

@misc{yangFoundationModelsDecision2023,
	title = {Foundation {Models} for {Decision} {Making}: {Problems}, {Methods}, and {Opportunities}},
	shorttitle = {Foundation {Models} for {Decision} {Making}},
	url = {http://arxiv.org/abs/2303.04129},
	abstract = {Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Yang, Sherry and Nachum, Ofir and Du, Yilun and Wei, Jason and Abbeel, Pieter and Schuurmans, Dale},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04129 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{salomonPracticeGuidelinesPerformance2011,
	title = {Practice guidelines for performance of the routine mid‐trimester fetal ultrasound scan},
	volume = {37},
	number = {1},
	journal = {Ultrasound in Obstetrics \& Gynecology},
	author = {Salomon, Laurent Julien and Alfirevic, Z. and Berghella, V. and Bilardo, C. and Hernandez‐Andrade, E. and Johnsen, S. L. and Kalache, K. and Leung, K.-Y. and Malinger, G. and Munoz, H.},
	year = {2011},
	note = {ISBN: 0960-7692
Publisher: John Wiley \& Sons, Ltd. Chichester, UK},
	pages = {116--126},
}

@article{liDeepReinforcementLearning2017,
	title = {Deep reinforcement learning: {An} overview},
	journal = {arXiv preprint arXiv:1701.07274},
	author = {Li, Yuxi},
	year = {2017},
}

@inproceedings{briskAIEnhanceInteractive2018,
	title = {{AI} to enhance interactive simulation-based training in resuscitation medicine},
	booktitle = {British {HCI} {Conference} 2018},
	author = {Brisk, Rob and Bond, R. R. and Liu, Jun and Finlay, Dewar and McLaughlin, James and McEneaney, David},
	year = {2018},
}

@article{reynaEarlyPredictionSepsis2020,
	title = {Early prediction of sepsis from clinical data: the {PhysioNet}/{Computing} in {Cardiology} {Challenge} 2019},
	volume = {48},
	number = {2},
	journal = {Critical care medicine},
	author = {Reyna, Matthew A. and Josef, Christopher S. and Jeter, Russell and Shashikumar, Supreeth P. and Westover, M. Brandon and Nemati, Shamim and Clifford, Gari D. and Sharma, Ashish},
	year = {2020},
	note = {ISBN: 0090-3493
Publisher: LWW},
	pages = {210--217},
}

@article{palMachineLearningHealthcare2023,
	title = {Machine {Learning} in {Healthcare}: {A} {Review}},
	volume = {12},
	number = {1},
	journal = {methods},
	author = {Pal, Debrupa and Ghosh, Animesh and Majumdar, Sourav and Pan, Ashadeep and Ghosh, Debmitra},
	year = {2023},
	pages = {60--66},
}

@article{wangGLUEMultitaskBenchmark2018,
	title = {{GLUE}: {A} multi-task benchmark and analysis platform for natural language understanding},
	journal = {arXiv preprint arXiv:1804.07461},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	year = {2018},
}

@inproceedings{dengImagenetLargescaleHierarchical2009,
	title = {Imagenet: {A} large-scale hierarchical image database},
	isbn = {1-4244-3992-2},
	booktitle = {2009 {IEEE} conference on computer vision and pattern recognition},
	publisher = {Ieee},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	pages = {248--255},
}

@article{Gilbert2015market,
	title = {The market in healthcare data},
	journal = {BMJ (Clinical research ed.)},
	author = {Gilbert, Ruth and Goldstein, Harvey and Hemingway, Harry},
	month = nov,
	year = {2015},
	note = {Publisher: BMJ},
	pages = {h5897},
}

@incollection{Anshik2021Handling,
	title = {Handling {Availability} of {Low}-{Training} {Data} in {Healthcare}},
	booktitle = {{AI} for {Healthcare} with {Keras} and {Tensorflow} 2.0},
	publisher = {Apress},
	author = {{Anshik}},
	year = {2021},
	pages = {173--214},
}

@inproceedings{Pahwa2021Big,
	title = {Big {Data} and {Machine} {Learning} in {Healthcare}: {Tools} \&amp; {Challenges}},
	booktitle = {2021 3rd {International} {Conference} on {Advances} in {Computing}, {Communication} {Control} and {Networking} ({ICAC3N})},
	publisher = {IEEE},
	author = {Pahwa, Kanika and Chauhan, Sharad},
	month = dec,
	year = {2021},
}

@inproceedings{Yazhini2019State,
	title = {A {State} of {Art} {Approaches} on {Deep} {Learning} {Models} in {Healthcare}: {An} {Application} {Perspective}},
	booktitle = {2019 3rd {International} {Conference} on {Trends} in {Electronics} and {Informatics} ({ICOEI})},
	publisher = {IEEE},
	author = {Yazhini, K. and Loganathan, D.},
	month = apr,
	year = {2019},
}

@article{S2017Benchmark,
	title = {Benchmark of {Deep} {Learning} {Models} on {Large} {Healthcare} {MIMIC} {Datasets}},
	journal = {arXiv.org},
	author = {{S. Purushotham} and {Chuizheng Meng} and {Zhengping Che} and {Yan Liu}},
	year = {2017},
}

@article{Crown2015Potential,
	title = {Potential {Application} of {Machine} {Learning} in {Health} {Outcomes} {Research} and {Some} {Statistical} {Cautions}},
	volume = {18},
	number = {2},
	journal = {Value in Health},
	author = {Crown, William H.},
	month = mar,
	year = {2015},
	note = {Publisher: Elsevier BV},
	pages = {137--140},
}

@article{UnderstaffingSignificantIssue2012,
	title = {Understaffing is ‘significant issue’ in nursing homes},
	volume = {27},
	number = {13},
	journal = {Nursing Standard},
	month = nov,
	year = {2012},
	note = {Publisher: RCN Publishing Ltd.},
	pages = {6--6},
}

@article{munnUnderstaffingWardsCompromising2017,
	title = {Understaffing on wards ‘compromising safe care’},
	volume = {31},
	number = {36},
	journal = {Nursing Standard},
	author = {Munn, Flavia},
	month = may,
	year = {2017},
	note = {Publisher: RCN Publishing Ltd.},
	pages = {9--9},
}

@article{hudsonUnderstaffing2015,
	title = {Understaffing},
	volume = {5},
	number = {3},
	journal = {Organizational Psychology Review},
	author = {Hudson, Cristina K. and Shen, Winny},
	month = mar,
	year = {2015},
	note = {Publisher: SAGE Publications},
	pages = {244--263},
}

@article{r.stanleyUnderstaffedOverwhelmed2010,
	title = {Understaffed and overwhelmed},
	author = {{R. Stanley}},
	year = {2010},
}

@misc{wangVoyagerOpenEndedEmbodied2023,
	title = {Voyager: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models}},
	shorttitle = {Voyager},
	url = {http://arxiv.org/abs/2305.16291},
	abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	month = may,
	year = {2023},
	note = {arXiv:2305.16291 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{kumarUsingNaturalLanguage2023,
	title = {Using {Natural} {Language} and {Program} {Abstractions} to {Instill} {Human} {Inductive} {Biases} in {Machines}},
	url = {http://arxiv.org/abs/2205.11558},
	doi = {10.48550/arXiv.2205.11558},
	abstract = {Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more human-like inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Kumar, Sreejan and Correa, Carlos G. and Dasgupta, Ishita and Marjieh, Raja and Hu, Michael Y. and Hawkins, Robert D. and Daw, Nathaniel D. and Cohen, Jonathan D. and Narasimhan, Karthik and Griffiths, Thomas L.},
	month = feb,
	year = {2023},
	note = {arXiv:2205.11558 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{nerellaTransformersHealthcareSurvey2023,
	title = {Transformers in {Healthcare}: {A} {Survey}},
	shorttitle = {Transformers in {Healthcare}},
	url = {http://arxiv.org/abs/2307.00067},
	abstract = {With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of data, including medical imaging, structured and unstructured Electronic Health Records (EHR), social media, physiological signals, and biomolecular sequences. Those models could help in clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. We identified relevant studies using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability, fairness, alignment with human values, ethical implications, and environmental impact.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Nerella, Subhash and Bandyopadhyay, Sabyasachi and Zhang, Jiaqing and Contreras, Miguel and Siegel, Scott and Bumin, Aysegul and Silva, Brandon and Sena, Jessica and Shickel, Benjamin and Bihorac, Azra and Khezeli, Kia and Rashidi, Parisa},
	month = jun,
	year = {2023},
	note = {arXiv:2307.00067 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{xhaferraRoleMachineLearning2022,
	title = {The {Role} of {Machine} {Learning} in the {Healthcare} {Sector}: {A} {Roadmap} to the {Potential} {Prospects}},
	booktitle = {2022 {International} {Congress} on {Human}-{Computer} {Interaction}, {Optimization} and {Robotic} {Applications} ({HORA})},
	publisher = {IEEE},
	author = {Xhaferra, Edmira and Ismaili, Florije},
	month = jun,
	year = {2022},
}

@misc{liTimeSeriesImages2023,
	title = {Time {Series} as {Images}: {Vision} {Transformer} for {Irregularly} {Sampled} {Time} {Series}},
	shorttitle = {Time {Series} as {Images}},
	url = {http://arxiv.org/abs/2303.12799},
	doi = {10.48550/arXiv.2303.12799},
	abstract = {Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance improvement is up to 54.0{\textbackslash}\% in absolute F1 score points. Our code and data are available at {\textbackslash}url\{https://github.com/Leezekun/ViTST\}.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Li, Zekun and Li, Shiyang and Yan, Xifeng},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12799 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{zuoTransformerHawkesProcess2020,
	title = {Transformer {Hawkes} {Process}},
	url = {https://proceedings.mlr.press/v119/zuo20a.html},
	abstract = {Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture long-term dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.},
	language = {en},
	urldate = {2023-08-23},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zuo, Simiao and Jiang, Haoming and Li, Zichong and Zhao, Tuo and Zha, Hongyuan},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {11692--11702},
}

@misc{wenTransformersTimeSeries2023,
	title = {Transformers in {Time} {Series}: {A} {Survey}},
	shorttitle = {Transformers in {Time} {Series}},
	url = {http://arxiv.org/abs/2202.07125},
	abstract = {Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. To the best of our knowledge, this paper is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
	month = may,
	year = {2023},
	note = {arXiv:2202.07125 [cs, eess, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@misc{singhalExpertLevelMedicalQuestion2023,
	title = {Towards {Expert}-{Level} {Medical} {Question} {Answering} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.09617},
	abstract = {Recent artificial intelligence (AI) systems have reached milestones in "grand challenges" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a "passing" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2\% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5\% on the MedQA dataset, improving upon Med-PaLM by over 19\% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p {\textless} 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p {\textless} 0.001) on newly introduced datasets of 240 long-form "adversarial" questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and Schaekermann, Mike and Wang, Amy and Amin, Mohamed and Lachgar, Sami and Mansfield, Philip and Prakash, Sushant and Green, Bradley and Dominowska, Ewa and Arcas, Blaise Aguera y and Tomasev, Nenad and Liu, Yun and Wong, Renee and Semturs, Christopher and Mahdavi, S. Sara and Barral, Joelle and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Azizi, Shekoofeh and Karthikesalingam, Alan and Natarajan, Vivek},
	month = may,
	year = {2023},
	note = {arXiv:2305.09617 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{wolpertLackPrioriDistinctions1996,
	title = {The {Lack} of {A} {Priori} {Distinctions} {Between} {Learning} {Algorithms}},
	volume = {8},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1996.8.7.1341},
	doi = {10.1162/neco.1996.8.7.1341},
	abstract = {This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.},
	number = {7},
	urldate = {2023-10-24},
	journal = {Neural Computation},
	author = {Wolpert, David H.},
	month = oct,
	year = {1996},
	pages = {1341--1390},
}

@misc{GreatMultivariateTime,
	title = {The great multivariate time series classification bake off: a review and experimental evaluation of recent algorithmic advances {\textbar} {Data} {Mining} and {Knowledge} {Discovery}},
	url = {https://link.springer.com/article/10.1007/s10618-020-00727-3},
	urldate = {2023-09-20},
}

@misc{frankleLotteryTicketHypothesis2019,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	doi = {10.48550/arXiv.1803.03635},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv:1803.03635 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{yunHiddenPowerPure2023,
	title = {The {Hidden} {Power} of {Pure} 16-bit {Floating}-{Point} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2301.12809},
	abstract = {Lowering the precision of neural networks from the prevalent 32-bit precision has long been considered harmful to performance, despite the gain in space and time. Many works propose various techniques to implement half-precision neural networks, but none study pure 16-bit settings. This paper investigates the unexpected performance gain of pure 16-bit neural networks over the 32-bit networks in classification tasks. We present extensive experimental results that favorably compare various 16-bit neural networks' performance to those of the 32-bit models. In addition, a theoretical analysis of the efficiency of 16-bit models is provided, which is coupled with empirical evidence to back it up. Finally, we discuss situations in which low-precision training is indeed detrimental.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Yun, Juyoung and Kang, Byungkon and Fu, Zhoulai},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12809 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance},
}

@article{hintonForwardForwardAlgorithmPreliminary,
	title = {The {Forward}-{Forward} {Algorithm}: {Some} {Preliminary} {Investigations}},
	abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth serious investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes can be separated in time, the negative passes can be done ofﬂine, which makes the learning much simpler in the positive pass and allows video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
	language = {en},
	author = {Hinton, Geoffrey},
}

@article{tipirneniSelfSupervisedTransformerSparse2022,
	title = {Self-{Supervised} {Transformer} for {Sparse} and {Irregularly} {Sampled} {Multivariate} {Clinical} {Time}-{Series}},
	volume = {16},
	issn = {1556-4681},
	url = {https://dl.acm.org/doi/10.1145/3516367},
	doi = {10.1145/3516367},
	abstract = {Multivariate time-series data are frequently observed in critical care settings and are typically characterized by sparsity (missing information) and irregular time intervals. Existing approaches for learning representations in this domain handle these challenges by either aggregation or imputation of values, which in-turn suppresses the fine-grained information and adds undesirable noise/overhead into the machine learning model. To tackle this problem, we propose a Self-supervised Transformer for Time-Series (STraTS) model, which overcomes these pitfalls by treating time-series as a set of observation triplets instead of using the standard dense matrix representation. It employs a novel Continuous Value Embedding technique to encode continuous time and variable values without the need for discretization. It is composed of a Transformer component with multi-head attention layers, which enable it to learn contextual triplet embeddings while avoiding the problems of recurrence and vanishing gradients that occur in recurrent architectures. In addition, to tackle the problem of limited availability of labeled data (which is typically observed in many healthcare applications), STraTS utilizes self-supervision by leveraging unlabeled data to learn better representations by using time-series forecasting as an auxiliary proxy task. Experiments on real-world multivariate clinical time-series benchmark datasets demonstrate that STraTS has better prediction performance than state-of-the-art methods for mortality prediction, especially when labeled data is limited. Finally, we also present an interpretable version of STraTS, which can identify important measurements in the time-series data. Our data preprocessing and model implementation codes are available at https://github.com/sindhura97/STraTS.},
	number = {6},
	urldate = {2023-08-23},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Tipirneni, Sindhu and Reddy, Chandan K.},
	month = jul,
	year = {2022},
	keywords = {Time-series, Transformer, deep learning, healthcare, neural networks, self-supervised learning},
	pages = {105:1--105:17},
}

@article{SurveyShowsHidden1993,
	title = {Survey shows hidden cuts},
	volume = {7},
	number = {35},
	journal = {Nursing Standard},
	month = may,
	year = {1993},
	note = {Publisher: RCN Publishing Ltd.},
	pages = {23--24},
}

@misc{damianSelfStabilizationImplicitBias2023,
	title = {Self-{Stabilization}: {The} {Implicit} {Bias} of {Gradient} {Descent} at the {Edge} of {Stability}},
	shorttitle = {Self-{Stabilization}},
	url = {http://arxiv.org/abs/2209.15594},
	doi = {10.48550/arXiv.2209.15594},
	abstract = {Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness \$S({\textbackslash}theta)\$, is bounded by \$2/{\textbackslash}eta\$, training is "stable" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff \$2/{\textbackslash}eta\$. The second, dubbed edge of stability, is that the sharpness hovers at \$2/{\textbackslash}eta\$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call self-stabilization, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows projected gradient descent (PGD) under the constraint \$S({\textbackslash}theta) {\textbackslash}le 2/{\textbackslash}eta\$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Damian, Alex and Nichani, Eshaan and Lee, Jason D.},
	month = apr,
	year = {2023},
	note = {arXiv:2209.15594 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{SparseTransformerHawkes,
	title = {Sparse {Transformer} {Hawkes} {Process} for {Long} {Event} {Sequences}},
	url = {https://www.springerprofessional.de/en/sparse-transformer-hawkes-process-for-long-event-sequences/26052840},
	abstract = {Large quantities of asynchronous event sequence data such as crime records, emergence call logs, and financial transactions are becoming increasingly available from various fields. These event sequences often exhibit both long-term and short-term …},
	language = {en},
	urldate = {2023-09-19},
	journal = {springerprofessional.de},
}

@misc{shawSelfAttentionRelativePosition2018,
	title = {Self-{Attention} with {Relative} {Position} {Representations}},
	url = {http://arxiv.org/abs/1803.02155},
	abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efﬁciently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efﬁcient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.},
	language = {en},
	urldate = {2023-10-10},
	publisher = {arXiv},
	author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
	month = apr,
	year = {2018},
	note = {arXiv:1803.02155 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhengRethinkingPositionalEncoding2021,
	title = {Rethinking {Positional} {Encoding}},
	url = {https://arxiv.org/abs/2107.02561v3},
	abstract = {It is well noted that coordinate based MLPs benefit -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice. Codes available at https://github.com/osiriszjq/Rethinking-positional-encoding.},
	language = {en},
	urldate = {2023-10-10},
	journal = {arXiv.org},
	author = {Zheng, Jianqiao and Ramasinghe, Sameera and Lucey, Simon},
	month = jul,
	year = {2021},
}

@misc{suRoFormerEnhancedTransformer2021,
	title = {{RoFormer}: {Enhanced} {Transformer} with {Rotary} {Position} {Embedding}},
	shorttitle = {{RoFormer}},
	url = {https://arxiv.org/abs/2104.09864v4},
	abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
	language = {en},
	urldate = {2023-10-10},
	journal = {arXiv.org},
	author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
	month = apr,
	year = {2021},
}

@misc{schmidhuberReinforcementLearningUpside2020,
	title = {Reinforcement {Learning} {Upside} {Down}: {Don}'t {Predict} {Rewards} -- {Just} {Map} {Them} to {Actions}},
	shorttitle = {Reinforcement {Learning} {Upside} {Down}},
	url = {http://arxiv.org/abs/1912.02875},
	abstract = {We transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL). Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [63] on first experiments with UDRL shows that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems. We also also conceptually simplify an approach [60] for teaching a robot to imitate humans. First videotape humans imitating the robot's current behaviors, then let the robot learn through SL to map the videos (as input commands) to these behaviors, then let it generalize and imitate videos of humans executing previously unknown behavior. This Imitate-Imitator concept may actually explain why biological evolution has resulted in parents who imitate the babbling of their babies.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Schmidhuber, Juergen},
	month = jun,
	year = {2020},
	note = {arXiv:1912.02875 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{liventsevReinforcementLearningMessage2021,
	title = {Reinforcement learning as message passing},
	copyright = {All rights reserved},
	url = {https://vadim.me/publications/mpdp/},
	abstract = {In theory, Reinforcement Learning is a discipline concerned with algorithms for decision-making in opaque environments that maximize cumulative reward. In practice, however, (and yours truly is guilty of this as much as everyone) reinforcement learning literature mostly discusses solving Markov Decision Processes. However, some decision making settings are fairly hard to model as a Markdov Decision Process with discrete timesteps. In this post, I would like to propose an alternative representation of reinforcement learning problems using message passing and discuss how they can be solved (tl;dr by reducing them to MDPs, but this reduction is a non-trivial problem in its own right).},
	language = {en},
	urldate = {2023-08-23},
	author = {Liventsev, Vadim},
	month = oct,
	year = {2021},
	note = {Section: publications},
}

@misc{austinProgramSynthesisLarge2021,
	title = {Program {Synthesis} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2108.07732},
	abstract = {This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.},
	urldate = {2023-03-22},
	publisher = {arXiv},
	author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
	month = aug,
	year = {2021},
	note = {arXiv:2108.07732 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@article{yuReinforcementLearningHealthcare2021,
	title = {Reinforcement {Learning} in {Healthcare}: {A} {Survey}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {Reinforcement {Learning} in {Healthcare}},
	url = {https://dl.acm.org/doi/10.1145/3477600},
	doi = {10.1145/3477600},
	abstract = {As a subfield of machine learning, reinforcement learning (RL) aims at optimizing decision making by using interaction samples of an agent with its environment and the potentially delayed feedbacks. In contrast to traditional supervised learning that typically relies on one-shot, exhaustive, and supervised reward signals, RL tackles sequential decision-making problems with sampled, evaluative, and delayed feedbacks simultaneously. Such a distinctive feature makes RL techniques a suitable candidate for developing powerful solutions in various healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged period with delayed feedbacks. By first briefly examining theoretical foundations and key methods in RL research, this survey provides an extensive overview of RL applications in a variety of healthcare domains, ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis, and many other control or scheduling problems that have infiltrated every aspect of the healthcare system. In addition, we discuss the challenges and open issues in the current research and highlight some potential solutions and directions for future research.},
	number = {1},
	urldate = {2023-06-04},
	journal = {ACM Computing Surveys},
	author = {Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
	month = nov,
	year = {2021},
	keywords = {Reinforcement learning, automated diagnosis, chronic disease, critical care, dynamic treatment regimes, healthcare},
	pages = {5:1--5:36},
}

@misc{PretrainedLanguageModels,
	title = {Pretrained {Language} {Models} for {Biomedical} and {Clinical} {Tasks}: {Understanding} and {Extending} the {State}-of-the-{Art} - {ACL} {Anthology}},
	url = {https://aclanthology.org/2020.clinicalnlp-1.17/},
	urldate = {2023-08-24},
}

@misc{PDFPlanningLarge,
	title = {[{PDF}] {Planning} with {Large} {Language} {Models} via {Corrective} {Re}-prompting {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Planning-with-Large-Language-Models-via-Corrective-Raman-Cohen/f318ab67ac22cb758e38a16dafdc8e486b7b9756},
	urldate = {2023-09-20},
}

@inproceedings{lewisPretrainedLanguageModels2020,
	address = {Online},
	title = {Pretrained {Language} {Models} for {Biomedical} and {Clinical} {Tasks}: {Understanding} and {Extending} the {State}-of-the-{Art}},
	shorttitle = {Pretrained {Language} {Models} for {Biomedical} and {Clinical} {Tasks}},
	url = {https://aclanthology.org/2020.clinicalnlp-1.17},
	doi = {10.18653/v1/2020.clinicalnlp-1.17},
	abstract = {A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial that models can be built quickly and are highly accurate. We present a large-scale study across 18 established biomedical and clinical NLP tasks to determine which of several popular open-source biomedical and clinical NLP models work well in different settings. Furthermore, we apply recent advances in pretraining to train new biomedical language models, and carefully investigate the effect of various design choices on downstream performance. Our best models perform well in all of our benchmarks, and set new State-of-the-Art in 9 tasks. We release these models in the hope that they can help the community to speed up and increase the accuracy of BioNLP and text mining applications.},
	urldate = {2023-08-24},
	booktitle = {Proceedings of the 3rd {Clinical} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Patrick and Ott, Myle and Du, Jingfei and Stoyanov, Veselin},
	month = nov,
	year = {2020},
	pages = {146--157},
}

@article{spaanPartiallyObservableMarkov2012,
	title = {Partially observable {Markov} decision processes},
	issn = {364227644X},
	journal = {Reinforcement learning: State-of-the-art},
	author = {Spaan, Matthijs TJ},
	year = {2012},
	note = {Publisher: Springer},
	pages = {387--414},
}

@misc{shchurNeuralTemporalPoint2021,
	title = {Neural {Temporal} {Point} {Processes}: {A} {Review}},
	shorttitle = {Neural {Temporal} {Point} {Processes}},
	url = {http://arxiv.org/abs/2104.03528},
	doi = {10.48550/arXiv.2104.03528},
	abstract = {Temporal point processes (TPP) are probabilistic generative models for continuous-time event sequences. Neural TPPs combine the fundamental ideas from point process literature with deep learning approaches, thus enabling construction of flexible and efficient models. The topic of neural TPPs has attracted significant attention in the recent years, leading to the development of numerous new architectures and applications for this class of models. In this review paper we aim to consolidate the existing body of knowledge on neural TPPs. Specifically, we focus on important design choices and general principles for defining neural TPP models. Next, we provide an overview of application areas commonly considered in the literature. We conclude this survey with the list of open challenges and important directions for future work in the field of neural TPPs.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Shchur, Oleksandr and Türkmen, Ali Caner and Januschowski, Tim and Günnemann, Stephan},
	month = aug,
	year = {2021},
	note = {arXiv:2104.03528 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{bhalwankarNeuroSymbolicReinforcementLearning,
	title = {Neuro-{Symbolic} {Reinforcement} {Learning} for {Sepsis} and {Emergency} {Care} {Treatment}},
	abstract = {Traditional Deep Reinforcement Learning (RL) lacks even the basic human cognitive faculties needed for safety, reliability, and explainability in real world applications where decision-making is critical. There is a pressing need especially in healthcare to include domain knowledge along with deep learning to have human understandable, controllable decision-making, especially to restrict harmful behaviors. Such systems can prove of immense help in form of decision support systems in medical care. In this work, we attempted to include domain knowledge along with RL to not only improve the performance of the agent but also with the benefits that such learned policies are also interpretable and controllable. We used the domain knowledge in two important medical care problems, Sepsis Treatment in ICU and Emergency Care Treatment for Stabilizing patient. We modified and used patient simulators for these problems and highlighted the benefits of using and developing such simulators. Our results, showed significant improvements p {\textless} 0.001 in the performance of the agent due to the domain knowledge infused with the help of Linear Temporal Logic formulae progression based on important events and actions in the treatment strategy. Future research are encouraged to use more advanced constraints, and the use of computational tree logic*.},
	language = {en},
	author = {Bhalwankar, Raj},
}

@article{weiMIMICELMIMICIVEvent2022,
	title = {{MIMICEL}: {MIMIC}-{IV} {Event} {Log} for {Emergency} {Department}},
	author = {Wei, Jia and He, Zhipeng and Ouyang, Chun and Moreira, Catarina},
	year = {2022},
	note = {Publisher: PhysioNet},
}

@article{johnsonMIMICIVFreelyAccessible2023,
	title = {{MIMIC}-{IV}, a freely accessible electronic health record dataset},
	volume = {10},
	copyright = {2023 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01899-x},
	doi = {10.1038/s41597-022-01899-x},
	abstract = {Digital data collection during routine clinical practice is now ubiquitous within hospitals. The data contains valuable information on the care of patients and their response to treatments, offering exciting opportunities for research. Typically, data are stored within archival systems that are not intended to support research. These systems are often inaccessible to researchers and structured for optimal storage, rather than interpretability and analysis. Here we present MIMIC-IV, a publicly available database sourced from the electronic health record of the Beth Israel Deaconess Medical Center. Information available includes patient measurements, orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. MIMIC-IV is intended to support a wide array of research studies and educational material, helping to reduce barriers to conducting clinical research.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Scientific Data},
	author = {Johnson, Alistair E. W. and Bulgarelli, Lucas and Shen, Lu and Gayles, Alvin and Shammout, Ayad and Horng, Steven and Pollard, Tom J. and Hao, Sicheng and Moody, Benjamin and Gow, Brian and Lehman, Li-wei H. and Celi, Leo A. and Mark, Roger G.},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Epidemiology, Health services, Public health},
	pages = {1},
}

@article{kellerNeuralWaveMachines2023,
	title = {Neural {Wave} {Machines}: {Learning} {Spatiotemporally} {Structured} {Representations} with {Locally} {Coupled} {Oscillatory} {Recurrent} {Neural} {Networks}},
	shorttitle = {Neural {Wave} {Machines}},
	url = {https://openreview.net/forum?id=5tJSt3kn4s},
	abstract = {Traveling waves have been measured at a diversity of regions and scales in the brain, however a consensus as to their computational purpose has yet to be reached. An intriguing hypothesis is that traveling waves serve to structure neural representations both in space and time, thereby acting as an inductive bias towards natural data. In this work, we investigate this hypothesis by introducing the Neural Wave Machine (NWM) -- a locally coupled oscillatory recurrent neural network capable of exhibiting traveling waves in its hidden state. After training on simple dynamic sequences, we show that this model indeed learns static spatial structure such as topographic organization, and further uses complex spatiotemporal structure such as traveling waves to encode observed transformations. To measure the computational implications of this structure, we use a suite of sequence classification and physical dynamics modeling tasks to show that the NWM is both more parameter efficient, and is able to forecast future trajectories of simple physical dynamical systems more accurately than existing state of the art counterparts.},
	language = {en},
	urldate = {2023-10-02},
	author = {Keller, T. Anderson and Welling, Max},
	month = jun,
	year = {2023},
}

@inproceedings{jiangNeuralLogicReinforcement2019,
	title = {Neural {Logic} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v97/jiang19a.html},
	abstract = {Deep reinforcement learning (DRL) has achieved significant breakthroughs in various tasks. However, most DRL algorithms suffer a problem of generalising the learned policy, which makes the policy performance largely affected even by minor modifications of the training environment. Except that, the use of deep neural networks makes the learned policies hard to be interpretable. To address these two challenges, we propose a novel algorithm named Neural Logic Reinforcement Learning (NLRL) to represent the policies in reinforcement learning by first-order logic. NLRL is based on policy gradient methods and differentiable inductive logic programming that have demonstrated significant advantages in terms of interpretability and generalisability in supervised tasks. Extensive experiments conducted on cliff-walking and blocks manipulation tasks demonstrate that NLRL can induce interpretable policies achieving near-optimal performance while showing good generalisability to environments of different initial states and problem sizes.},
	language = {en},
	urldate = {2023-09-20},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jiang, Zhengyao and Luo, Shan},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3110--3119},
}

@misc{levineOfflineReinforcementLearning2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2005.01643},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	month = nov,
	year = {2020},
	note = {arXiv:2005.01643 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{jannerOfflineReinforcementLearning2021,
	title = {Offline {Reinforcement} {Learning} as {One} {Big} {Sequence} {Modeling} {Problem}},
	url = {http://arxiv.org/abs/2106.02039},
	abstract = {Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
	month = nov,
	year = {2021},
	note = {arXiv:2106.02039 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{burtsevMemoryTransformer2021,
	title = {Memory {Transformer}},
	url = {http://arxiv.org/abs/2006.11527},
	abstract = {Transformer-based models have achieved state-of-the-art results in many natural language processing tasks. The self-attention architecture allows transformer to combine information from all elements of a sequence into context-aware representations. However, information about the context is stored mostly in the same element-wise representations. This might limit the processing of properties related to the sequence as a whole more difficult. Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model. Memory-augmented neural networks (MANNs) extend traditional neural architectures with general-purpose memory for representations. MANNs have demonstrated the capability to learn simple algorithms like Copy or Reverse and can be successfully trained via backpropagation on diverse tasks from question answering to language modeling outperforming RNNs and LSTMs of comparable complexity. In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer. We evaluate these memory augmented Transformers and demonstrate that presence of memory positively correlates with the model performance for machine translation and language modelling tasks. Augmentation of pre-trained masked language model with memory tokens shows mixed results for tasks from GLUE benchmark. Visualization of attention patterns over the memory suggest that it improves the model's ability to process a global context.},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Burtsev, Mikhail S. and Kuratov, Yuri and Peganov, Anton and Sapunov, Grigory V.},
	month = feb,
	year = {2021},
	note = {arXiv:2006.11527 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{johnsonMIMICIVED2021,
	title = {{MIMIC}-{IV}-{ED}},
	journal = {PhysioNet},
	author = {Johnson, Alistair and Bulgarelli, Lucas and Pollard, Tom and Celi, Leo Anthony and Mark, Roger and Horng IV, S},
	year = {2021},
}

@misc{hendrycksMeasuringCodingChallenge2021,
	title = {Measuring {Coding} {Challenge} {Competence} {With} {APPS}},
	url = {http://arxiv.org/abs/2105.09938},
	abstract = {While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20\% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and Steinhardt, Jacob},
	month = nov,
	year = {2021},
	note = {arXiv:2105.09938 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{ganguliMachineLearningPursuit2020,
	title = {Machine {Learning} and the {Pursuit} of {High}-{Value} {Health} {Care}},
	volume = {1},
	number = {6},
	journal = {NEJM Catalyst},
	author = {Ganguli, Ishani and Gordon, William J. and Lupo, Claire and Sands-Lincoln, Megan and George, Judy and Jackson, Gretchen and Rhee, Kyu and Bates, David W.},
	month = nov,
	year = {2020},
	note = {Publisher: Massachusetts Medical Society},
}

@inproceedings{maityMachineLearningImproved2017,
	title = {Machine learning for improved diagnosis and prognosis in healthcare},
	booktitle = {2017 {IEEE} {Aerospace} {Conference}},
	publisher = {IEEE},
	author = {Maity, Niharika G. and Das, Sreerupa},
	month = mar,
	year = {2017},
}

@book{agrawalMachineLearningHealthcare2020,
	title = {Machine {Learning} for {Healthcare}},
	publisher = {Chapman and Hall/CRC},
	author = {Agrawal, Rashmi and Chatterjee, Jyotir Moy and Kumar, Abhishek and Rathore, Pramod Singh and Le, Dac-Nhuong},
	editor = {Agrawal, Rashmi and Chatterjee, Jyotir Moy and Kumar, Abhishek and Rathore, Pramod Singh and Le, Dac-Nhuong},
	month = dec,
	year = {2020},
}

@incollection{mitraMachineLearningHealthcare2021,
	title = {Machine {Learning} in {Healthcare}},
	booktitle = {{AI} {Innovation} in {Medical} {Imaging} {Diagnostics}},
	publisher = {IGI Global},
	author = {Mitra, Debasree and Paul, Apurba and Chatterjee, Sumanta},
	year = {2021},
	pages = {37--60},
}

@misc{liuLostMiddleHow2023,
	title = {Lost in the {Middle}: {How} {Language} {Models} {Use} {Long} {Contexts}},
	shorttitle = {Lost in the {Middle}},
	url = {http://arxiv.org/abs/2307.03172},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03172 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{liLearningIrregularlySampledTime2020,
	title = {Learning from {Irregularly}-{Sampled} {Time} {Series}: {A} {Missing} {Data} {Perspective}},
	shorttitle = {Learning from {Irregularly}-{Sampled} {Time} {Series}},
	url = {https://proceedings.mlr.press/v119/li20k.html},
	abstract = {Irregularly-sampled time series occur in many domains including healthcare. They can be challenging to model because they do not naturally yield a fixed-dimensional representation as required by many standard machine learning models. In this paper, we consider irregular sampling from the perspective of missing data. We model observed irregularly-sampled time series data as a sequence of index-value pairs sampled from a continuous but unobserved function. We introduce an encoder-decoder framework for learning from such generic indexed sequences. We propose learning methods for this framework based on variational autoencoders and generative adversarial networks. For continuous irregularly-sampled time series, we introduce continuous convolutional layers that can efficiently interface with existing neural network architectures. Experiments show that our models are able to achieve competitive or better classification results on irregularly-sampled multivariate time series compared to recent RNN models while offering significantly faster training times.},
	language = {en},
	urldate = {2023-08-23},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Steven Cheng-Xian and Marlin, Benjamin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {5937--5946},
}

@misc{gruverLargeLanguageModels2023,
	title = {Large {Language} {Models} {Are} {Zero}-{Shot} {Time} {Series} {Forecasters}},
	url = {http://arxiv.org/abs/2310.07820},
	abstract = {By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew Gordon},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07820 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{madaanLanguageModelsCode2022,
	title = {Language {Models} of {Code} are {Few}-{Shot} {Commonsense} {Learners}},
	url = {http://arxiv.org/abs/2210.07128},
	abstract = {We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event -- or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches ``serialize'' the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot setting.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming and Neubig, Graham},
	month = dec,
	year = {2022},
	note = {arXiv:2210.07128 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{pianykhImprovingHealthcareOperations2020,
	title = {Improving healthcare operations management with machine learning},
	volume = {2},
	number = {5},
	journal = {Nature Machine Intelligence},
	author = {Pianykh, Oleg S. and Guitron, Steven and Parke, Darren and Zhang, Chengzhao and Pandharipande, Pari and Brink, James and Rosenthal, Daniel},
	month = may,
	year = {2020},
	note = {Publisher: Springer Science and Business Media LLC},
	pages = {266--273},
}

@misc{serafiniLogicTensorNetworks2016,
	title = {Logic {Tensor} {Networks}: {Deep} {Learning} and {Logical} {Reasoning} from {Data} and {Knowledge}},
	shorttitle = {Logic {Tensor} {Networks}},
	url = {http://arxiv.org/abs/1606.04422},
	abstract = {We propose Logic Tensor Networks: a uniform framework for integrating automatic learning and reasoning. A logic formalism called Real Logic is defined on a first-order language whereby formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as feature vectors of real numbers. Real Logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google's tensorflow primitives. The paper concludes with experiments applying Logic Tensor Networks on a simple but representative example of knowledge completion.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Serafini, Luciano and Garcez, Artur d'Avila},
	month = jul,
	year = {2016},
	note = {arXiv:1606.04422 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{badreddineLogicTensorNetworks2022,
	title = {Logic {Tensor} {Networks}},
	volume = {303},
	issn = {00043702},
	url = {http://arxiv.org/abs/2012.13635},
	doi = {10.1016/j.artint.2021.103649},
	abstract = {Artificial Intelligence agents are required to learn from their surroundings and to reason about the knowledge that has been learned in order to make decisions. While state-of-the-art learning from data typically uses sub-symbolic distributed representations, reasoning is normally useful at a higher level of abstraction with the use of a first-order logic language for knowledge representation. As a result, attempts at combining symbolic AI and neural computation into neural-symbolic systems have been on the increase. In this paper, we present Logic Tensor Networks (LTN), a neurosymbolic formalism and computational model that supports learning and reasoning through the introduction of a many-valued, end-to-end differentiable first-order logic called Real Logic as a representation language for deep learning. We show that LTN provides a uniform language for the specification and the computation of several AI tasks such as data clustering, multi-label classification, relational learning, query answering, semi-supervised learning, regression and embedding learning. We implement and illustrate each of the above tasks with a number of simple explanatory examples using TensorFlow 2. Keywords: Neurosymbolic AI, Deep Learning and Reasoning, Many-valued Logic.},
	urldate = {2023-09-19},
	journal = {Artificial Intelligence},
	author = {Badreddine, Samy and Garcez, Artur d'Avila and Serafini, Luciano and Spranger, Michael},
	month = feb,
	year = {2022},
	note = {arXiv:2012.13635 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
	pages = {103649},
}

@article{zhouInformerEfficientTransformer2021,
	title = {Informer: {Beyond} {Efficient} {Transformer} for {Long} {Sequence} {Time}-{Series} {Forecasting}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Informer},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17325},
	doi = {10.1609/aaai.v35i12.17325},
	abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.},
	language = {en},
	number = {12},
	urldate = {2023-08-21},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {Energy, Environment \& Sustainability},
	pages = {11106--11115},
}

@article{thirunavukarasuLargeLanguageModels2023,
	title = {Large language models in medicine},
	volume = {29},
	copyright = {2023 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02448-8},
	doi = {10.1038/s41591-023-02448-8},
	abstract = {Large language models (LLMs) can respond to free-text queries without being specifically trained in the task in question, causing excitement and concern about their use in healthcare settings. ChatGPT is a generative artificial intelligence (AI) chatbot produced through sophisticated fine-tuning of an LLM, and other tools are emerging through similar developmental processes. Here we outline how LLM applications such as ChatGPT are developed, and we discuss how they are being leveraged in clinical settings. We consider the strengths and limitations of LLMs and their potential to improve the efficiency and effectiveness of clinical, educational and research work in medicine. LLM chatbots have already been deployed in a range of biomedical contexts, with impressive but mixed results. This review acts as a primer for interested clinicians, who will determine if and how LLM technology is used in healthcare for the benefit of patients and practitioners.},
	language = {en},
	number = {8},
	urldate = {2023-08-21},
	journal = {Nature Medicine},
	author = {Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
	month = aug,
	year = {2023},
	note = {Number: 8
Publisher: Nature Publishing Group},
	keywords = {Patient education, Translational research},
	pages = {1930--1940},
}

@misc{singhalLargeLanguageModels2022,
	title = {Large {Language} {Models} {Encode} {Clinical} {Knowledge}},
	url = {http://arxiv.org/abs/2212.13138},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6\% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17\%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery, Aakanksha and Mansfield, Philip and Arcas, Blaise Aguera y and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13138 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{radfordImprovingLanguageUnderstanding,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
}

@article{thimInitialAssessmentTreatment2012,
	title = {Initial assessment and treatment with the {Airway}, {Breathing}, {Circulation}, {Disability}, {Exposure} ({ABCDE}) approach},
	issn = {1178-7074},
	journal = {International journal of general medicine},
	author = {Thim, Troels and Krarup, Niels Henrik Vinther and Grove, Erik Lerkevang and Rohde, Claus Valter and Løfgren, Bo},
	year = {2012},
	note = {Publisher: Taylor \& Francis},
	pages = {117--121},
}

@misc{nawrotHierarchicalTransformersAre2022,
	title = {Hierarchical {Transformers} {Are} {More} {Efficient} {Language} {Models}},
	url = {http://arxiv.org/abs/2110.13711},
	doi = {10.48550/arXiv.2110.13711},
	abstract = {Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Nawrot, Piotr and Tworkowski, Szymon and Tyrolski, Michał and Kaiser, Łukasz and Wu, Yuhuai and Szegedy, Christian and Michalewski, Henryk},
	month = apr,
	year = {2022},
	note = {arXiv:2110.13711 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{liventsevFullyAutonomousProgramming2023,
	title = {Fully {Autonomous} {Programming} with {Large} {Language} {Models}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2304.10423},
	doi = {10.1145/3583131.3590481},
	abstract = {Current approaches to program synthesis with Large Language Models (LLMs) exhibit a "near miss syndrome": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format. This calls for an approach known as Synthesize, Execute, Debug (SED), whereby a draft of the solution is generated first, followed by a program repair phase addressing the failed tests. To effectively apply this approach to instruction-driven LLMs, one needs to determine which prompts perform best as instructions for LLMs, as well as strike a balance between repairing unsuccessful programs and replacing them with newly generated ones. We explore these trade-offs empirically, comparing replace-focused, repair-focused, and hybrid debug strategies, as well as different template-based and model-based prompt-generation techniques. We use OpenAI Codex as the LLM and Program Synthesis Benchmark 2 as a database of problem descriptions and tests for evaluation. The resulting framework outperforms both conventional usage of Codex without the repair phase and traditional genetic programming approaches.},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	author = {Liventsev, Vadim and Grishina, Anastasiia and Härmä, Aki and Moonen, Leon},
	month = jul,
	year = {2023},
	note = {arXiv:2304.10423 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Software Engineering},
	pages = {1146--1155},
}

@article{thelancetHealthcareSystemStaffing2018,
	title = {Health-care system staffing: a universal shortfall},
	volume = {392},
	number = {10161},
	journal = {The Lancet},
	author = {{The Lancet}},
	month = nov,
	year = {2018},
	note = {Publisher: Elsevier BV},
	pages = {2238},
}

@article{ashleyy.metcalfHospitalUnitUnderstaffing2016,
	title = {Hospital {Unit} {Understaffing} and {Missed} {Treatments}: {The} {Moderating} {Effect} of {Teamwork}.},
	author = {{Ashley Y. Metcalf}},
	year = {2016},
}

@article{tortorellaHealthcareTrendsChallenges2020,
	title = {Healthcare 4.0: trends, challenges and research directions},
	volume = {31},
	issn = {0953-7287},
	shorttitle = {Healthcare 4.0},
	url = {https://doi.org/10.1080/09537287.2019.1702226},
	doi = {10.1080/09537287.2019.1702226},
	abstract = {This paper aims at examining the trends, challenges and theoretical gaps in the implementation of Healthcare 4.0 (H4.0) based on a scoping review of the literature. For that, we searched journal articles in four widely known databases and screened the retrieved articles to obtain a publications’ portfolio. Our findings indicate that, despite the recency of the subject, research in H4.0 has been conducted in an interdisciplinary way with a diversified set of applications and functionalities. In terms of its implementation, H4.0 has been more commonly found in hospitals’ information flows, especially the ones related to healthcare treatments. The identified implementation trends, however, neglect a more holistic approach for H4.0, which originated three main research directions for this topic. Although identified as a trending topic in the area of healthcare operations management, literature on H4.0 may be viewed as randomly conceived, lacking academic alignment and practical orientation based on a grounded theory, which we aim at providing with the present study.},
	number = {15},
	urldate = {2023-10-11},
	journal = {Production Planning \& Control},
	author = {Tortorella, Guilherme Luz and Fogliatto, Flávio Sanson and Mac Cawley Vergara, Alejandro and Vassolo, Roberto and Sawhney, Rapinder},
	month = nov,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/09537287.2019.1702226},
	keywords = {Healthcare 4.0, healthcare operations management, literature review, smart healthcare},
	pages = {1245--1260},
}

@inproceedings{zhouFEDformerFrequencyEnhanced2022,
	title = {{FEDformer}: {Frequency} {Enhanced} {Decomposed} {Transformer} for {Long}-term {Series} {Forecasting}},
	shorttitle = {{FEDformer}},
	url = {https://proceedings.mlr.press/v162/zhou22g.html},
	abstract = {Long-term time series forecasting is challenging since prediction accuracy tends to decrease dramatically with the increasing horizon. Although Transformer-based methods have significantly improved state-of-the-art results for long-term forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in a well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer (FEDformer), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, Fedformer can reduce prediction error by 14.8\% and 22.6\% for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.},
	language = {en},
	urldate = {2023-08-21},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {27268--27286},
}

@misc{liangHolisticEvaluationLanguage2022,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	urldate = {2023-07-14},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ramsauerHopfieldNetworksAll2021,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = apr,
	year = {2021},
	note = {arXiv:2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{shinFewShotSemanticParsing2022,
	title = {Few-{Shot} {Semantic} {Parsing} with {Language} {Models} {Trained} {On} {Code}},
	url = {http://arxiv.org/abs/2112.08696},
	abstract = {Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Shin, Richard and Van Durme, Benjamin},
	month = may,
	year = {2022},
	note = {arXiv:2112.08696 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{GeneticAlgorithmProgram,
	title = {Genetic {Algorithm} for {Program} {Synthesis}. ({arXiv}:2211.11937v1 [cs.{NE}])},
	url = {https://arxiv.org/abs/2211.11937?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound},
	doi = {arXiv:2211.11937v1},
	abstract = {A deductive program synthesis tool takes a specification as input and derives
a program that satisfies the specification. The drawback of this approach is
that search spaces for such correct programs tend to be enormous, making it
difficult to derive correct programs within a realistic timeout. To speed up
such program derivation, we improve the search strategy of a deductive program
synthesis tool, SuSLik, using evolutionary computation. Our cross-validation
shows that the improvement brought by evolutionary computation generalises to
unforeseen problems.},
	journal = {arXiv Computer Science},
	keywords = {Researcher App},
}

@misc{gowdaDemystifyingDatabaseTranscations2023,
	title = {Demystifying {Database} {Transcations}},
	url = {https://dineshgowda.com/posts/demystifying-database-transcations/},
	abstract = {The most compelling feature of relational database systems is ACID(Atomicity Consistency Isolation Durability). Isolation is achieved using transactions. Isolation is needed to avoid race conditions when concurrent actors act upon the same row.
SQL standard defines isolation with different levels. Every isolation level offers certain guarantees and possible anomalies that can occur.},
	language = {en},
	urldate = {2023-09-28},
	journal = {Dinesh Gowda},
	author = {gowda, dinesh},
	month = jul,
	year = {2023},
	note = {Section: posts},
}

@article{meznarEfficientGeneratorMathematical2023,
	title = {Efficient {Generator} of {Mathematical} {Expressions} for {Symbolic} {Regression}},
	issn = {0885-6125, 1573-0565},
	url = {http://arxiv.org/abs/2302.09893},
	doi = {10.1007/s10994-023-06400-2},
	abstract = {We propose an approach to symbolic regression based on a novel variational autoencoder for generating hierarchical structures, HVAE. It combines simple atomic units with shared weights to recursively encode and decode the individual nodes in the hierarchy. Encoding is performed bottom-up and decoding top-down. We empirically show that HVAE can be trained efficiently with small corpora of mathematical expressions and can accurately encode expressions into a smooth low-dimensional latent space. The latter can be efficiently explored with various optimization methods to address the task of symbolic regression. Indeed, random search through the latent space of HVAE performs better than random search through expressions generated by manually crafted probabilistic grammars for mathematical expressions. Finally, EDHiE system for symbolic regression, which applies an evolutionary algorithm to the latent space of HVAE, reconstructs equations from a standard symbolic regression benchmark better than a state-of-the-art system based on a similar combination of deep learning and evolutionary algorithms.{\textbackslash}v\{z\}},
	urldate = {2023-09-20},
	journal = {Machine Learning},
	author = {Mežnar, Sebastian and Džeroski, Sašo and Todorovski, Ljupčo},
	month = sep,
	year = {2023},
	note = {arXiv:2302.09893 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.0, I.2.6},
}

@misc{mcdermottEventStreamGPT2023,
	title = {Event {Stream} {GPT}: {A} {Data} {Pre}-processing and {Modeling} {Library} for {Generative}, {Pre}-trained {Transformers} over {Continuous}-time {Sequences} of {Complex} {Events}},
	shorttitle = {Event {Stream} {GPT}},
	url = {http://arxiv.org/abs/2306.11547},
	abstract = {Generative, pre-trained transformers (GPTs, a.k.a. "Foundation Models") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper provides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that incorporates intra-event causal dependency structures and autoregressive generation capabilities, and (3) evaluate models via standardized processes that can assess few and even zero-shot performance of pre-trained models on user-specified fine-tuning tasks.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {McDermott, Matthew B. A. and Nestor, Bret and Argaw, Peniel and Kohane, Isaac},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11547 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{chenEvaluatingLargeLanguage2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{maEurekaHumanLevelReward2023,
	title = {Eureka: {Human}-{Level} {Reward} {Design} via {Coding} {Large} {Language} {Models}},
	journal = {arXiv preprint arXiv:2310.12931},
	author = {Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	year = {2023},
}

@article{deviDesignImplementationAdvanced2022,
	title = {Design and {Implementation} of {Advanced} {Machine} {Learning} {Management} and {Its} {Impact} on {Better} {Healthcare} {Services}: {A} {Multiple} {Regression} {Analysis} {Approach} ({MRAA})},
	volume = {2022},
	journal = {Computational and Mathematical Methods in Medicine},
	author = {Devi, M. Kiruthiga and Vemuri, Veena Prasad and Arumugam, Mahalakshmi and UmaMaheswaran, S. K. and Acharjee, Purnendu Bikash and Singh, Rupali and Kaliyaperumal, Karthikeyan},
	editor = {Koundal, Deepika},
	month = apr,
	year = {2022},
	note = {Publisher: Hindawi Limited},
	pages = {1--7},
}

@misc{hazraDeepExplainableRelational2023,
	title = {Deep {Explainable} {Relational} {Reinforcement} {Learning}: {A} {Neuro}-{Symbolic} {Approach}},
	shorttitle = {Deep {Explainable} {Relational} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2304.08349},
	abstract = {Despite numerous successes in Deep Reinforcement Learning (DRL), the learned policies are not interpretable. Moreover, since DRL does not exploit symbolic relational representations, it has difficulties in coping with structural changes in its environment (such as increasing the number of objects). Relational Reinforcement Learning, on the other hand, inherits the relational representations from symbolic planning to learn reusable policies. However, it has so far been unable to scale up and exploit the power of deep neural networks. We propose Deep Explainable Relational Reinforcement Learning (DERRL), a framework that exploits the best of both -- neural and symbolic worlds. By resorting to a neuro-symbolic approach, DERRL combines relational representations and constraints from symbolic planning with deep learning to extract interpretable policies. These policies are in the form of logical rules that explain how each decision (or action) is arrived at. Through several experiments, in setups like the Countdown Game, Blocks World, Gridworld, and Traffic, we show that the policies learned by DERRL can be applied to different configurations and contexts, hence generalizing to environmental modifications.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Hazra, Rishi and De Raedt, Luc},
	month = jul,
	year = {2023},
	note = {arXiv:2304.08349 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{lehmanWeStillNeed2023,
	title = {Do {We} {Still} {Need} {Clinical} {Language} {Models}?},
	url = {https://proceedings.mlr.press/v209/eric23a.html},
	abstract = {Although recent advances in scaling large language models (LLMs) have resulted in improvements on many NLP tasks, it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as {\textbackslash}emph\{clinical text\}. Recent results have suggested that LLMs encode a surprising amount of medical knowledge.  This raises an important question regarding the utility of smaller domain-specific language models. With the success of general-domain LLMs, is there still a need for specialized clinical models?  To investigate this question, we conduct an extensive empirical analysis of 12 language models, ranging from 220M to 175B parameters, measuring their performance on 3 different clinical tasks that test their ability to parse and reason over electronic health records. As part of our experiments, we train T5-Base and T5-Large models from scratch on clinical notes from MIMIC III and IV to directly investigate the efficiency of clinical tokens. We show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when finetuned on limited annotated data.  Further, we find that pretraining on clinical tokens allows for smaller, more parameter-efficient models that either match or outperform much larger language models trained on general text.  We release the code and the models used under the PhysioNet Credentialed Health Data license and data use agreement.{\textbackslash}footnote\{{\textbackslash}href\{https://github.com/elehman16/clinical\_llm\}\{https://github.com/elehman16/clinical\_llm\}\}},
	language = {en},
	urldate = {2023-08-24},
	booktitle = {Proceedings of the {Conference} on {Health}, {Inference}, and {Learning}},
	publisher = {PMLR},
	author = {Lehman, Eric and Hernandez, Evan and Mahajan, Diwakar and Wulff, Jonas and Smith, Micah J. and Ziegler, Zachary and Nadler, Daniel and Szolovits, Peter and Johnson, Alistair and Alsentzer, Emily},
	month = jun,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {578--597},
}

@inproceedings{sunLongRangeLanguageModels2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Do {Long}-{Range} {Language} {Models} {Actually} {Use} {Long}-{Range} {Context}?},
	url = {https://aclanthology.org/2021.emnlp-main.62},
	doi = {10.18653/v1/2021.emnlp-main.62},
	abstract = {Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).},
	urldate = {2023-08-23},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew and Iyyer, Mohit},
	month = nov,
	year = {2021},
	pages = {807--822},
}

@inproceedings{rodinEgocentricActionAnticipation2023,
	title = {Egocentric {Action} {Anticipation} for {Personal} {Health}},
	doi = {10.1109/ICASSP49357.2023.10096388},
	abstract = {The egocentric action anticipation task consists in predicting future (unobserved) actions based on input from a wearable first-person view camera. In this work, we are focusing on applying egocentric action anticipation methods to the personal health domain, i.e., utilizing them for the analysis of dietary and hygienic activities routine and for the prevention of undesirable behavior (such as tasting food before washing hands or adding sugar). We collect a dataset of egocentric videos, capturing specific activities related to food preparation; fine-tune existing action anticipation models on this dataset and analyse effects caused by domain shift.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Rodin, Ivan and Furnari, Antonino and Mavroeidis, Dimitrios and Farinella, Giovanni Maria},
	month = jun,
	year = {2023},
	keywords = {Adaptation models, Cameras, Domain Shift, Egocentric Action Anticipation, Focusing, Personal Health, Predictive models, Signal processing, Speech processing, Task analysis},
	pages = {1--5},
}

@misc{sukhbaatarAugmentingSelfattentionPersistent2019,
	title = {Augmenting {Self}-attention with {Persistent} {Memory}},
	url = {http://arxiv.org/abs/1907.01470},
	abstract = {Transformer networks have lead to important progress in language modeling and machine translation. These models include two consecutive modules, a feed-forward layer and a self-attention layer. The latter allows the network to capture long term dependencies and are often regarded as the key ingredient in the success of Transformers. Building upon this intuition, we propose a new model that solely consists of attention layers. More precisely, we augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer. Thanks to these vectors, we can remove the feed-forward layer without degrading the performance of a transformer. Our evaluation shows the benefits brought by our model on standard character and word level language modeling benchmarks.},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
	month = jul,
	year = {2019},
	note = {arXiv:1907.01470 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{vaswaniAttentionAllYou2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{CodeLlamaOpen,
	title = {Code {Llama}: {Open} {Foundation} {Models} for {Code} {\textbar} {Meta} {AI} {Research}},
	url = {https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/},
	urldate = {2023-08-28},
}

@article{liCompetitionLevelCodeGeneration2022,
	title = {Competition-{Level} {Code} {Generation} with {AlphaCode}},
	volume = {378},
	issn = {0036-8075, 1095-9203},
	url = {http://arxiv.org/abs/2203.07814},
	doi = {10.1126/science.abq1158},
	abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
	number = {6624},
	urldate = {2023-08-28},
	journal = {Science},
	author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and d'Autume, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
	month = dec,
	year = {2022},
	note = {arXiv:2203.07814 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
	pages = {1092--1097},
}

@article{arroyoComparisonOptimalControl2022,
	title = {Comparison of {Optimal} {Control} {Techniques} for {Building} {Energy} {Management}},
	volume = {8},
	issn = {2297-3362},
	url = {https://www.frontiersin.org/articles/10.3389/fbuil.2022.849754},
	abstract = {Optimal controllers can enhance buildings’ energy efficiency by taking forecast and uncertainties into account (e.g., weather and occupancy). This practice results in energy savings by making better use of energy systems within the buildings. Even though the benefits of advanced optimal controllers have been demonstrated in several research studies and some demonstration cases, the adoption of these techniques in the built environment remains somewhat limited. One of the main reasons is that these novel control algorithms continue to be evaluated individually. This hampers the identification of best practices to deploy optimal control widely in the building sector. This paper implements and compares variations of model predictive control (MPC), reinforcement learning (RL), and reinforced model predictive control (RL-MPC) in the same optimal control problem for building energy management. Particularly, variations of the controllers’ hyperparameters like the control step, the prediction horizon, the state-action spaces, the learning algorithm, or the network architecture of the value function are investigated. The building optimization testing (BOPTEST) framework is used as the simulation benchmark to carry out the study as it offers standardized testing scenarios. The results reveal that, contrary to what is stated in previous literature, model-free RL approaches poorly perform when tested in building environments with realistic system dynamics. Even when a model is available and simulation-based RL can be implemented, MPC outperforms RL for an equivalent formulation of the optimal control problem. The performance gap between both controllers reduces when using the RL-MPC algorithm that merges elements from both families of methods.},
	urldate = {2023-08-31},
	journal = {Frontiers in Built Environment},
	author = {Arroyo, Javier and Spiessens, Fred and Helsen, Lieve},
	year = {2022},
}

@article{liBEHRTTransformerElectronic2020,
	title = {{BEHRT}: {Transformer} for {Electronic} {Health} {Records}},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	shorttitle = {{BEHRT}},
	url = {https://www.nature.com/articles/s41598-020-62922-y},
	doi = {10.1038/s41598-020-62922-y},
	abstract = {Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (including deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for electronic health records (EHR), capable of simultaneously predicting the likelihood of 301 conditions in one’s future visits. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking improvement of 8.0–13.2\% (in terms of average precision scores for different tasks), over the existing state-of-the-art deep EHR models. In addition to its scalability and superior accuracy, BEHRT enables personalised interpretation of its predictions; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to further improve the accuracy of its predictions; its (pre-)training results in disease and patient representations can be useful for future studies (i.e., transfer learning).},
	language = {en},
	number = {1},
	urldate = {2023-08-22},
	journal = {Scientific Reports},
	author = {Li, Yikuan and Rao, Shishir and Solares, José Roberto Ayala and Hassaine, Abdelaali and Ramakrishnan, Rema and Canoy, Dexter and Zhu, Yajie and Rahimi, Kazem and Salimi-Khorshidi, Gholamreza},
	month = apr,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Experimental models of disease, Preventive medicine},
	pages = {7155},
}

@inproceedings{wuAutoformerDecompositionTransformers2021,
	title = {Autoformer: {Decomposition} {Transformers} with {Auto}-{Correlation} for {Long}-{Term} {Series} {Forecasting}},
	volume = {34},
	shorttitle = {Autoformer},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html},
	abstract = {Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38\% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: https://github.com/thuml/Autoformer.},
	urldate = {2023-08-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
	year = {2021},
	pages = {22419--22430},
}

@misc{devlinBERTPretrainingDeep2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chenDecisionTransformerReinforcement2021,
	title = {Decision {Transformer}: {Reinforcement} {Learning} via {Sequence} {Modeling}},
	shorttitle = {Decision {Transformer}},
	url = {http://arxiv.org/abs/2106.01345},
	abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	month = jun,
	year = {2021},
	note = {arXiv:2106.01345 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{shinConstrainedLanguageModels2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Constrained {Language} {Models} {Yield} {Few}-{Shot} {Semantic} {Parsers}},
	url = {https://aclanthology.org/2021.emnlp-main.608},
	doi = {10.18653/v1/2021.emnlp-main.608},
	abstract = {We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.},
	urldate = {2023-05-23},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Shin, Richard and Lin, Christopher and Thomson, Sam and Chen, Charles and Roy, Subhro and Platanios, Emmanouil Antonios and Pauls, Adam and Klein, Dan and Eisner, Jason and Van Durme, Benjamin},
	month = nov,
	year = {2021},
	pages = {7699--7715},
}

@article{abuhamadAutomatedSonography2007,
	title = {Automated {Sonography}},
	volume = {26},
	issn = {1550-9613},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.7863/jum.2007.26.4.501},
	doi = {10.7863/jum.2007.26.4.501},
	abstract = {Objectives This study defines the spatial relationship of the diagnostic planes of the fetal heart to the 4-chamber view plane in the second trimester of pregnancy as a first step in the automation process. Methods Three-dimensional static volumes of the fetal chest were acquired at the level of the 4-chamber view on 75 fetuses between 18 and 23 weeks' gestation. The spatial relationship of the diagnostic cardiac planes to the 4-chamber view plane were determined for each gestational week by using rotations along the x-, y-, and z-axes and a parallel slide (millimeters) when applicable. Results The 5-chamber view (cardiac 1 plane) was best obtained by an initial parallel slide of the reference plane (plane A) toward the fetal head followed by a rotation along the y-axis. The right ventricular outflow tract (cardiac 2) and the abdominal circumference (cardiac 3) planes were best obtained by a parallel slide only: toward the fetal head in cardiac 2 and toward the fetal abdomen in cardiac 3. Conclusions This study shows the spatial relationship of fetal cardiac diagnostic planes to the 4-chamber view plane in the second trimester of pregnancy in 3-dimensional volumes. Testing the clinical applicability of automated software based on these formulas is the next step.},
	language = {en},
	number = {4},
	urldate = {2023-05-15},
	journal = {Journal of Ultrasound in Medicine},
	author = {Abuhamad, Alfred and Falkensammer, Peter and Zhao, Yueqin},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.7863/jum.2007.26.4.501},
	keywords = {3-dimensional sonography, automated sonography, fetal echocardiography, volume sonography},
	pages = {501--507},
}

@misc{shuklaSurveyPrinciplesModels2021,
	title = {A {Survey} on {Principles}, {Models} and {Methods} for {Learning} from {Irregularly} {Sampled} {Time} {Series}},
	url = {http://arxiv.org/abs/2012.00168},
	doi = {10.48550/arXiv.2012.00168},
	abstract = {Irregularly sampled time series data arise naturally in many application domains including biology, ecology, climate science, astronomy, and health. Such data represent fundamental challenges to many classical models from machine learning and statistics due to the presence of non-uniform intervals between observations. However, there has been significant progress within the machine learning community over the last decade on developing specialized models and architectures for learning from irregularly sampled univariate and multivariate time series data. In this survey, we first describe several axes along which approaches to learning from irregularly sampled time series differ including what data representations they are based on, what modeling primitives they leverage to deal with the fundamental problem of irregular sampling, and what inference tasks they are designed to perform. We then survey the recent literature organized primarily along the axis of modeling primitives. We describe approaches based on temporal discretization, interpolation, recurrence, attention and structural invariance. We discuss similarities and differences between approaches and highlight primary strengths and weaknesses.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Shukla, Satya Narayan and Marlin, Benjamin M.},
	month = jan,
	year = {2021},
	note = {arXiv:2012.00168 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{weerakodyReviewIrregularTime2021,
	title = {A review of irregular time series data handling with gated recurrent neural networks},
	volume = {441},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221003003},
	doi = {10.1016/j.neucom.2021.02.046},
	abstract = {Irregular time series data is becoming increasingly prevalent with the growth of multi-sensor systems as well as the continued use of unstructured manual data recording mechanisms. Irregular data and the resulting missing values severely limit the data’s ability to be analysed and modelled for classiﬁcation and forecasting tasks. Often, conventional methods used for handling time series data introduce bias and make strong assumptions on the underlying data generation process, which can lead to poor model predictions. Traditional machine learning and deep learning methods, although at the forefront of data modelling, are at best compromised by irregular time series data sets and fail to model the temporal irregularity of incomplete time series. Gated recurrent neural networks (RNN), such as LSTM and GRU, have had outstanding success in sequential modelling, and have been applied in many application ﬁelds, including natural language processing. These models have become an obvious choice for time series modelling and a promising tool for handling irregular time series data. RNNs have a unique ability to be adapted to make effective use of missing value patterns, time intervals and complex temporal dependencies in irregular univariate and multivariate time series data. In this paper, we provide a systematic review of recent studies in which gated recurrent neural networks have been successfully applied to irregular time series data for prediction tasks within several ﬁelds, including medical, human activity recognition, trafﬁc monitoring and environmental monitoring. The review highlights the two common approaches for handling irregular time series data: missing value imputation at the data preprocessing stage and modiﬁcation of algorithms to directly handle missing values in the learning process. Reviewed models are conﬁned to those that can address issues with irregular time series data and does not cover the broader range of models that deal more generally with sequences and regular time series. This paper aims to present the most effective techniques emerging within this branch of research as well as to identify remaining challenges, so that researchers may build upon this platform of work towards further novel techniques for handling irregular time series data.},
	language = {en},
	urldate = {2023-06-05},
	journal = {Neurocomputing},
	author = {Weerakody, Philip B. and Wong, Kok Wai and Wang, Guanjin and Ela, Wendell},
	month = jun,
	year = {2021},
	pages = {161--178},
}

@incollection{zhangAdacketADAptiveConvolutional2023,
	address = {Cham},
	title = {Adacket: {ADAptive} {Convolutional} {KErnel} {Transform} for {Multivariate} {Time} {Series} {Classification}},
	volume = {14173},
	isbn = {978-3-031-43423-5 978-3-031-43424-2},
	shorttitle = {Adacket},
	url = {https://link.springer.com/10.1007/978-3-031-43424-2_12},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}: {Research} {Track}},
	publisher = {Springer Nature Switzerland},
	author = {Zhang, Junru and Feng, Lang and Zhang, Haowen and Wu, Yuhan and Dong, Yabo},
	editor = {Koutra, Danai and Plant, Claudia and Gomez Rodriguez, Manuel and Baralis, Elena and Bonchi, Francesco},
	year = {2023},
	doi = {10.1007/978-3-031-43424-2_12},
	pages = {189--204},
}

@article{kolterAlphaCodeDatadrivenProgramming2022,
	title = {{AlphaCode} and “data-driven” programming},
	volume = {378},
	url = {https://www.science.org/doi/10.1126/science.add8258},
	doi = {10.1126/science.add8258},
	number = {6624},
	urldate = {2023-02-09},
	journal = {Science},
	author = {Kolter, J. Zico},
	month = dec,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1056--1056},
}

@article{g.kumarSurveyMachineLearning2016,
	title = {A survey on {Machine} {Learning} {Techniques} in {Health} {Care} {Industry}},
	author = {{G. Kumar} and {Rohit Kalra}},
	year = {2016},
}

@misc{PassageRetrievalTransformerBased,
	title = {A {Passage} {Retrieval} {Transformer}-{Based} {Re}-{Ranking} {Model} for {Truthful} {Consumer} {Health} {Search}},
	url = {https://www.springerprofessional.de/a-passage-retrieval-transformer-based-re-ranking-model-for-truth/26051688},
	abstract = {Searching for online information is nowadays a critical task in a scenario characterized by information overload and misinformation. To address these issues, it is necessary to provide users with both topically relevant and truthful information.},
	language = {de},
	urldate = {2023-09-19},
	journal = {springerprofessional.de},
}

@misc{luAreEmergentAbilities2023,
	title = {Are {Emergent} {Abilities} in {Large} {Language} {Models} just {In}-{Context} {Learning}?},
	url = {http://arxiv.org/abs/2309.01809},
	doi = {10.48550/arXiv.2309.01809},
	abstract = {Large language models have exhibited emergent abilities, demonstrating exceptional performance across diverse tasks for which they were not explicitly trained, including those that require complex reasoning abilities. The emergence of such abilities carries profound implications for the future direction of research in NLP, especially as the deployment of such models becomes more prevalent. However, one key challenge is that the evaluation of these abilities is often confounded by competencies that arise in models through alternative prompting techniques, such as in-context learning and instruction following, which also emerge as the models are scaled up. In this study, we provide the first comprehensive examination of these emergent abilities while accounting for various potentially biasing factors that can influence the evaluation of models. We conduct rigorous tests on a set of 18 models, encompassing a parameter range from 60 million to 175 billion parameters, across a comprehensive set of 22 tasks. Through an extensive series of over 1,000 experiments, we provide compelling evidence that emergent abilities can primarily be ascribed to in-context learning. We find no evidence for the emergence of reasoning abilities, thus providing valuable insights into the underlying mechanisms driving the observed abilities and thus alleviating safety concerns regarding their use.},
	urldate = {2023-09-08},
	publisher = {arXiv},
	author = {Lu, Sheng and Bigoulaeva, Irina and Sachdeva, Rachneet and Madabushi, Harish Tayyar and Gurevych, Iryna},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01809 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wangAreLargeLanguage2023,
	title = {Are {Large} {Language} {Models} {Ready} for {Healthcare}? {A} {Comparative} {Study} on {Clinical} {Language} {Understanding}},
	shorttitle = {Are {Large} {Language} {Models} {Ready} for {Healthcare}?},
	url = {http://arxiv.org/abs/2304.05368},
	doi = {10.48550/arXiv.2304.05368},
	abstract = {Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks. Additionally, our in-depth error analysis on the challenging relation extraction task offers valuable insights into error distribution and potential avenues for improvement using SQP. Our study sheds light on the practical implications of employing LLMs in the specialized domain of healthcare, serving as a foundation for future research and the development of potential applications in healthcare settings.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Wang, Yuqing and Zhao, Yun and Petzold, Linda},
	month = jul,
	year = {2023},
	note = {arXiv:2304.05368 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{sunReviewDeepLearning2020,
	title = {A {Review} of {Deep} {Learning} {Methods} for {Irregularly} {Sampled} {Medical} {Time} {Series} {Data}},
	url = {http://arxiv.org/abs/2010.12493},
	doi = {10.48550/arXiv.2010.12493},
	abstract = {Irregularly sampled time series (ISTS) data has irregular temporal intervals between observations and different sampling rates between sequences. ISTS commonly appears in healthcare, economics, and geoscience. Especially in the medical environment, the widely used Electronic Health Records (EHRs) have abundant typical irregularly sampled medical time series (ISMTS) data. Developing deep learning methods on EHRs data is critical for personalized treatment, precise diagnosis and medical management. However, it is challenging to directly use deep learning models for ISMTS data. On the one hand, ISMTS data has the intra-series and inter-series relations. Both the local and global structures should be considered. On the other hand, methods should consider the trade-off between task accuracy and model complexity and remain generality and interpretability. So far, many existing works have tried to solve the above problems and have achieved good results. In this paper, we review these deep learning methods from the perspectives of technology and task. Under the technology-driven perspective, we summarize them into two categories - missing data-based methods and raw data-based methods. Under the task-driven perspective, we also summarize them into two categories - data imputation-oriented and downstream task-oriented. For each of them, we point out their advantages and disadvantages. Moreover, we implement some representative methods and compare them on four medical datasets with two tasks. Finally, we discuss the challenges and opportunities in this area.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Sun, Chenxi and Hong, Shenda and Song, Moxian and Li, Hongyan},
	month = oct,
	year = {2020},
	note = {arXiv:2010.12493 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{glanoisSurveyInterpretableReinforcement2022,
	title = {A {Survey} on {Interpretable} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2112.13112},
	abstract = {Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
	month = feb,
	year = {2022},
	note = {arXiv:2112.13112 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.6},
}

@misc{kamathSurveySemanticParsing2019,
	title = {A {Survey} on {Semantic} {Parsing}},
	url = {http://arxiv.org/abs/1812.00978},
	abstract = {A significant amount of information in today's world is stored in structured and semi-structured knowledge bases. Efficient and simple methods to query them are essential and must not be restricted to only those who have expertise in formal query languages. The field of semantic parsing deals with converting natural language utterances to logical forms that can be easily executed on a knowledge base. In this survey, we examine the various components of a semantic parsing system and discuss prominent work ranging from the initial rule based methods to the current neural approaches to program synthesis. We also discuss methods that operate using varying levels of supervision and highlight the key challenges involved in the learning of such systems.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Kamath, Aishwarya and Das, Rajarshi},
	month = may,
	year = {2019},
	note = {arXiv:1812.00978 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{balestrieroSplineTheoryDeep2018,
	title = {A {Spline} {Theory} of {Deep} {Learning}},
	url = {https://proceedings.mlr.press/v80/balestriero18b.html},
	abstract = {We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.},
	language = {en},
	urldate = {2022-11-25},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Balestriero, Randall and baraniuk},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {374--383},
}

@article{mercerMessageEditorinChief2008,
	title = {A {Message} from the {Editor}-in-{Chief}},
	volume = {21},
	number = {2},
	journal = {Healthcare Management Forum},
	author = {Mercer, Kevin},
	month = jun,
	year = {2008},
	note = {Publisher: SAGE Publications},
	pages = {4--4},
}

@misc{sinhaBetterWayLogging2021,
	title = {A better way to logging in {Python}},
	url = {https://ankitbko.github.io/blog/2021/04/logging-in-python/},
	abstract = {A generic logging decorator for python},
	language = {en},
	urldate = {2023-07-31},
	journal = {F5 - Squashing Bugs},
	author = {Sinha{\textless}/a{\textgreater}, target='\_blank'{\textgreater}Ankit, {\textless}a href='https://twitter com/ankitbko'},
	month = apr,
	year = {2021},
}

@article{adamatzkyBriefHistoryLiquid2019,
	title = {A brief history of liquid computers},
	volume = {374},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0372},
	doi = {10.1098/rstb.2018.0372},
	abstract = {A substrate does not have to be solid to compute. It is possible to make a computer purely from a liquid. I demonstrate this using a variety of experimental prototypes where a liquid carries signals, actuates mechanical computing devices and hosts chemical reactions. We show hydraulic mathematical machines that compute functions based on mass transfer analogies. I discuss several prototypes of computing devices that employ fluid flows and jets. They are fluid mappers, where the fluid flow explores a geometrically constrained space to find an optimal way around, e.g. the shortest path in a maze, and fluid logic devices where fluid jet streams interact at the junctions of inlets and results of the computation are represented by fluid jets at selected outlets. Fluid mappers and fluidic logic devices compute continuously valued functions albeit discretized. There is also an opportunity to do discrete operation directly by representing information by droplets and liquid marbles (droplets coated by hydrophobic powder). There, computation is implemented at the sites, in time and space, where droplets collide one with another. The liquid computers mentioned above use liquid as signal carrier or actuator: the exact nature of the liquid is not that important. What is inside the liquid becomes crucial when reaction–diffusion liquid-phase computing devices come into play: there, the liquid hosts families of chemical species that interact with each other in a massive-parallel fashion. I shall illustrate a range of computational tasks, including computational geometry, implementable by excitation wave fronts in nonlinear active chemical medium. The overview will enable scientists and engineers to understand how vast is the variety of liquid computers and will inspire them to design their own experimental laboratory prototypes.
            This article is part of the theme issue ‘Liquid brains, solid brains: How distributed cognitive architectures process information’.},
	language = {en},
	number = {1774},
	urldate = {2023-06-23},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Adamatzky, Andrew},
	month = jun,
	year = {2019},
	pages = {20180372},
}

@misc{reedGeneralistAgent2022,
	title = {A {Generalist} {Agent}},
	url = {http://arxiv.org/abs/2205.06175},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
	month = nov,
	year = {2022},
	note = {arXiv:2205.06175 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@book{gazhonova3DAutomatedBreast2016,
	address = {Cham, Switzerland},
	title = {{3D} automated breast volume sonography : a practical guide},
	isbn = {978-3-319-41971-8 3-319-41971-4},
	url = {https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=1340828},
	language = {English},
	publisher = {Springer},
	author = {Gazhonova, V. E. (Veronika Evgenʹevna)},
	year = {2016},
}

@misc{shinn2023:reflecting,
	type = {Substack newsletter},
	title = {Reflecting on {Reflexion}},
	url = {https://nanothoughts.substack.com/p/reflecting-on-reflexion},
	abstract = {Here’s a discussion on our recent paper with an additional experiment that uses GPT-4 to beat past GPT-4 standards. In our recent paper, "Reflexion: An Autonomous Agent with Dynamic Memory and Self-Reflection," we introduce a framework that allows AI agents to emulate human-like self-reflection and evaluate its performance on the},
	urldate = {2023-04-18},
	journal = {Nano Thoughts},
	author = {Shinn, Noah and Gopinath, Ashwin},
	month = mar,
	year = {2023},
}
