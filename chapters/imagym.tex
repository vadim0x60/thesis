\chapter{High-dimensional challenge: imagym}
\label{ch:imagym}
\todo{Make sure a research question is explicitly mentioned}

\section{Introduction}

Patient simulators are the cornerstone of clinical decision support solutions, providing a means to validate the efficacy of medical interventions in silico so cheaply that reinforcement learning algorithms can be used to develop protocols of clinical interventions automatically.
Unlike other machine learning applications, notably language modeling that can obtain a near-unlimited amount of data from the internet\footnote{See CommonCrawl \cite{commoncrawl} \url{commoncrawl.org}}, Healthcare suffers from an acute shortage of training data \cite{datashortage}.
As a result, many existing patient simulators are not trained on data from real patients, but instead rely upon expert knowledge, leading to issues like \emph{confirmation bias} (the types of interventions currently favoured by clinical experts will work well in simulators developed by said experts irregardless of their real-world performance) and a high Sim2Real \cite{sim2real} gap.

In this chapter, we utilize a dataset of fetal ultrasound images to propose a novel data-driven patient simulator for decision support and automation in the field of obstetric ultrasonography \cite{obstetrics-sonography}.

The goal of this simulator is to accurately model the job of an ultrasound sonographer in the context of a patient undergoing a pregnancy in a way compatible with modern Reinforcement Learning methods \cite{liDeepReinforcementLearning2017}
to pave the way for autonomous or semi-autonomous ultrasound sonography \cite{autonomous-ultrasound-review}.
The job in question entails moving the ultrasound probe along the patient's body in order to acquire an image of the fetus that satisfies the guidelines for fetal-screening \cite{isoug-guidelines}, most importantly, the fact that the fetus' stomach and umbilical vein are on the image while their heart is not.

\newpage
\section{Methodology}

\paragraph{Dataset}

\begin{figure}
    \centering
    \begin{subfigure}{.45\linewidth}
      \centering
      \includegraphics[width=.95\linewidth]{images/fetal_start.PNG}
      \caption{Starting point: the probe is located at the exact average of available positions. Heart, stomach and umbilical vein are unseen.}
      \label{fig:img-before}
    \end{subfigure}%
    \begin{subfigure}{.45\linewidth}
      \centering
      \includegraphics[width=.95\linewidth]{images/fetal_goal.PNG}
      \caption{Endpoint: stomach (green) and umbilical vein (blue) are present, heart is absent.}
      \label{fig:img-after}
    \end{subfigure}
    \caption{Two examples of the agent's observation at different positions of the probe}
    \label{fig:imgs}
\end{figure}

The simulator is based on a dataset of 3D volumes representing fetal abdominal ultrasound scans (though be easily adapted to other medical imaging scenarios as long as a dataset of volumes and relevant organ annotations is available).
Each scan is a 3d tensor $\mlinputvec \in \realnums^{n_1 \times n_2 \times n_3}$ along with three mask images $\mlinputvec_\text{heart}, \mlinputvec_\text{stomach}, \mlinputvec_\text{uv} \in \realnums^{n_1 \times n_2 \times n_3}$ over the same domain, indicating which parts of the scan are considered to be the heart, the stomach and the umbilical vein respectively.

\paragraph{Framework}

We adopt the industry-standard framework of {\em Episodic Partially Observable Markov Decision Process} \cite{kramerjdavidrPartiallyObservableMarkov1964, spaanPartiallyObservableMarkov2012}, implemented as a Gymnasium \cite{towersGymnasiumStandardInterface2024} where every simulator is a 8-tuple of non-terminal state space $\states$, action space $\actions$, observation space $\obss$, stochastic observation model $\obss, \prob_\obs(\obs | \state, \action)$, stochastic transition model $\prob_s(\state_\text{next} | \state_\text{prev}, \action)$, stochastic reward model $\prob_\reward(\reward | \state, \action)$, initial state distribution $\prob_\text{init}(\state)$ and episode termination model $\prob_\text{end}(\state)$.

\paragraph{State}

The state, in this case, is the current position of the probe defined by 2 3-vectors, the position of the probe $\state_\text{loc} \in \states_\text{loc}$ and its direction $\state_\text{dir} \in R^{3}$.

The simulator has 2 modes for the space of possible locations $\states_\text{loc}$: \emph{free} and \emph{realistic} movement.
In \emph{free movement} mode, $\states_\text{loc} = [0; n_1] \times [0, n_2] \times [0, n_3]$, thus the agent can place the probe anywhere within the bounds of the image, even inside the patient.
In \emph{realistic movement} mode, $\states_\text{loc} \subset [0; n_1] \times [0, n_2] \times [0, n_3]$ representing the surface of the patient's body available to the agent.

\paragraph{Actions}

The action space $\actions = \realnums^{7}$, where any $\action \in \actions$ can be decomposed as

\begin{equation}
    \action = (\action_1 , \action_2 , \action_3 , \action_\text{roll} , \action_\text{pitch} , \action_\text{yaw}, \action_\text{end})
\end{equation}

The first 3 components modify the $\state_\text{loc}$, the next 3 modify $\state_\text{dir}$ and, finally if $\action_\text{end} > 0$, the episode terminates and the current image is considered final.
Note that in \emph{realistic movement} mode, blind application of $(\action_1 , \action_2 , \action_3$ can lead to the probe being placed outside of the allowed domain of $\states_\text{loc}$.
In this case, a legal location will be chosen, in a manner that minimizes the distance between requested and real probe position:

\begin{equation}
    \state_\text{loc} \leftarrow \min_{\state \in \states_\text{loc}} \left\{ \lVert \state - (\state_\text{loc} + (\action_1 , \action_2 , \action_3)) \rVert \right\}
\end{equation}

\paragraph{Observations}

The observation is obtained by projecting $\mlinputvec$ onto a plane defined by the current position of the probe $\state_\text{loc}$ and its direction $\state_\text{dir}$, simulating the image that the sonographer would see on their screen.
The projection $\obs$ is evaluated on a coordinate grid the scale of which is a hyperparameter.

\paragraph{Rewards}

The quality of the image is defined in accordande with ISOUG guidelines \cite{isoug-guidelines} to be measured by the surface area of the heart, the stomach and the umbilical vein on the projection, normalized by the volume of these organs.

The reward then is the difference in quality between the current image and the one obtained at the previous iteration, so that probe movements that improve the image are rewarded positively and all rewards obtained during the episode sum up to the quality of the chosen image.

\newpage
\section{Related work}

Another simulator with a somewhat similar methodology is SonoRL \cite{sonorl}. The most important differences underlining the need for this simulator are:
\begin{itemize}
    \item SonoRL models ultrasonography of the spine (\emph{paramedian sagittal lamina view} \cite{spinal-guidelines}), while we focus on fetal screening.
    \item The reward function in SonoRL is a vector distance between the current position of the probe and the correct position. However, in reality, "correct" locations of the probe are numerous, comprising an equivalence class of solutions that satisfy the requirements in \cite{isoug-guidelines}.
    \item The implementation of SonoRL is not available to the wider community.
\end{itemize}