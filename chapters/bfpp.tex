\chapter{BF++}\label{ch:bfpp}

\begin{remark}
    An earlier revision of this chapter \cite{liventsevBFLanguageGeneralpurpose2022} was presented at ECML-PKDD 2023
\end{remark}

\section{Motivation}

As discussed in section \ref{sec:dsl}, domain specific languages like SQL make program synthesis tasks tractable by virtue of being simpler than industry-grade programming languages like Python or Java.
In the absense of a programming language both simple enough to sufficiently limit the optimization space and yet expressive enough to be general purpose, DSLs trade off the complexity of the optimization problem against the range of application domains a program synthesis system can tackle: SQL, for example, is only applied in the domain of relational databases \cite{atzeniRelationalDatabaseTheory1993}.
But what if such a language were to exist?

State of the art Reinforcement Learning from Code Execution Feedback work \cite{abolafiaNeuralProgramSynthesis2018} identifies BF\footnote{Brainfuck} \cite{brainfuck} as a promising program synthesis language for arbitrary reinforcement learning environments for the following reasons:
\begin{itemize}
    \item In industry programming languages a program $\code$ can contain a very large variety of characters since any of the 143859 Unicode \cite{allenUnicodeStandard2012} characters can be used in string literals. In BF, however, only 8 characters can be used: they can be one-hot-encoded with vectors of size 8. 
    \item BF's simple syntax means that a large percentage of possible strings of valid characters are valid programs. 
    This property is essential for RLCEF settings, since learning requires positive and negative examples, which, without a pre-existing training set of programs, can only be obtained by trial-and-error.
    Without it, the training process risks being stuck in a long exploration phase.
    \item Despite all of the above, it is a Turing-complete language.
\end{itemize}

BF falls short of the platonic ideal of a language for program synthesis, in which any randomly generated string would be a valid and meaningful program.
In particular, to be grammatical and meaningful in a POMDP environment, a BF program must:
\begin{enumerate}
    \item Contain equal number of opening and closing loop commands, \texttt{[} and \texttt{]}
    \item Always alternate between one read operation and one write operation
\end{enumerate}

The framework proposed in \cite{abolafiaNeuralProgramSynthesis2018} uses Reinforcement Learning techniques, however, due to the above limitations, as well as the lack of a general enough input output system in BF, it is only applied in non-interactive \emph{programming by example} settings such as string manipulation tasks.
In this chapter we propose an extension of BF with additional constructs that address these limitations and make BF++, the proposed language, suitable for Reinforcement Learning from Code Execution Feedback in an arbitrarily Partially Observable Markov Decision Process environment.

\newpage
\section{BF++}

We introduce an extended version of the original BF language, BF++. 
As explained below, the extensions to the original BF syntax are particularly useful in the Reinforcement Learning use cases. 

\subsection{BF syntax}
\label{sec:bf}

BF's runtime model is inspired by the classic Turing Machine \cite{turing}: at any point during the program's execution, the state of the program consists of:

\begin{itemize}
    \item An infinite\footnote{If one happens to be executing a BF program on a computer with finite memory, the tape will be finite due to hardware limitations.} tape of cells $\memory$ where each cell holds an integer number.
    \item A \textit{memory pointer} $\pointer_\memory$ that points to a certain cell in the tape (\textit{active cell} $\memory^{\pointer_\memory}$).
    \item A string of characters $\code$ that represents program code.
    \item A \textit{code pointer} $\pointer_\code$ pointing to a character about to be executed.
\end{itemize}

The code pointer starts at the first character, then this character gets executed and the pointer is incremented (moved to the next character).
There are 8 possible characters:

\begin{description}
\item[\texttt{>}] Move the memory pointer one cell right. $\pointer_\memory := \pointer_\memory + 1$
\item[\texttt{<}] Move the memory pointer one cell left. $\pointer_\memory := \pointer_\memory - 1$
\item[\texttt{+}] Increment the \textit{active cell}. $\memory^{\pointer_\memory} := \memory^{\pointer_\memory} + 1$
\item[\texttt{-}] Decrement the \textit{active cell}. $\memory^{\pointer_\memory} := \memory^{\pointer_\memory} - 1$
\item[\texttt{.}] Write $\memory^{\pointer_\memory}$ from the \textit{active cell} to the \textit{output stream}\footnote{The definition of input and output streams is purposefully underspecified, it may depend on the particular implementation.}
\item[\texttt{,}] Read $x$ from the \textit{input stream} to the \textit{active cell}. $\memory^{\pointer_\memory} := x$
\item[ \texttt{[} ] If the \textit{active cell} $\memory^{\pointer_\memory} = 0$, jump (move $\pointer_\code$) to the matching $]$.
\item[ \texttt{]} ] If the \textit{active cell} $\memory^{\pointer_\memory} \neq 0$, jump (move $\pointer_\code$) to the matching $[$
\end{description}

[ and ] commands constitute a loop that will be executed repeatedly until the \textit{active cell} becomes zero.
They are also the only way to write a BF program with a syntax error: a valid BF program is one that does not contain non-matching [ or ].

% TODO: example?

\subsection{Negative values}

In \textbf{BF} memory cells $\memory^\step$ hold non-negative values only.
In \textbf{BF++} $\memory^\step \in \integers$, a negation operator \texttt{\~} is introduced and operators \texttt{[]}are redefined to loop while the \textit{active cell} is non-positive, i.e.

\begin{description}
\item[ \texttt{\~} ] If the \textit{active cell} $\memory^{\pointer_\memory} := - \memory^{\pointer_\memory}$.
\item[ \texttt{[} ] If the \textit{active cell} $\memory^{\pointer_\memory} \geq 0$, jump (move $\pointer_\code$) to the matching $]$.
\item[ \texttt{]} ] If the \textit{active cell} $\memory^{\pointer_\memory} < 0$, jump (move $\pointer_\code$) to the matching $[$
\end{description}

This decision was taken because negative observations are common in control problems (see section \ref{sec:envs}) as is branching on whether the observed value is positive or negative. 

\subsection{Non-blocking action operators}
\label{sec:queue}

% TODO: cite some literature
% We're not the first people tackling these challenges, right?

The main issue of \textbf{BF} as a language for Reinforcement Learning is its input-output system.
It assumes that the program can freely decide on the relative frequency of inputs to outputs.
For example, the following program

\begin{center}
\begin{lstlisting}
+[.....,]
\end{lstlisting}
\end{center}

inputs 5 integers, outputs the 5th character it read, then goes back to the beginning and proceeds indefinitely outputting every 5th character it inputs.
Thus it assumes a 5:1 frequency of inputs to outputs.
If we simply assume that inputs are observations and outputs are actions, such program will not be able to operate in a POMDP environment where I/O frequency is fixed at 1:1 and the agent that has made an observation has to act before it can make the next observation.
In other words, operators \texttt{.} and \texttt{,} are blocking: \texttt{.} stops program execution and waits until new input is received to resume execution, \texttt{,} stops program execution and waits until there is an opportunity to act in the environment.

To address this, in \textbf{BF++} \texttt{.} operator is non-blocking.
It outputs the current value of the active cell by placing it at the bottom of the \textit{action queue} $\actionqueue$ - a sequence of integer numbers that represent actions the program is planning to take in the environment. We also introduce a non-blocking operator \texttt{!} that places $\memory^{\pointer_\memory}$ on top of the action queue.

\begin{equation}
    \begin{array}{cc}
         . & \actionqueue := \actionqueue^\frown (\memory^{\pointer_\memory}) \\
         ! & \actionqueue := (\memory^{\pointer_\memory})^\frown \actionqueue
    \end{array}
\end{equation}

where $\frown$ denotes concatenation of tuples

The program can thus decide by using \texttt{.} or \texttt{!} whether the newly added action takes precedence over ones already in the queue.
As soon as an opportunity to act arises, the top of the action queue (item $\actionqueue^1$ or several items $\actionqueue^1,\actionqueue^2,\dots$, see section \ref{sec:envs}) defines which action the program takes and is then removed from the queue. 
If $\actionqueue^\steppp$ does not exist (the queue is empty or shorter than $\steppp$) default value of $\actionqueue^\steppp=0$ is assumed.

\texttt{,} operator, on the other hand, is blocking. 
Thus its function is more important than just reading an observation into memory.
Executing \texttt{,} is when the program moves to the next step of POMDP.

\subsection{Virtual comma}
\label{sec:virtualcomma}

% TODO: fading text for virtual commas

The system where the only way to proceed to the following iteration is the \texttt{,} operator, naively implemented, means that to be successful in any POMDP environment, a program has to contain an infinite loop with a \texttt{,} operator.
Any program that has a finite number of \texttt{,} steps will terminate prematurely in an environment that supports arbitrarily long number of iterations.
Since the original goal is to develop a language where most random programs would be valid, this had to be addressed.

We decided to turn any \textbf{BF++} program into an infinite loop with a \texttt{,} operator by default:
\begin{enumerate}
    \item Every \textbf{BF++} program starts with a virtual \texttt{,} operator at address $\pointer_\code = -1$: it is executed before all operators in the code of the program, they are indexed starting from $\pointer_\code = 0$
    \item When the code pointer $\pointer_\code$ reaches the end of the program it loops back to the virtual comma $\pointer_\code := -1$
\end{enumerate}

Due to the virtual comma, every program starts executing with the initial observation already stored in memory and available for branching/decision-making.

\subsection{Observation discretization}
\label{sec:observe}

Another issue complicating applications of \textbf{BF} to Reinforcement Learning is that since its memory tape holds only integer numbers its inputs and outputs have to be integers as well.
And this issue cannot be fixed simply by replacing an integer tape with a tape of floating point numbers as \textbf{BF}'s only operations for manipulating numbers are \texttt{+} and \texttt{-} - increment and decrement.
Non-integer action and observation spaces are fairly common in reinforcement learning tasks hence \textbf{BF++} implements coercion mechanisms for reading and writing continuous vectors into discrete memory.

We assume that the vector observation space $\obss$ is an intersection of $n$ separate scalar observation spaces $\obss^\steppp$ such that 
\begin{equation}
 \obs_1 \in \obs_1^\steppp,\obs_2 \in \obs_2^\steppp,\dots,\obs_n \in \obs_n^\steppp \Leftrightarrow (\obs_1,\obs_2,\dots,\obs_n) \in \obss  
\end{equation}

This assumption theoretically excludes some possible observation spaces, but almost all POMDPs discussed in the research literature and all Gymnasium tasks conform to it.

To write an observation onto the memory tape the observation vector of size $n$ is aligned with memory cells $\memory^{\pointer_\memory},\memory^{\pointer_\memory+1},\dots,\memory^{\pointer_\memory+n-1}$ and discretized with the use of $\discretebins$ bins.

\begin{equation}
\label{eq:discretization}
\memory^{\pointer_\memory+\steppp-1} := \min_{\stepp \in 1,\dots \discretebins | o^\steppp < \threshold^\steppp_\stepp} \stepp
\end{equation}

If $\obss^\steppp$ is an interval $\obss^\steppp=[\obs_{low}, \obs_{high}]$, it is split into discretization bins evenly:

\begin{equation}
\label{eq:static-thresholds}
\threshold_\stepp = \begin{cases}
\obs_{low}+\frac{\obs_{high}-\obs_{low}}{\discretebins}\stepp, \stepp=1,2,\dots,\discretebins-1 \\
+\infty, \stepp=\discretebins 
\end{cases}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{observ.png}
    \caption{Fluid discretization example in Mountain Car}
    \label{fig:obs}
\end{figure}

Some environments, however, have unbounded observation spaces $\obss^\steppp=(-\infty;+\infty)$, $\obss^\steppp=(-\infty;\obs_{high}]$,  $\obss^\steppp=[\obs_{low};+\infty)$.
These spaces are challenging because the formal description $\obss^\steppp$ does not in any way reflect the actual underlying distributions of observations.
It can be the case, for example, that $\obss^\steppp=(-\infty;+\infty)$ but most observations found in the environment fall in the interval $\obss^\steppp=[42;43]$.
For such observation spaces, \textbf{BF++} uses a \textit{fluid discretization} system that learns the true distribution of observations online. The idea was inspired by a work of Touati {\sl et al} \cite{adaptivediscretization}, although, they assumed that $\obss^\steppp$ has a finite diameter and did not support unbounded observation spaces.
Initial thresholds $\threshold_\stepp$ can be arbitrary.
With each new observation, thresholds $\threshold_\stepp$ are readjusted so that among $\historylen$ prior observations, roughly $\stepp$ out of $\discretebins$  observations are lower values that $\threshold_\stepp$:

\begin{equation}
\underset{\threshold}{\text{minimize}} \sum_{\stepp \in 0,1,\dots \discretebins} |\frac{\stepp}{\discretebins} - \frac{\sum_{\step' \in \step-\historylen,\step-\historylen+1,\dots,\step-1} \indicator (\obs_{\step'}^\steppp < \threshold_\stepp)}{\historylen}|
\end{equation}

To solve this optimization problem, one has to sort previous $\discretebins$ observations in ascending order so that 

\begin{equation}
    \text{sort}: \{\obs_i | \step \in \step-\historylen,\step-\historylen+1,\dots,\step-1\} \longrightarrow \{ \obs'_i | \step \in 1,2,\dots,\historylen \}
\end{equation}

is such a bijection that $\obs'_1 < \obs'_2 < \dots < \obs'_h$ holds and set

\begin{equation}
    \threshold_{\stepp} = \obs'_{\lceil \frac{\stepp}{\discretebins} \historylen \rceil}
\end{equation}

See figure \ref{fig:obs} for a visual example.

%TODO: prove

This system has 2 hyperparameters: $\discretebins$ and $\historylen$.
With a low $\discretebins$ a lot of the information observed form the environment is lost, while when $\discretebins$ is in the hunderds the generated programs can become very complex.
$\historylen$ switches between relative and absolute observations.
With a very high $\historylen$, $\stepp=0$ means that this observation is one of the lowest that can be observed in this environment, with $\historylen=1$ it means that the observation is lower than the previous one.

High values of $\historylen$ present an additional challenge: how to correctly discretize observation in the first $\historylen$ iterations?
We implemented \textit{burn-in}: before training or evaluation we run $\historylen$ iterations of a random agent (see section \ref{sec:random}) to collect a history of $\historylen$ observations and pick correct thresholds.

\newpage
\subsection{Action coercion}
\label{sec:act}

A symmetrical problem arises with actions taken by the agent. 
Memory tape holds integer numbers $\memory^\steppp \in \integers$ and any value can be pushed onto the action stack.
However, the action that's output to the environment has to belong to a $N$-dimensional action space $\actions$, an intersection of unidimensional action spaces $\actions^\steppp$.
The "act" operation thus includes a coercion system and is defined as:

\begin{equation}
\label{eq:act}
\begin{array}{l}
    \action^\steppp := \begin{cases}
\frac{\actionqueue^\steppp}{\discretebins-1}, \actions^\steppp = (- \infty; + \infty) \\
\action_{\text{min}} + |\frac{\actionqueue^\steppp}{\discretebins-1} - \action_{\text{min}}|, \actions^\steppp = [\action_{\text{min}}; + \infty) \\
\action_{\text{max}} - |\action_{\text{max}} - \frac{\actionqueue^\steppp}{\discretebins-1}|, \actions^\steppp = (- \infty; \action_{\text{max}}] \\
\action_{\text{min}} + \frac{(\actionqueue^\steppp \bmod \discretebins)}{\discretebins - 1} * (\action_{\text{max}} - \action_{\text{min}}), \actions^\steppp = [\action_{\text{min}}; \action_{\text{max}}] \\
\actionqueue^\steppp, \actions^\steppp \subset \integers
\end{cases} \\
    \actionqueue := (\actionqueue^{N+1}, \actionqueue^{N+2}, \dots)
\end{array}
\end{equation}

\subsection{Goto}
\label{sec:goto}

It is notoriously hard to introduce any kind of branching behavior in \textbf{BF} \cite{linanderControlFlowBrainfuck2016}.
To facilitate if-then style programs we introduce a \textit{goto} operator \verb|^| defined as 

\begin{equation}
   \pointer_\memory := \memory^{\pointer_\memory} 
\end{equation}

Note that it is not a \texttt{goto;} in the traditional C sense, since the memory pointer is being moved, not the code pointer.
Still, it lets the agent preemptively store potential actions in memory cells and than branch between this actions based on the observation.

\subsection{Random number generator}
\label{sec:random}

Operator \texttt{@} writes a random number into the \textit{active cell}.
A random agent is often used as a starting point for exploration and in \textbf{BF++} a random agent can be implemented as \verb|@!|

\subsection{Shorthands}
\label{sec:shorthands}

With all the commands we introduced in sections \ref{sec:bf} - \ref{sec:goto} it is still surprisingly hard to encode relatively simple decisions like "add action 5 to the top of the action queue":

\begin{center}
\begin{lstlisting}
[>]+++++!
\end{lstlisting}
\end{center}

This program moves the memory pointer right until it hits a cell that contains zero, increments it five times, and then pushes $\memory^{\pointer_\memory}$ to the top of the action queue. It also loses the current value of the memory pointer which might be meaningful. Our experiments have shown that it takes a very long time for the neural model to learn to write this kind of combinations.

To mitigate this issue we introduce \textit{shorthands}: commands \texttt{01234} mean "write the respective number (0,1,2,3 or 4)" into the \textit{cell} and commands \texttt{abcde} mean "move the memory pointer to cell a,b,c,d or e" where cells a,b,c,d and e are the first 5 cells in the memory tape.
We intentionally made the number of \textit{shorthands} equal to discretization constant $\discretebins=5$.
Due to our method of discretization of continious action spaces (see sections \ref{sec:observe}, \ref{sec:act}) the program will often encounter situations when it can choose between $d$ different actions and thanks to shorthands taking them can be encoded as \texttt{1!}, \texttt{2!}, \dots

\subsection{Summary}
\label{sec:summary}

In total (assuming 5 shorthands) \textbf{BF++} has 22 commands:

\begin{center}
\begin{lstlisting}
><^@+~-[].,!01234abcde
\end{lstlisting}
\end{center}

Commands \verb|@^~01234abcde| are considered optional and can be disabled if the task at hand calls for it.
The number of shorthand commands can be increased or decreased.

% Achilles hill
Observation discretization and action coercion techniques built into the language mean that \textbf{BF++} is compatible with any POMDP environment. 
However, in practice, there is one important limitation: the complexity of the program required to operate in an environment is directly proportional to dimensionality of it's action and observation spaces $\actions$ and $\obss$. 
If, for example the observation space is 10000-dimensional, once an observation is read onto tape $T$ it takes 9999 \verb|>| operators to reach second to last observation.
Thus, in practice, \textbf{BF++} should be used with low-dimensional POMDPs.

An extension of our methodology to high-dimensional POMDPs (such as Atari games \cite{atari}, where the observation is a matrix of pixels on simulated game screen) can be achieved by adding a scene encoder neural network that maps the observed image to a low-dimensional vector as proposed in \cite{daqn}.

\newpage
\section{Experimental design}
\label{sec:bfpp-experiments}

\subsection{Hypotheses and goals}
\label{sec:exgoals}

Our experiments were designed to answer the following research questions:

\begin{description}
    \item[\rqbfpp] can \textbf{BF++} be used in conjunction with a program synthesis algorithm to solve arbitrary reinforcement learning challenges (POMDPs)?
    \item[\rqbfppexpert] can \textbf{BF++} be used to take a program written by an expert and use program synthesis to automatically improve it?
    \item[\rqbfppexplainable] can \textbf{BF++} be used to generate an interpretable solution to Reinforcement Learning Challenges that experts can learn from?
    \item[\rqbfppablation] do optional commands \verb|@^~01234abcde| improve the quality of programs synthesized by neural models?
\end{description}

Hence we

\begin{enumerate}
    \item Pick several commonly studied reinforcement learning environments
    \item Employ an expert\footnote{the author of the present thesis} to write \textbf{BF++} programs to solve them
    \item Develop a program synthesis model following from \cite{abolafiaNeuralProgramSynthesis2018}
    \item Compare the best programs generated by the model with expert programs in terms of program quality
    \item Perform ablation studies: remove some of the optional commands from the language (resulting language is called \textbf{BF+}), remove the expert program from the model's program pool, compare program quality
    \item Perform case studies: analyze programs generated by the model to gain insight into how the model approached the problem
\end{enumerate}

\subsection{Environments}
\label{sec:envs}
%TODO: add table of action and observation spaces

\begin{figure}
    \centering
    \subfloat[CartPole-v1]{
        \centering
        \includegraphics[height=1in]{Cartpole1.png}
    }
    \subfloat[MCC-v0]{
        \centering
        \includegraphics[height=1in]{MountainCar0.jpeg}
    }
    \subfloat[Taxi-v3]{
        \centering
        \includegraphics[height=1in]{Taxi3.png}
    }
    \subfloat[BW-v2]{
        \centering
        \includegraphics[height=1in]{BipedalWalker2.png}
    }
    \caption{Selected tasks}
    \label{fig:envs}
\end{figure}

We evaluate our framework on 4 low-dimensional (see section \ref{sec:summary}) POMDPs sampled from Gymnasium \cite{towersGymnasiumStandardInterface2024} leaderboard\footnote{\url{https://github.com/openai/gym/wiki/Leaderboard}}:

\begin{enumerate}
\item \textbf{CartPole-v1} \cite{cartpole}.
A pole is attached to a cart.  which moves along a frictionless track.
The agent observes cart position, cart velocity, pole angle and pole velocity at tip.
The goal is to keep the pole upright by applying force between -1 and 1 to the cart.
At every step the agent receives a +1 reward for survival.
The episode terminates when the pole inclines too far.
\item MountainCarContinuous-v0 (\textbf{MCC-v0})\cite{mountain_car}.
A car is on a one-dimensional track, positioned between two "mountains". 
The goal is to drive up the mountain consuming a minimal amount of fuel by controlling the engine, setting it's torque in the range $[-1;1]$; however, the engine is not strong enough to scale the mountain in a single pass.
Therefore, the only way to succeed is to drive back and forth to build up momentum. 
We picked MountainCarContinuous-v0 as opposed to MountainCar-v0 to demonstrate the performance of our discretization system.
\item \textbf{Taxi-v3} \cite{taxi}. There are 4 locations (labeled by different letters) and the goal is to pick up the passenger at one location and drop him off in another in as few timesteps as possible spending as little fuel as possible.
\item BipedalWalker-v2 (\textbf{BW-v2}). A simulated 2D robot with legs has to learn how to walk. 
Moving rightwards is rewarded, falling is penalized.
Observation vector consists of lateral speeds, angular speeds and spacial coordinates of each joint collected by the robot's sensors.
These observations do not, however, include any global coordinates - they can only be inferred from sensor inputs.
With action vector of size 4 the agent controls speeds of the robots hip and knee motors.
\end{enumerate}

\subsection{Hyperparameters}

For observation discretization (section \ref{sec:observe}) we picked $\discretebins=5$ (so that it's equal to the number of shorthands) and $\historylen=500$ for our experiments, hence when the observation is among the highest 20\% of the last 500 observations it is written into memory as 4 while if it falls between 40-th and 60-th percentiles it is 2.

\subsection{Expert programs}
\label{sec:expert-progs}

For \textbf{CartPole} we wrote 2 programs. 
One completely ignores all observations and just alternates between "move right" and "move left":

\begin{center}
\begin{lstlisting}
0!,1!
\end{lstlisting}
\end{center}

Another calculates the difference between velocity of the cart and angular velocity of the pole.
If it's positive, the cart is pushed to the right (the cart has to catch up with the pole), if it's negative the cart is pushed to the left, if zero it is pushed randomly:

\begin{center}
\begin{lstlisting}
[a0>0>0>0>0>@>1>1>1>1>1>,>[->>-<<]>>+++++^!1]
\end{lstlisting}
\end{center}

The first part of this program sets up an action map on the tape where every possible value of the velocity differential has a respective cell with 0, 1 or (in the center) random number.
Then \verb|[->>-<<]| block does subtraction, \verb|+++++| adds 5 to the result, so that it belongs to in $0..10$ and not $-5..5$, \verb|^| moves the memory pointer to the correct cell in the action map and \verb|!| puts the action onto the action stack.

For \textbf{Mountain Car} we wrote an elegant algorithm that reads the observation vector into the tape, goes to the second observation (car velocity) and outputs it as action:

\begin{center}
\begin{lstlisting}
>!a
\end{lstlisting}
\end{center}

In other words, we apply motor torque in the same direction where we're currently headed, thus always accelerating our car.
If we're headed right, that helps us get to the destination and if we're headed left that helps us get as high as possible onto the hill so that when direction reverses, the car has more energy to push through the right hill.

For \textbf{Taxi} we introduce 2 programs.
The first program:
\begin{enumerate}
    \item Finds the coordinates of the current destination (passenger to pick up or current passenger's destination)
    \item Subtracts the current destination 
    \item Moves in the resulting direction
\end{enumerate}

The problem with this approach is that it always gets stuck when it hits a wall.
To compensate for that, the second program alternates between the strategy above (for 5 iterations) and random movements (for 5 iterations) so that it eventually gets unstuck. See source code repository for the programs.

Optional commands \verb|@^~01234abcde| have all been invaluable in developing these programs - a fact in support of $H_4$.
A more rigorous way to confirm it would be employing several human experts to develop programs with and without optional operators, but finding volunteer \textbf{BF++} developers has proven difficult. 

Developing BF++ programs for \textbf{Bipedal Walker} manually is very challenging and left outside the scope.

\subsection{Program synthesis model}

In order to train a generative model $\policy$ to write \textbf{BF++} programs we treat the writing process as a reinforcement learning episode in its own right \cite{abolafiaNeuralProgramSynthesis2018} .
Every character of a program is an action taken by the \emph{writer agent}, the programs are terminated by a NULL character.
When the NULL character is written, a \emph{BF++ agent} is created in the target POMDP environment (e.g. CartPole) and sum total of rewards $\returntot$ collected in that episode is assigned as a reward to the \emph{writer agent} for the NULL character.
All other characters are rewarded with zero.

The \emph{writer agent's} policy is modeled with an LSTM \cite{hochreiterLongShorttermMemory1997} neural network and is trained with a modified version of REINFORCE \cite{williamsSimpleStatisticalGradientfollowing1992}algorithm.
While standard REINFORCE optimizes Policy Gradient:

\begin{equation}
    \obj_{\text{PG}}(\learnables) = \mathbb{E_{\policy(\code; \learnables)}}(\returntot)
\end{equation}

where $\learnables$ are LSTM parameters, $\code$ - program, $\returntot$ - reward obtained by the program in target environment,

we optimize

\begin{equation}
    \obj(\learnables)=\obj_{PG}(\learnables)+\obj_{PQT}(\learnables)
\end{equation}

where

\begin{equation}
    \obj_{\text{PQT}} = \frac{1}{N} \sum_{\steppp=0}^N \log \pi(\code_\steppp; \learnables)
\end{equation}

where $\code_1$ is the best (highest $\returntot$) known program, $\code_2$ - second best, \dots

Intuitively, both $\obj_{\text{PG}}(\learnables)$ and $\obj_{\text{PQT}}(\learnables)$ when optimized update the weights of the LSTM so that programs that we have found to be successful are more likely.
But Policy Gradient weighs programs proportionately to their respective rewards while PQT creates a \textit{priority queue} of the \textit{best known programs} and assigns a high importance to them and zero to the rest.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{cycle.png}
    \caption{Neural-symbolic learning cycle \cite{cycle}}
    \label{fig:cycle}
\end{figure}

$\obj_{\text{PQT}}$ component has been shown to have "a stabilizing affect and helps reduce catastrophic forgetting in the policy" \cite{abolafiaNeuralProgramSynthesis2018}.
In addition to this, we use $\obj_{\text{PQT}}$ to implement \textbf{expert inspiration}.
By default, the \textit{priority queue} of the \textit{best known programs} is initialized as an empty set.
But if expert-written programs are available, it can be prepopulated with these programs that act as useful positive examples for teaching the \emph{writer agent}.
This approach is used to incorporate programs from section \ref{sec:expert-progs} and transfer knowledge from experts to the neural developer.

This approach to \textbf{expert inspiration} follows what's known as neural-symbolic learning cycle, displayed in figure \ref{fig:cycle} - expert knowledge is represented symbolically, in terms of a \textbf{BF++} program, then a neural network is trained to generate this program, effectively translating the expert knowledge from symbolic into connectionist format (\emph{representation}), the neural network learns from reinforcement how to solve the task better than the expert (\emph{training}).
Unlike in most neural-symbolic systems \cite{neuralsymbolic} that extract knowledge from connectionist systems with algortihms like TREPAN \cite{trepan} or JRip extraction \cite{jripextr}, the \emph{extraction} step is trivial since the neural network outputs a symbolic program directly.

In all experiments below, the \emph{writer agent}'s LSTM has hidden size of 50, batch size of 4 and is trained with RMSProp \cite{tielemanLectureRmsPropDivide2012} optimizer.

\subsection{Stopping and Scoring}

All experiments were run with an upper limit of 100000 training episodes.
Environments other than \textbf{Taxi} also used Exponential Variance Elimination \cite{evestop} early stopping technique - training was stopped when the postive trend in the quality of the best found program stopped, i.e. when the exponential moving average of program quality is lower that it was 1000 episodes ago.
Agents for \textbf{Taxi} are trained for a fixed number of episodes, because we noticed that in this environment the longest part of the training process is learning to pick up the first passenger and until that happens $\returntot=-200$ holds.

Once the training process is finished, we take the best known programs and since each of them was only tested once (leading to high variance) we test them again, averaging total rewards over 100 episodes. 
We use this averaged reward to pick the best program.

\subsection{Implementation}

\begin{remark}
    A BF++ interpreter is available as part of Cibi \cite{liventsevVadim0x60Cibi2024} package
\end{remark}

\textbf{BF++} interpreter and the training system were written in Python with TensorFlow for neural models.
GPU resources were not used, because the performance bottleneck of the system is not backpropagation but rather testing a \textbf{BF++} program in the environment, single experiment runtime was between 1 hour (CartPole) and 10 (Taxi).

\newpage
\section{Results}

\subsection{Quantitative results}

\begin{table}[H]
  \caption{Total episode reward $\returntot$ achieved by best programs found, averaged over 100 episodes}
  \label{tab:quality}
  \centering
  \begin{tabular}{lcccc}
    Environment     & CartPole-v1     & MCC-v0 & Taxi-v3 & BW-v2 \\
    \midrule
    Random agent & 9.3 & 0 & -200 & -91.92  \\
    \midrule
    BF++ expert program 1 & 20.48 & -6.55 & -179.49 & - \\
    BF++ expert program 2 & 18.23 & - & -150.44 & - \\
    BF+ (without shorthands) LSTM & 44.55 & 91.57 & -57.93 & -91.9 \\
    BF+ (without \verb|@^~|) LSTM & 48.14 & 81.16 & -42.21 & -31.79 \\
    BF++ LSTM     & 71.38 & 88.41 & -199.82 & -26.97 \\
    BF++ LSTM with expert inspiration  & 96.64 & 91.39 & -60.65 & - \\
    \midrule
    Leaderboard threshold & 195 & 90 & 0 & 300 \\
    \bottomrule
  \end{tabular}
\end{table}

The table above presents the quality metric (average 100-episode reward) of the best program in every category, compared to that of a fully random agent and the result required to join the OpenAI gym leaderboard for context.
Note that the expert programs used a lot of optional operators (shorthands and \verb|@^!|), so it was not possible to implement expert inspiration with limited command sets.

These results support positive answers to (see section \ref{sec:exgoals}) \rqbfpp - we have obtained functional programs for all environments, and \rqbfppexpert - when expert inspiration was used the resulting programs were better than expert programs and better than programs generated without expert inspiration and \rqbfppablation - ablation studies for optional operators do indeed show that those operators are useful.

\subsection{Case studies}
\label{sec:casestudies}
% Need to make sure that this paragraph and "expert programs" paragraph help each other (same style, same terms)

We have established that the program synthesis model is able to learn from human experts.
But can experts learn from the model? (\rqbfppexplainable) 
To confirm this, we offer a detailed explanation of the most successful program of all experiments listed in section \ref{sec:bfpp-experiments}.

This program scored \emph{91.39} on \textbf{Mountain Car}:

\begin{center}
\begin{lstlisting}
-..~+
\end{lstlisting}
\end{center}

The trailing \verb|~| and \verb|+| do not affect the behavior of the agent: they modify the value of the active cell only for it to be immediately rewritten by the virtual comma (section \ref{sec:virtualcomma}) before it has any chance to influence actions.
One can think about these commands as inactive genes in the DNA - we have found many resulting programs to contain such commands.
If necessary this effect can be accounted for by incorporating program length into the loss function.
So this program is equivalent to:

\begin{center}
\begin{lstlisting}
-..
\end{lstlisting}
\end{center}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{MountainCarWinner.png}
    \caption{Visual summary of the strategy enacted by \texttt{-..} on \textbf{Mountain Car}}
    \label{fig:mountaincarwinner}
\end{figure}

When the virtual comma is executed, car position and car velocity are read into memory, discretized into integers $0\dots4$.
The position is read into the active memory cell $\pointer_\memory$, while the velocity is in cell $\pointer_\memory+1$.
Then the active cell is decremented and the resulting number is put onto the action stack twice.
There is 1 read operation and 2 write operations to the end of the action stack, which introduces a delay before the actions get executed.
When it's time to act, the number on the action stack is coerced to one of the actions possible in this environment (0 for going left, 1 for doing nothing, 2 for right). 

A strategy emerges, illustrated in figure \ref{fig:mountaincarwinner}, in which the car puts "going right" onto the agenda if it's on the far left or the center right of the landscape, puts "going left" onto the agenda when it's on the far right or center left and schedules doing nothing if it's in the center.
This strategy helps the car successfully reach the right fringe every time it is applied.

\newpage
\section{Conclusions}

In this chapter, we have introduced a new programming language tailored to the task of Reinforcement Learning from Code Execution Feedback.
We have shown experimentally that this language can facilitate program synthesis as well as knowledge transfer between expert-based systems and data-driven systems. 

The results in the OpenAI gym test examples show that the proposed system is able to find a functional solution to the problem. In some cases the performance is similar to the best deep learning solution but the obtained program remains still explainable. This is a very encouraging result and suggest that the use of program induction methods may indeed be a viable way towards explainable solutions in RL applications. 

We propose the following directions for future work:
\begin{enumerate}
    \item Develop translation mechanisms between \textbf{BF++} and other languages. Potentially, \textbf{BF++} can be used as \emph{bytecode} \cite{bytecode} for reinforcement learning. The expert would write a program in a higher-level language and transpile it into \textbf{BF++} so that the program then can be improved with reinforcement learning.
    \item Use other neural network architectures as well as non-neural evolution methods like genetic programming \cite{genprog1,genprog2} in conjunction with \textbf{BF++}
    \item Apply the framework to problems in Healthcare where expert inspiration is important for crossing the AI chasm \cite{aichasm}.    \item Use Natural Language Generation techniques to translate the BF++ code automatically to a friendly human-readable text description as in \cite{richardsonCode2TextChallengeText2017,code2nlg2}.
\end{enumerate}