\chapter{Anthropodidactic learning: Auto-ALS}
\label{ch:auto-als}

\section{Anthropodidactic learning: a modest proposal}
\label{sec:anthropodidactic}

\emph{Anthropodidactic machine learning} is using didactic materials developed for human students (textbooks, lectures and/or lecture notes, explanations, homeworks, exercises, games and other sorts of interactive edutainment) to train artificial intelligence.
Examples of anthropodidactic learning include using language textbooks to train a machine translation model or using a flight simulator developed for pilot training to train an autopilot with reinforcement learning \cite{staudingerXPlaneMLEnvironmentLearning2018}.

\paragraph{Motivation}

The education industry puts a lot of effort into curating and systematizing knowledge in ways that can be reasonably expected to be useful for a learner of any biological substrate
For example: 
\begin{itemize}
    \item Exercise sets in mathematics, physics and language learning, to name a few fields, are explicitly designed to cover all important clusters/corner cases of the subject area, something that is not guaranteed in most datasets like logs, business records or text corpora.
    \item Exercise sets tend to be sorted by difficulty. This creates a useful curriculum to follow when training a machine learning model.
    \item Educational software aims to give users immediate and precise feedback on their mistakes: delayed gratification (also known as \emph{temporal credit assignment problem}), as it turns out, is hard for people \cite{tobinDelayGratificationReview2010} and reinforcement learning algorithms alike.
\end{itemize}

See \cite{brownArtProblemPosing2005} for more on the art of problem posing.

\paragraph{Related Work}

Machine learning community is undoubtedly interested in taking lessons from human learning, efforts to do so bear the umbrella term of \emph{antropomorphic
machine learning} \cite{angelovAnthropomorphicMachineLearning2018}. The prime example is curiculum learning \cite{sovianyCurriculumLearningSurvey2022, zhouCurBenchCurriculumLearning2024}: it was born with the observation that the order in which data is presented to human students is crucial for them achieving their learning goals, so it is likely a difference for machines as well.

However, examples of directly reusing learning aids developed for human students are hard to come by. A notable exception is Reinforcement Learning where decision-making agents are often trained on games initially intended for people. And while the claim that \emph{Atari games} \cite{mnihPlayingAtariDeep2013} and 
\emph{Minecraft} \cite{hofmannMinecraftAIPlayground2019} are educational material may be somewhat stretching the definition of education, interactive simulators first developed for people and later adapted for reinforcement learning include \emph{X-plane} \cite{staudingerXPlaneMLEnvironmentLearning2018} (used for training pilots) and Virtu-ALS (used for training nurses), to be discussed in section \ref{sec:virtu-als}. 
Some antropodidactic work has also been done in natural language processing, training language models on children's books \cite{mayhewSimultaneousTranslationParaphrase2020} and exercises for language learning from \emph{Duolingo} \cite{mayhewSimultaneousTranslationParaphrase2020}
More recently, curated datasets with a focus on educational materials like FineWeb-EDU \cite{penedoFineWebDatasetsDecanting2024} have been used to improve the quality of large language model pretraining.

\newpage
\section{Virtu-ALS}
\label{sec:virtu-als}

\begin{remark}
  An earlier revision of this section \cite[section 3.1]{liventsevEffectivePatientSimulators2021} appeared in Frontiers in Artificial Intelligence. Auto-ALS \cite{liventsevVadim0x60Autoals2024} was unveiled at HEALTHINF 2023.
\end{remark}

In this chapter, we apply the idea of anthropodidactic learning to patient simulation in order to facilitate the transfer of healthcare knowledge into AI systems.
We develop a reinforcement learning environment based on a learning aid for junior healthcare professionals.
This is particularly important for validating RLCEPS (chapter \ref{ch:proposal}), since the correct protocol to follow is known and the simulator has been developed specifically for training this protocol, hence, the program synthesized by RLCEPS can be evaluated in terms of its similarity to the correct protocol.
The downside of \emph{didactic} simulators like Auto-ALS is that their inherent \emph{confirmation bias} (any decision that's not prescribed by the standard emergency care protocol is considered a mistake and rewarded negatively) makes them a bad tool for discovering novel improved protocols.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Virtu-ALS.png}
    \caption{Virtu-ALS}
    \label{fig:virtu-als}
\end{figure}

Virtu-ALS is a \emph{didactic} emergency care simulator developed to teach students and junior healthcare professionals ABCDE assessment protocol \cite{thimInitialAssessmentTreatment2012}, although its application as a reinforcement learning \emph{benchmark} was anticipated and accounted for by the authors \cite{briskAIEnhanceInteractive2018}.
Its most prominent feature is its graphical nature (figure \ref{fig:virtu-als}): the user has access to a 3D-rendered virtual copy of a hospital room, view the monitor, press buttons on a defibrillator, etc.
However, the visual modality means that its observation space 
\begin{equation}
    \obss = \realnums^{307200}
\end{equation}

Such a high dimensionality of the observation space makes it an extremely challenging reinforcement learning task.
Tasks from this family have been solved with deep neural networks \cite{mnihPlayingAtariDeep2013}, however not only does it require a long and expensive training process, it also means that resulting treatment strategies are black box neural networks that no clinical expert understands.
This approach to decision-making is extremely hard to introduce into clinical practice \cite{priceBigDataBlackbox2018,watsonClinicalApplicationsMachine2019}

This chapter proposes \emph{Auto-ALS}, a low-dimensional modification of \emph{Virtu-ALS}.

\newpage
\section{Decision model}
\label{sec:mpdp}

Implementing Auto-ALS requires grappling with some of the limitations of POMDP framework (as described in section \ref{sec:rlcef}), namely:
\begin{enumerate}
    \item Some decision making scenarios legitimately have a discrete flow of time: a CPU, for example, makes a decision every $\frac{1}{\text{clock frequency, Hz}}$ of a second. However, most real-life decision making settings (such as the hospital setting) happen in a continuous time flow with no limits on how often or how rare actions and observations occur.
    \item The agent cannot respond to an observation with less than one (i.e.~zero) or more than one action. This can be fixed by modeling $\action_n)$ as a set of actions.
\end{enumerate}

Both of these problems have been successfully addressed with artificial time discretization and complex action spaces $\actions$, however this is done on a case by case basis for each particular environment.
We would like to break from that trend and propose a generalist model that can address these challenges for Auto-ALS as well as other environments in the future.

\paragraph{Message Passing Decision Process}

Let us attach a timestamp $\timepoint$ to every observation, reward and action in the decision process, making them 2-tuples. An action $\langle a, \timepoint\rangle$ is a message from the agent to the environment, an observation $\langle \obs, \timepoint \rangle$ or a reward $\langle r, \timepoint \rangle$ is a message from the environment
to the agent. Messages can be sent as often or as rare as needed:

Observations and actions are sampled from the \emph{environment} and conditioned on the timestamp and all actions of the agent before that time:

\begin{equation}
    \obs_\timepoint \sim p(\obs_\timepoint|\timepoint,\{\action, \timepoint_\action | \timepoint_\action < \timepoint\})
\end{equation}

The actions are sampled from the \emph{agent}, also conditioned on the
timestamp and all observations before that time:

\begin{equation}
a_\timepoint \sim \policy(a_\timepoint|\timepoint, \{ o, \timepoint_o | \timepoint_o < \timepoint \})
\end{equation}

where $\obs_\timepoint \in \obss^{+} = \{\text{no observation} \} \cup \obss$ and $\action_\timepoint \in \actions^{+} = \{\text{noaction} \} \cup \actions$

\paragraph{Discretization}

Apart from liquid computers \cite{adamatzkyBriefHistoryLiquid2019}, computational hardware normally operates on a discrete time clock frequency \cite{publishingCPU2023}, thus, for most real world deployments continuous MPDP will have to be discretized, namely, in any timeframe $\langle \timepoint, \timepoint + \Delta \timepoint \rangle$ a finite number of decisions (evaluations of $\policy(a_\timepoint)$) should be made. This can be achieved by a decision schedule that is some combination of:

\begin{itemize}
\item   making a decision every time any message from the environment   (observation or reward) is received 
\item   making a decision at regular intervals $\Delta \timepoint$ 
\item   after making a decision, scheduling a decision time $\timepoint_d$ later in the future
\end{itemize}

At every decision point the agent has to receive information about the observations that recently occurred and output some number of actions (may be zero)

\paragraph{Observation space}

When an observation is modeled as $\langle \obs, \timepoint \rangle$, the most faithful way to represent the history of observations to the agent is

\begin{equation} 
\obs_\text{history} = \langle \obs_1, \timepoint_1, \obs_2, \timepoint_2, \dots \rangle 
\end{equation}

This representation has 2 issues: 
\begin{enumerate}
    \item It has a variable size. There are, of course, machine learning algorithms that can work with variable size inputs \cite{hochreiterLongShorttermMemory1997}, however, most traditional RL approaches cannot and compatibility with them would be an advantage
    \item There is a common type of observation that makes older observations obsolete. For example, in a thermostat a new temperature measurement for all intents and purposes overrides the old one. In a navigation task, a new "location observed" event means you are no longer in the previous location. In Auto-ALS, new measurement of a vital sign, such as heart rate, overrides earlier ones. This has to be taken into account, lest a vast array of outdated information will be fed to the agent at every decision point.
\end{enumerate}

A solution (probably \emph{the} solution?) to these is to sort observations into \emph{observation classes} $\obs_1 \cup \obs_2 \cup \dots \cup \obs_n = O$ such that if several observations from the same class has been made, only the last of them is important. Then $\obs$ should be a vector of the latest observation in each class

\begin{equation} 
  \obs_\text{history} = \langle \obs_1 \in \obs_1, \exp(\timepoint_1-\timepoint), \dots, \obs_n \in \obs_n, \exp(\timepoint_n-\timepoint), \rangle
\end{equation}

where $\timepoint$ is the decision time. 
$\exp(\timepoint_n-\timepoint)$ is preferable to the more naive approach of $\timepoint-\timepoint_n$, because if no observation in the observation class $\obs_n$ has occurred yet, $\timepoint=-\infty$ which creates all kinds of problems for actually solving the MDP downstream. 
$\exp(\timepoint_n-\timepoint)$ in this case would be simply zero and observations that have occurred will have an exponentially decaying relevance factor attached to them - a more directly useful value for decision-making then ``time since event''.

\paragraph{Action space}

Using one of the decision schedules and the observation system described above, it is fairly trivial to support a variable number of actions. 
The agent has to output an action $\action \in \actions^{+}$ and if that action is not a "no action", action sampling repeats again.

\newpage
\section{Auto-ALS}

\paragraph{Events}

\begin{table}
\input{tables/auto-als-events.tex}
\caption{All action and observation events of Auto-ALS}
\label{tab:auto-als}
\end{table}

\emph{Auto-ALS} is a Message Passing Decision Process environment as described in section \ref{sec:mpdp}, developed by identifying all meaningful events that occur in \emph{Virtu-ALS} and registering them as actions and observations in lieu of screen images and mouse clicks.
This effort resulted in table \ref{tab:auto-als}, the first column of which describes the action space $\actions^{+}$.
For each of the actions, the second column lists possible observations the agent could receive in response to it.

\emph{Tick} is a special event that occurs every time the simulator is advanced a timestep, and is negatively reinforced, which when used with reinforcement learning algorithms discourages clinically unnecessary actions.

Finally, the third column indicates the reward $\reward$ the agent will receive after completing the action.
$\reward_\text{blunder}$ and $\reward_\text{tick}$ are hyperparameters that control the priorities of acting in a way that stabilizes the patient versus acting fast.
Note that acting fast is not merely a matter of efficiency: in emergency care settings it's very much clinically relevant.
We recommend using a reward schedule where $\reward_\text{tick}$ is initially set to zero and once the agent has learned to reliably stabilize the patient, $\reward_\text{tick}$ is introduced to \emph{prune} the policy, i.e. remove all unnecessary steps that cost valuable time.
Note that in experiments in section \ref{sec:auto-als-exp} $\reward_\text{tick}=0$ always, since none of the methods we study achieve robust enough performance to proceed to the pruning stage.

Most of the observations are binary events.
The exceptions to this rule are \emph{measurements} \texttt{MeasuredHeartRate, MeasuredRespRate, MeasuredCapillaryGlucose, MeasuredTemperature, MeasuredMAP, MeasuredSats, MeasuredResps}, each of which has a value $(-\infty, +\infty)$ associated with it.
For \emph{measurements}, the $\obs_i$ equals the magnitude of the measurement, however, for binary observations $\obs_i$ would always be equal to one.
For memory efficiency, for all $i$ that correspond to binary observations, $\obs_i$ is skipped from the $o^{+} $ vector and the actual observation vector $o$ has size $36+7*2=50$, as opposed to $36+7)*2=86$

It's important to note that observations only get registered if the player agent has \emph{learnt} some piece of information, meaning that, for example, \verb|AirwayVomit| will only occur if the patient has vomit in their airway \emph{and} the player agent has checked the airway (which is part of the standard protocol \cite{thimInitialAssessmentTreatment2012}).
Assessment skills (knowing where to look and how to establish the patient's state) are crucial for patient resuscitation, so revealing all known health variables to the agent automatically would jeopardize the simulation.

\paragraph{Implementation}

Auto-ALS~\cite{liventsevVadim0x60Autoals2024} is impemented as an OpenAI Gym~\cite{openai-gym} environment using Unity ML Agents~\cite{almon-manzanoDeepReinforcementLearning2022, julianiUnityGeneralPlatform2020, lanhamLearnUnityMLAgentsfundamentals2018, nandyUnityMLAgents2018} framework via forking Virtu-ALS~\cite{liventsevVadim0x60Virtualsplus2024} and editing it to attach an event listener and an action executor.
Text messages displayed to the user are included in the \texttt{info} metadata dictionary returned by the environment.

\newpage
\section{Experimental Design}
\label{sec:auto-als-exp}

\subsection{Reinforcement Learning baseline}

Auto-ALS is a more expensive environment than HeartPole when it comes to computation per episode.
For this reason, in lieu of 3 reinforcement learning algorithms with diverse characteristics that we used for HeartPole, for Auto-ALS we use a single algorithm known for its versatility~\cite{alaghaTargetLocalizationUsing2022, bilbanOptimizingAutonomousVehicle2025, bohnDeepReinforcementLearning2019, coreccoProximalPolicyOptimizationbased2023, funikaAutomatedCloudResources2023, hasankhaniIntegratedPathPlanning2023, huangImprovingTrafficSignal2023, kalusivalingamOptimizingIndustrialSystems2020, kleinOptimizingParametersSwarm2024, liIntelligentDecisionJoint2025, liuApplicationAdvancedTree2022, lopesIntelligentControlQuadrotor2018, luAdaptiveEnergyManagement2024, maMultiuserMobileEdge2024, maTaskOffloadingScheme2025, mayerModularProductionControl2021, meloLearningHumanoidRobot2019, mengProximalPolicyOptimization2023, ouyangTokenlevelProximalPolicy2024, pinciroliDeepReinforcementLearning2021, proctorProximalPolicyOptimization2021, quangtranProximalPolicyOptimization2020, rehmanReinforcementLearningdrivenProximal2024, silvaControlConventionalContinuous2024, silvaTemporalGraphTraversals2020, vanvuchelenUseProximalPolicy2020, weiMixedautonomyTrafficControl2019, wuCombustionOptimizationStudy2024, wuProximalPolicyOptimization2023, yeAutomatedLaneChange2020, yinHyperheuristicAlgorithmProximal2024, zhangProximalPolicyOptimization2024, zhangRealtimeAutonomousLine2020, zhaoPPOTAAdaptiveTask2023}, namely, PPO \cite{schulmanProximalPolicyOptimization2017}. 

\subsection{Program Synthesis}

We use the better of the 2 hyperparameter setups explored in chapter \ref{ch:heartpole}, see table \ref{tab:rlceps-auto-als}.

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        model & $\treearity_\text{draft}$ & $\treearity_\text{explain}$ & $\treearity_\text{debug}$ & $\beamwidth$ & selection \\
        \midrule
        gpt-4o & 3 & 2 & 2 & 5 & tournament
    \end{tabular}
    \caption{Hyperparameter choice for evaluating RLCEPS on Auto-ALS}
    \label{tab:rlceps-auto-als}
\end{table}

Due to the inherent stochasticity of the environment (see chapter \ref{ch:auto-als}) we use 2 tests, which are 2 identical copies of the Auto-ALS environment.
Since lexicase selection assumes multiple deterministic tests, tournament selection is used instead.
We add hard limits on the length of the episode in terms of time (3 hours) as well as steps (350 environment steps).
Text messages displayed to the user are fed into the INSTRUCT agent (section \ref{sec:seidr-ingredients}) as \texttt{stderr}.
This means the agent has no access to them at runtime, but the synthesis algorithm uses them to evolve the agent.
The initial prompt is as follows:

\lstinputlisting{listings/metanurse_prompt.txt}

\newpage
\section{Results}

\lstinputlisting{listings/auto-als.py}

The final program generated by SEIDR (version 37, above) achieves a score of \textbf{0}, meaning it never blunders, but in the end it does not stabilize the patient.
Looking at the program gives us some insight as to why this could be the case: it correctly handles various eventualities such as starting chest compressions in case mean arterial pressure drops too low, but it fails to follow the general ABCDE assessment protocol, despite the fact that a full description of it was included in the prompt. Compare version 0:

\lstinputlisting{listings/auto-als-0.py}

Version 0 (score \textbf{-63}), generated by \texttt{gpt-4o} with no feedback from \emph{Auto-ALS}, based on prompt only, follows ABCDE assessment procedure much more closely.
This is a counterintuitive result: a learning aid designed specifically to promote the ABCDE protocol, encourages SEIDR to follow it less.
This can be seen as \emph{mode collapse}~\cite{kossaleModeCollapseGenerative2022, zhangModeCollapseGenerative2021} or \emph{reward hacking} (discussed in section \ref{sec:alignment}): the agent learns to avoid blunders by avoiding important parts of its work entirely.

\begin{figure}
    \begin{subfigure}{0.48\linewidth}
      \includegraphics[width=\linewidth]{images/auto-als-ppo-len.png}
      \caption{length of episodes}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
      \includegraphics[width=\linewidth]{images/auto-als-ppo-rew.png}
      \caption{total episode reward}
    \end{subfigure}
    \caption{Evolution of PPO agent's performance over the course of the training run}
    \label{fig:ppo-auto-als}
\end{figure}

A neural engine trained with PPO (figure \ref{fig:ppo-auto-als}) does blunder frequently.
In fact, the best score it achieves over the course of training is \textbf{-81}, worse than version 0 in SEIDR.

In the end,

\begin{highlight}
the synthesized program commits less blunders than the reinforcement learning baseline, but neither solution is able to reliably stabilize the patient
\end{highlight}

Given that both SEIDR and PPO are extensively validated in other environments this speaks to the difficulty of \emph{Auto-ALS} as a benchmark.
As such, the primary value of \emph{Auto-ALS} is a verification mechanism for (programmatically interpretable or otherwise) reinforcement learning methods in Healthcare.
An approach that scores well on \emph{Auto-ALS} can be considered a strong candidate for training in a more data driven patient simulator and potential implementation in clinical settings.