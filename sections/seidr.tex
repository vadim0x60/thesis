\chapter{Multiagent Programming with LLMs}
\label{ch:seidr}

\todo{Ensure standard nomenclature}

\section{Introduction}
\label{sec:seidr-intro}

Two recent innovations potentially make the latter task tractable.

One is \emph{Synthesize, Execute, Debug}~\cite{guptaSynthesizeExecuteDebug2020}, a framework that attempts to bridge the ``last mile'' gap by introducing program repair into the program synthesis algorithm. 
A programming task is specified using both a natural language description and a set of input/output (I/O) pairs that demonstrate what output is expected from the program, thereby combining text-to-code~\cite{iyer2018:mapping} and programming by example~\cite{halbertProgrammingExample1984,gulwani2016:programming} paradigms typical for competitive programming~\cite{zavershynskyi2018:naps}.
\emph{Synthesize, Execute, Debug} creates a first draft program using a generative model, compiles and executes it with given input examples.
This is followed by a program repair step to fix the identified errors.

Another relevant innovation is instruction fine-tuned large language models~\cite{zhang2024:instruction}. Instruction fine-tuned models use human feedback in their training process and are designed to explicitly or implicitly admit two inputs: a source text (or code) and a textual command instructing the model to edit the source in a particular way, e.g., ``summarize'' or ``translate to Python.''
These models have been shown to be highly successful in automatic program repair~\cite{fanAutomatedRepairPrograms2023}. 
However, given the free-form nature of these instructions, how one should engineer instructions that maximize repair performance is an open question. 

These innovations have led to the proposal of a framework, \emph{Synthesize, Execute, Instruct, Debug and Rank}, or SEIDR,\footnote{~Seiðr also refers to a type of Norse magic~\cite{blain2002:nine} pertaining to predicting and controlling the future, which we deem thematically appropriate.} initially introduced in our GECCO-2023 paper~\cite{liventsevFullyAutonomousProgramming2023}. 
%The current paper partly reiterates the methodology and findings of~\cite{liventsev2023:fully} and expands on them in this extended version of the original paper.
SEIDR is a multi-agent iterative framework that uses feedback from code execution and failing test cases to update the initially generated buggy code. 
Our initial explorations cover SEIDR with GPT-3 as the bug summarization model and Codex (or GPT-3 trained on code) as the program generation and debugging model.  
% We test a number of prompting strategies for LLMs to fix a working prompt.
In addition, by modifying the tree arity parameter (see Section~\ref{sec:seidr-beam-search}), we investigate the trade-off between generating and repairing only one program versus regenerating any program that does not pass all the tests, as well as intermediate configurations, where we build a tree of programs and update the best ones.

While Codex is an early code generation model, the emergence of new models that score better in programming and natural languages motivates further research into the use of SEIDR with newer models. 
To study the generalizability of the initial SEIDR results, we use two other LLMs, Llama 3\footnote{~\url{https://ai.meta.com/blog/meta-llama-3/}} and GPT-3.5,\footnote{~\url{https://openai.com/form/researcher-access-program/}} and an additional dataset, HumanEval-X, with different tree arity parameters~\cite{brown2020:language,chenEvaluatingLargeLanguage2021,zheng2023:codegeex}. 
Moreover, we build up on the initial experiments with Codex and zoom in on the area with the best-performing tree arities in a hyperparameter search for a better repair-replace trade-off resolution. 

To reflect on the parent selection strategies used in the Rank agent of SEIDR, we also explore whether the programs should be chosen based on the average performance across all tests or whether SEIDR can benefit from keeping such programs in the loop that fully cover individual tests, but do not perform well on average.
Therefore, as an alternative to the tournament selection, we test the best tree arity setups with lexicase selection-based ranking~\cite{helmuth2015:solving}.
Moreover, since language models bring in stochasticity, we run the experiments several times to measure the variability of results obtained with fixed hyperparameters and reflect on the results repeatability.

Overall, the current paper contributes to the field of Large Language Models for Software Engineering (LLM4SE) with a framework for code generation and its iterative repair. 
Compared to the earlier GECCO-2023 paper, this article refines and extends the preliminary results by providing a more in-depth explanation of SEIDR as a multi-agent framework for autonomous programming, extending the original experiments and analysis to include HumanEval-X as an additional benchmark, evaluating GPT-3.5 and Llama 3 as additional LLMs, investigating lexicase selection as an alternative to tournament selection, and conducting a repeatability analysis of the framework’s performance over multiple independent runs. Overall, this extension adds three research questions and doubles the number of configurations considered in these questions compared to our preliminary work. 

Section~\ref{sec:seidr-methodology} presents a framework that adapts \emph{Synthesize, Execute, Debug} to instruction-tuned Large Language Models in agents that can solve programming tasks in an autonomous fashion. 
We discuss related work in Section~\ref{sec:seidr-related-work}, introduce experiments to explore effective search 
% and prompting 
strategies for this framework in Section~\ref{sec:seidr-eval}. 
Finally, in Section~\ref{sec:seidr-results}, we demonstrate that our framework outperforms conventional automatic programming techniques, such as genetic programming and naive application of large language models that generate one solution per problem without updating it iteratively. 

\section{Methodology}
\label{sec:seidr-methodology}
The proposed five-agent SEIDR framework is summarized in Figure~\ref{fig:method}, which we discuss in detail in Section~\ref{sec:seidr-ingredients}.
In essence, to solve a programming task defined as a text description and a collection of I/O examples, we split I/O examples into prompt and validation sets and use the prompt set in a large language model to SYNTHESIZE a population of candidate solutions.
We EXECUTE the solutions, test them against the validation set, generate a text description of the identified problems used to INSTRUCT a large language model to produce repaired candidate solutions similar to the way a human developer DEBUGs a program.
We RANK the candidates
by correctness, measured as matching I/O pairs, discard the worst candidates, and repeat until a fully correct solution is found.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth,trim={0mm 0mm 0mm 0mm}]{images/codex-for-psb-seidr-methodology-5.drawio.pdf}
    \caption{Overview of SEIDR, a multi-agent iterative framework that uses LLMs to implement the Synthesize, Execute, Instruct, Debug, and Rank feedback loop.}
    \label{fig:method}
\end{figure}
 

\subsection{Ingredients}
\label{sec:seidr-ingredients}

SEIDR makes use of instruction fine-tuned large language models: a \emph{synthesis} model $p_{\text{synth}}(\text{code, }$ descr), a \emph{debugging} model $ \langmodel_\text{debug} $, as well as a model $ \langmodel_\text{text} $ that can be used for writing textual instructions, which are forwarded to the code generation model $ \langmodel_\text{debug} $ for code updates. 
Therefore, the design can be described as two agents communicating with each other, whereby one generates code and another provides critical or supervising comments on what should be changed in the generated code. 

The models $ \langmodel_\text{synth} $, $ \langmodel_\text{debug} $, and $ \langmodel_\text{text} $ can be either separate or the same model.
The prerequisites are that $ \langmodel_\text{synth} $ and $ \langmodel_\text{debug} $ models are able to ``understand'' natural language (descr) and partial or full programs (code) and generate code based on them. 
The model $ \langmodel_\text{text} $ should be able to ``understand'' code and natural language and either autocomplete or generate the debugging instruction from scratch. 
Note that $ \langmodel_\text{text} $ is optional, since alternatively the debugging instructions can be generated from failing tests using static pre-defined templates.
%as described in~\cite{liventsev2023:fully}. 
In general, SEIDR requires sequence-to-sequence generative models for these agents. 
%In our experiments, we have chosen models $ \langmodel_\text{synth} $, $ \langmodel_\text{debug} $, and $ \langmodel_\text{text} $ that are based on the state-of-the-art transformer architecture~\cite{vaswani2017:attention} as a backbone and its subsequent improvements (see Section~\ref{sec:seidr-models}). 
In our experiments, we select models for $ \langmodel_\text{synth} $, $ \langmodel_\text{debug} $, and $ \langmodel_\text{text} $ based on the state-of-the-art transformer architectures~\cite{vaswaniAttentionAllYou2023} 
%and its subsequent improvements 
(see Section~\ref{sec:seidr-models}). 

Each LLM is a highly parameterised probability distribution over the space of (code, description)-tuples with parameters estimated on a large diverse (i.e., non-task-specific) corpus.
This stochastic nature of language models is an important prerequisite for SEIDR, since it allows us to sample batches of diverse candidate solutions from $ \langmodel_\text{synth} $, $ \langmodel_\text{debug} $, and $ \langmodel_\text{text} $. 
We denote the number of outputs generated with $\treearity_\text{draft},$ $\treearity_\text{debug},$ and $\treearity_\text{explain},$ correspondingly.
Moreover, each model generates the most probable and less probable outputs in each batch, which helps diversify problem solving attempts. 
In the following implementation-related subsections, we explain how we vary the number of candidate solutions, debug instructions, and repairs generated in a batch by each LLM in SEIDR.

While SEIDR is described here as a multi-agent system, it can equally be seen as a form of evolutionary algorithm or genetic programming, where the initialization and mutation steps of the system are performed by LLMs, $ \langmodel_\text{synth} $ and $ \langmodel_\text{debug} $, correspondingly.
Throughout this work, we use agent-oriented programming terminology~\cite{shoham1993:agentoriented} and evolutionary optimization terminology interchangeably to try to bridge the gaps between these domains.

\subsubsection{Synthesize}
\label{sec:seidr-synth}

The framework starts with the SYNTHESIZE agent, which is responsible for generating initial draft solutions.
% to programming tasks to be repaired in later stages of SEIDR.
We start with a basic template for a chosen programming language that contains a number of standard library imports, as shown in Figure~\ref{fig:template}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth, trim={0mm 40mm 0mm 0mm}, clip]{images/Templates-new-v2.pdf}
    \caption{Anatomy of SYNTHESIZE templates}
    \label{fig:template}
\end{figure}

We populate this template with a comment indicating a textual task description and several I/O examples from the training set.
We design the templates with prompt engineering guidelines\footnote{~\url{https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results}} and prior work~\cite{debruin2021:autoencoders} in mind.
We then sample $\treearity_\text{draft}$ programs from $ \langmodel_\text{synth} $, setting \texttt{code} to the populated template and \texttt{description} to the natural language description of what the model should generate.
We use spring sampling:\footnote{~\url{https://vadim.me/publications/spring/}} a temperature-based sampling with a monotonically increasing temperature schedule where the $i$-th program is sampled with temperature $t_i \approx \frac{i-1}{\treearity_\text{draft}}$ (we use approximate equality to enable efficient implementation by means of batching).
Thus, the sampling procedure for the first programs approximates a deterministic maximum-likelihood estimation.
%Ultimately, this approach ensures that samples are diverse, but always contain the likeliest programs.
In combination with the naturalness principle of source code \cite{allamanis2018:survey,jiang2022:bugs}, this approach ensures that the samples are diverse, but always contain the most likely programs for the given task.

%\input{blogposts/spring.tex}

\subsubsection{Execute}
\label{sec:seidr-execute}

\todo{Holy war on the letter n}

The EXECUTE agent compiles the programs (if necessary) and launches them using the standard tools for the programming language.
The program is run once for every I/O pair in the validation set. 
Its \texttt{stdin} stream receives all input lines in a given input pair, and its \texttt{stdout} and \texttt{stderr} streams are captured and saved.
We then measure the \emph{score} of the program defined as the accuracy over the output lines, with \expoutputvec{} being the expected output, and $n=\max\{|\expoutputvec{}|, |\text{stdout}|\}$:
\[    
\text{score}(\expoutputvec{}, \text{stdout}) = \frac{\sum^{n}_i{\mathbb{I}[\text{stdout}_i = O_i]}}{n} 
\]
unless \texttt{stderr} is non-empty during compilation or execution, which is considered to indicate failure and is assigned a score of 0.

\subsubsection{Instruct}
\label{sec:seidr-instruct}

The goal of the INSTRUCT agent is to provide instructions that summarize bugs in a candidate program and suggest a solution for $ \langmodel_\text{debug} $. 
The resulting instructions with the bug summary should indicate what requirement is violated and instruct the LLM to edit the candidate program accordingly. 
The input to INSTRUCT consists of failing I/O pairs from the validation set and \texttt{stderr} output of the candidate execution. 
In order to represent this heterogeneous input as text that can be further processed by an LLM, we use template engines that replace placeholders in files or strings with input values and return a formatted string. 
In recent chat- and instruction-based LLMs, the terms \emph{template engine} and \emph{prompt template} are used interchangeably.

We consider two different designs of the INSTRUCT agent: INSTRUCT$^{\text{static}}$ and INSTRUCT$^{\text{LLM}}$ shown in Figure~\ref{fig:method-instruct}. 
In both cases, if \texttt{stderr} is not empty, i.e., execution exits with code 0 before getting any output to compare it with the expected output, the \texttt{stderr}-based template engine generates the instruction to fix the error. 
However, the designs differ in the way they transform failing I/O pairs to generate instructions in case \texttt{stderr} is empty.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth,trim={0mm 0mm 0mm 0mm}]{images/codex-for-psb-seidr-instruct-2.drawio.pdf}
    \caption{Overview of the two designs for the INSTRUCT agent.}
    \label{fig:method-instruct}
\end{figure}

INSTRUCT$^{\text{static}}$ uses a fixed template and substitutes placeholders for input and output with the corresponding strings of the first failing test case in its template engine.
For example, we show the resulting instruction for an exemplar template in Figure~\ref{fig:method-instruct}.
In contrast, INSTRUCT$^{\text{LLM}}$ uses the failing I/O pair in the LLM for text completion, thereby prompting the text LLM to produce the bug explanation and a summary for debugging. 
In addition to providing a failing test case or \texttt{stderr}, one may choose to give the model $ \langmodel_\text{text} $ more context, such as the problem name, task description, and the code generated so far. 
Each call to $ \langmodel_\text{text} $ can result in $\treearity_\text{explain}\ge1$ instructions, a batch output of an LLM.
The prompt templates used for the experiments are detailed in Section~\ref{sec:seidr-prompts}.

% An exemplar output of the code behavior template engine in Figure~\ref{fig:method-instruct} describes that the code returns output O instead of expected output O$_{\text{val}}$ for the failing test case with input string I$_{\text{val}}.$
% The LLM is then prompted to auto-complete this description of program behavior with the bug summary. 
% The bug summary is passed further to the next template engine that uses it as debugging instruction, such as ``\emph{Fix \{bug summary\}}''.

\subsubsection{Debug}

The main component of SEIDR that addresses the ``near-miss syndrome'' is the DEBUG agent.  
This agent iterates over all programs in the population to repair candidate programs and pass more tests. 
It uses the instructions written by INSTRUCT to sample from the $ \langmodel_\text{debug} $ model $\treearity_\text{debug}$ times
to repair each candidate and create a new population of \treearity{} candidates.
For $ \langmodel_\text{debug} $, the parameter \texttt{code} is set to the current version of the candidate solution and \texttt{descr} to the output of INSTRUCT and any additional context chosen for a specific implementation.
The current generation of candidates is then replaced by \treearity{} outputs of DEBUG.

\subsubsection{Rank}

The RANK agent implements what is known in genetic programming as \emph{parent selection}~\cite{koza1994:genetic}: it selects the best $\beamwidth{}$ programs to be further improved by the DEBUG agent.
We consider two different parent selection algorithms: tournament selection and lexicase selection. 
See Section~\ref{sec:seidr-lexicase-results} for their empirical comparison.

\emph{Tournament selection} variant used in this work sorts the programs according to their average test score and selects top $\beamwidth{}$ candidates. 
% Such ranking approach is a \emph{quality-based} selection method.
The programs are selected based on the intuition that repair of the best so far (yet imperfect) programs begets good programs. 
The simple ranking is also referred to as \emph{tournament selection}, where the best-performing candidates are chosen to participate in the next round.  
In the classical tournament selection, the first step is to draw $n$ random candidates from the population. 
In our implementation, the first step is to generate exactly $n$ candidates that are needed for the next round. 
This approach prioritizes candidates that perform the best on average over all tests.
% At the same time, tournament selection does not favor candidates that pass fully one or several tests but may not have a high average test pass rate score. 

\emph{Lexicase selection}~\cite{helmuth2015:solving} is a ranking approach that maximizes the diversity of selected candidates in addition to their metric-based score.
Lexicase selection ensures diversity by keeping the program candidates that perform the best on unique tests as opposed to the program candidates that perform best on average over all tests.
The algorithm is as follows:
\begin{enumerate}

\setlength{\parskip}{0pt}
\setlength\itemsep{0pt}

    \item randomly shuffle the set of tests;
    \item select a program with the best score on test 1;
    \item if several programs are tied, resolve the tie by selecting the best program on test 2;
    \item repeat for tests $3,4,\dots,$ until only one program is left;
    \item mark this program as ``selected'';
    \item if less than $\beamwidth$ programs are selected, go back to step 1.
\end{enumerate}
This ensures that even if the average quality of the selected candidates is lower, the batch of $\beamwidth{}$ programs collectively contains a higher number of required ``skills'', as measured by tests.



\subsection{Meaning of Hyperparameters}
\label{sec:seidr-beam-search}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth, trim={0mm 4mm 0mm 0mm}]{images/beamsearch.pdf}
    \caption{Repair-replace trade-off as a tree search problem.}
    \label{fig:beam-search}
\end{figure}

After evaluating a given candidate solution in EXECUTE, SEIDR supports two approaches to address the candidate's flaws:
\begin{itemize}
\setlength{\parskip}{0pt}
\setlength\itemsep{0pt}
  \item \emph{Replace} the candidate with another sample from the current population.
  \item Use INSTRUCT and DEBUG to repair the candidate.
\end{itemize}
We refer to this problem as the \emph{repair-replace trade-off}, by analogy with production economics~\cite{jack2000:optimal}. 

How does the choice of hyperparameters $\treearity{},$ the total number of candidate programs in each generation, and $\beamwidth{},$ the number of selected repairs to be preserved in a generation, influence the flow of SEIDR?
$\treearity$ and $\beamwidth{}$ act as upper bounds on the \emph{replace} option by limiting the size of the population.
In the edge cases, $\treearity{} = \beamwidth{} = 1$ corresponds to a repair-only process, while $\treearity{} = \beamwidth{} = \infty$ corresponds to replace-only, as illustrated in Figure~\ref{fig:beam-search}. 
Here, the repair-only scenario can also be seen as an LLM-guided random walk~\cite{xia2020:random} and replace-only as random sampling from the LLM.
Strategies with tree arities between $1$ and $\infty$ are similar to population-based evolutionary algorithms.
Note that $\treearity{}$ is defined by $\treearity_\text{draft}$ for the initial draft solutions in the first generation and $\treearity_\text{explain} \cdot \treearity_\text{debug}$ for later generations. 

Observe that a mutation-only genetic algorithm with tournament selection with fixed population size $\beamwidth{},$ such as SEIDR, is equivalent to \emph{local beam search} with beam width $\beamwidth{}$ on an $\treearity{}$-ary tree ~\cite[Section 4.1.4]{russell2010:artificial}. This corresponds to a known property of local beam search: it degenerates into a depth-first search at $\beamwidth{} = 1$, whereas setting $\beamwidth{} = \infty$ yields a breadth-first search.

% Hence, we refer to $\treearity{}$ as \emph{tree arity} and $\beamwidth{}$ as \emph{beam width}.

\section{Related Work}
\label{sec:seidr-related-work}

% Changes: explicit author surnames, order and some phrasing

Until recently, the tasks of neural program synthesis~\cite{gulwaniProgramSynthesis2017} and program repair~\cite{gouesAutomatedProgramRepair2019,petke2018:genetic,bugfixing,zhang2024systematic} have been considered separately.
However, results from genetic programming~\cite{sobaniaRecentDevelopmentsProgram2021} suggest that evolution is a crucial step in synthesis.
A number of important studies bridging this gap in the application of large language models have been carried out concurrently with this work, discussed below.

The use of large language models for a program repair step within a program synthesis pipeline has been studied by \cite{joshi2022:repair} and \cite{guptaSynthesizeExecuteDebug2020}, 
while the specific case of instruction-driven LLMs has been explored by \cite{fanAutomatedRepairPrograms2023}, where the initial synthesis is done by the Codex model~\cite{chenEvaluatingLargeLanguage2021} and for program repair both Codex and state-of-the-art specialized repair tools such as TBar and Recoder~\cite{just2014:defects4j} are considered and compared. 
\cite{zhangSelfEditFaultAwareCode2023} do the same, but fine-tune PyCodeGPT-110M~\cite{zanCERTContinualPretraining2022} to use it as a repair model. 
The resulting framework is a two-step process (1 draft step and 1 debug step), while iterative evolution and search are not explored. 

Evolution through Large Models (ELM)~\cite{lehmanEvolutionLargeModels2022} proposes to use a language model in place of a mutation operator within a traditional genetic programming framework~\cite{koza1994:genetic}. They use a type of instruction fine-tuned model trained on git commit messages known as a diff model.\footnote{~\url{https://carper.ai/diff-models-a-new-way-to-edit-code/}}
%~\cite{DiffModelsNew2023}. 
However, the model is directed neither to solve the programming problem at hand nor to fix bugs.
Instead, it is provided with generic instructions such as ``Change function f.'' 
This approach is meant for the cases where creative freedom~\cite{stanleyWhyGreatnessCannot2015a} is encouraged rather than satisfying concrete requirements.
\cite{liu2023algorithm} demonstrate the advantages of applying ELM to modular components of a solution, rather than the entire solution to the given problem.

Some related work is explicitly guided by the metaphor of conversation between agents. 
\cite{zhang2023steam} implements a repair-only program synthesis loop as a conversation between an analyst agent, a coder, and a tester, while \cite{dongSelfcollaborationCodeGeneration2023} does the same for tester, developer, and reviewer. 
The coder and the developer are roughly equivalent to SYNTHESIZE, the tester to EXECUTE, and the reviewer to INSTRUCT, while the analyst is an agent that prepares a generation prompt for the coder. 
Both testers are language models that predict the output of a program without actual compilation, execution, or testing, which makes \cite{dongSelfcollaborationCodeGeneration2023} and \cite{zhang2023steam} specific cases of chain-of-thought prompting~\cite{yuBetterChainofThoughtPrompting2023} for program synthesis.

Several studies explore an iterative approach to program synthesis in a manner similar to SEIDR~\cite{xiaConversationalAutomatedProgram2023,chenTeachingLargeLanguage2023,shinnReflexionLanguageAgents2023}. 
However, they do not explore the repair-replace trade-off and exclusively implement the repair-only approach that is prone to local minima.
SelfEvolve~\cite{jiangSelfEvolveCodeEvolution2023} is a repair-only version of a framework similar to SEIDR.
SeflEvolve demonstrates the benefits of LLMs evolving not just the source code but an additional natural language text file that acts as the system's knowledge base and is included in the model prompt when generating code. 
Finally, Self-Taught Optimizer (STOP)~\cite{zelikmanSelfTaughtOptimizerSTOP2023} takes the concept of self-improvement to the meta level and uses a large language model to edit the evolutionary algorithm itself (i.e., blocks or contents of Figure~\ref{fig:method}). 
Their reflections on the safety implications of such automated algorithm changes are of particular interest when considering this trajectory~\cite[Section 8]{zelikmanSelfTaughtOptimizerSTOP2023}. These concerns do not hold in the context of SEIDR because the algorithm is fixed.
In other words, SEIDR does not self-evolve but creates solutions that it improves, and the solutions that are synthesized are closely constrained by the validation set used by the EXECUTE agent.
% that will prevent FizzBuzz from scheming a planetary takeover.

These efforts to combine large language models and evolutionary techniques fall within the broader context of the quest for effective inductive bias in genetic programming~\cite{whighamSearchBiasLanguage1996}: a crucial property for any learning algorithm~\cite{haussler1988:quantifying}. 
\cite{reuterGraphNetworksInductive2023} suggest using graph neural networks for this purpose, while grammatical evolution methods, such as WHGE~\cite{bartoliWeightedHierarchicalGrammatical2020} and $\pi$-GE~\cite{oneill2004:pgrammatical} use the known grammar of the programming language.
Large language models are a novel and promising~\cite{custodeComparingLargeLanguage2024} alternative to grammatical evolution that incorporates semantics and idiom~\cite{allamanisMiningIdiomsSource2014,orlovFindingIdiomsSource2020} of a programming language in addition to its grammar. 

\section{Experimental Design}
\label{sec:seidr-eval}

To explore the capabilities of SEIDR and its generalizability, we test the framework on two benchmarks (PSB2 and
HumanEval-X), a total of two different ranking strategies (tournament selection and lexicase selection),
three models in the coding part of SEIDR (Codex, GPT-3.5, and Llama 3), two programming languages (Python and C++), and various branching factors.
We use three models over the two parts of our experiments: one model in the initial exploration (also reported by~\cite{liventsevFullyAutonomousProgramming2023}) and two models in the generalizability part.
The problems in the benchmarks originate from coding competitions and human-written programming assignments. 

During our empirical evaluation of SEIDR, we address the following research questions:
\head{\rqtreearity{}. Repair-replace trade-off exploration} 
What is the impact of using different tree search strategies
in the autonomous programming setting? 
We experiment with six different tree arities but fix the tournament selection in the ranking part and one prompt. 
Here, we study the impact of tree arity on the number of resolved problems as well as the speed of obtaining solutions.   
% \head{RQ2. Prompt engineering} What is the effect of using LLM-produced bug summaries compared to static instructions on the repair of automatically synthesized code? We test six static debug instructions that describe bug behavior based on violated requirements and five dynamic debug prompt templates auto-completed with LLMs. 
\head{\rqllama{}. Generalizability of the approach to different LLMs and an additional dataset} 
How does the choice of an LLM affect the performance of SEIDR? 
We vary the tree arity and experiment with two additional LLMs and one additional dataset.
By default, we use tournament selection as the ranking strategy. 
\head{\rqmultirun{}. Repeatability of SEIDR in multiple runs with the same hyperparameters} 
How does the non-deterministic nature of LLMs affect SEIDR performance when the method is restarted several times with the same hyperparameters?
We study how SEIDR results vary in different restarts of the same experiments with unchanged hyperparameters as a result of the LLM non-determinism. 
The motivation for \rqmultirun{} is that LLMs exhibit stochastic behavior: the same prompt can yield different responses. 
Essentially, LLMs generate answers token-by-token and predict the next tokens based on the probability distribution over a vocabulary of tokens, which is sensitive to the precision of floating-point operations. 
If two tokens are predicted to be the next ones, with a very similar probability, it is likely that either of them will be chosen at each individual run. 
Further tokens are generated auto-regressively and depend on the previous tokens, so once one token diverges, the whole sequence is likely to diverge, too. 
\head{\rqlexicase{}. Effect of changing the parent selection strategy in the RANK agent}
How does the lexicase selection-based ranking strategy impact performance in comparison to tournament selection in the RANK agent? 
We use the best-performing tree arities from \rqmultirun{} to run the experiments with lexicase selection as the ranking strategy instead of tournament selection, to explore whether a different parent selection algorithm can further improve the results.


\subsection{Data}
\label{sec:seidr-data}

Our experiments use the Program Synthesis Benchmark~2 (PSB2)~\cite{helmuth2022:applying} and HumanEval-X~\cite{zheng2023:codegeex} in C++ and Python. 
The key criteria for this choice are the availability of task descriptions in English and unit tests in Python and C++ or language-agnostic unit tests as well as the wide acceptance of these benchmarks in the areas of generative LLMs (HumanEval) and genetic programming (PSB2). 

\subsubsection{PSB2}
The first dataset is a benchmark suite of 25 problems for program synthesis that resemble small real-world tasks. PSB2 was developed as a more realistic and challenging version of PSB1~\cite{helmuth2015:general}, the latter consisting of textbook problems and is widely used in genetic programming~\cite{sobania2022:choose}. 
The problems require different data structures and control flows to be used for effective solutions and are taken from sources, such as competitive programming platforms and educational courses. 
The problems have descriptions in English, as well as 1 million~(M) tests for training and 1M testing-stage tests, including edge or corner cases that test the resulting program on complicated inputs. 
The tests are provided as I/O pairs and are distributed together with the problem descriptions as a PyPI package.\footnote{~\url{https://pypi.org/project/psb2/}} 

In PSB1, the training set consists of the edge test cases and is augmented by random test cases if the number of edge tests is not enough. The test set is formed by random test cases. 
This terminology is preserved in PSB2.
We use the PSB2 training set for ranking and selection of programs (validation) within an experiment and the test set for reporting the result thereof (testing).
Thus, we will refer to the PSB2 training set as the \emph{validation set}, to be more consistent with how it is used in SEIDR.

\subsubsection{HumanEval-X}
The second dataset that we use is a development from the original set of human-written programming tasks in HumanEval~\cite{chenEvaluatingLargeLanguage2021}, which is a standard code generation benchmark for LLMs.
HumanEval consists of 164 problems with a docstring representing problem description, a function signature, a correct solution, and unit tests in Python. 
HumanEval-X is the result of translating correct HumanEval programs and unit tests into five programming languages~\cite{zheng2023:codegeex}. 
We use HumanEval-Python for experiments in Python to ensure a comparison with other models in the setup without SEIDR. 
In addition, we test SEIDR on the HumanEval-C++ part of HumanEval-X. %here and in an earlier SEIDR study~\cite{liventsev2023:fully}. 

The test functions of HumanEval-X contain all tests in one function. We split the aggregated test functions into separate tests so that the RANK agent can evaluate the \text{score}. 
On average, the number of tests in HumanEval-Python is 7.25 and 6.95 in HumanEval-C++, which is appointed to a repeated additional test present in some HumanEval-Python examples of the following type: \texttt{assert True, "This prints if this assert fails 1 (good for debugging!)"}.
This test type is not present in HumanEval-C++.

To reiterate, we keep the original HumanEval-Python setup for direct comparison with the models tested on this benchmark without SEIDR. 
Because of the limited number of tests, we pass up to five tests to the draft prompt and make all tests visible to SEIDR for the debugging loop. 
In other words, we do not have a held-out test split for HumanEval-X in the same manner as we do for PSB2.


\subsection{Models}
\label{sec:seidr-models}

SEIDR uses up to three LLMs --- $ \langmodel_\text{synth} $, $ \langmodel_\text{text} $, and $ \langmodel_\text{debug} $ ---
in SYNTHESIZE, INSTRUCT$^{\text{LLM}}$, and DEBUG, respectively. 
% These models can be instantiated with the same LLM or different ones. 
The main prerequisite is that $ \langmodel_\text{synth} $ and $ \langmodel_\text{debug} $ are a text-to-code models which take both a textual description and a draft code as input.
Therefore, $ \langmodel_\text{synth} $ and $ \langmodel_\text{debug} $ can be a chat model, a code completion model, an instruction fine-tuned, or a foundation generative language model pre-trained on code in addition to text. 
By analogy, the text-to-text $ \langmodel_\text{text} $ can be a chat, instruction fine-tuned, a text completion, or a text generation model pre-trained on text and code.
% In our experiments, all three models are instantiated with the same fixed model. 

In our experiments, we use Open AI Generative Pre-trained Transformer (GPT) models by Open AI and Llama 3 by Meta~\cite{roziere2023:code}. 
GPT models are auto-regressive transformer models that have the decoder-only architecture as opposed to the original full encoder-decoder transformer.
They are pre-trained on both text and code and excel at sequence-to-sequence generative tasks, including code-to-code, text-to-code, and code-to-text.

In our initial experiments, we use Codex-edit\footnote{~\href{https://openai.com/index/gpt-3-edit-insert/}{code-davinci-edit-001}} 
as the LLM for writing and debugging programs and GPT-3\footnote{~\href{https://platform.openai.com/docs/deprecations}{text-davinci-003}} for bug summarization via text completion~\cite{brown2020:language} --- both being 175B-parameter models.
In our generalizability experiments, we use the GPT-3.5 model\footnote{~\href{https://platform.openai.com/docs/models/gpt-3-5-turbo}{gpt-3.5-turbo}} for program synthesis, bug summarization, and debugging. 
The GPT-3.5 model is an improvement over GPT-3 that is optimized for chat and available through an API.
This switch is mainly motivated by rapid model updates, an OpenAI announcement that GPT-3 was due to become obsolete, i.e., not actively supported by the company, along with the company's recommendation to switch to GPT-3.5. 

To further evaluate the generalization capabilities of SEIDR, we have chosen Llama 3-8B~\cite{roziere2023:code}, an open-source alternative to GPT models with the same standard decoder-only transformer architecture. 
Compared to Llama 2~\cite{touvron2023:llama}, Llama 3 introduces improvements to the architecture and the training process, such as grouped query attention.

\subsection{Prompts}
\label{sec:seidr-prompts}

\subsubsection{Prompts in the Initial Exploration of SEIDR}
\label{sec:seidr-prompt-strategies}

The prompt for the LLM model is static and consists of the input for editing --- candidate program generated so far --- and a debug instruction to repair the candidate. 
The debug instructions are formulated as templates. The instructions describe the violated requirements in terms of the wrong output in a failing I/O test or summarize the bug to capture issues in code logic.
We present debug instructions using the template engine format: the brackets \{ \} denote that the placeholder in the brackets will be replaced with the value generated during execution, \{I$_{\text{val}}$\} and \{O$_{\text{val}}$\} stand for values of the validation set I/O pair. As shown in Figure~\ref{fig:method-instruct}, the instruction to fix execution errors that abort the program before the resulting output is obtained with \texttt{stderr} lines: Fix \{stderr\}. Debug instruction that uses the output of candidate program execution is static, formulated as follows: 
\begin{equation}\label{seidr:prompt-0} 
    \text{Make sure that I}_{\text{val}} \text{ -> O}_{\text{val}}. \tag{S0}
\end{equation}


\subsubsection{Prompts for Instruction Fine-tuned and Chat Models}
\label{sec:seidr-ollama-prompts}
% Experiments with GPT-3.5 and Code Llama: 

The prompts presented in this section are used in the experiments with GPT-3.5 and Llama 3.
This implementation is optimized for instruction and chat models, which use prompts as inputs represented as text, partially with code fragments.
The models use a system message that describes the ``role'' of the LLM and a regular message that works as an instruction or a chat message from a user.
To provide more context, we always use a problem description, problem name, and programming language to the model as textual input (\texttt{descr}). 
As code, we also add an initial template depicted in Figure~\ref{fig:template} to $ \langmodel_\text{synth} $ and the current program candidate to $ \langmodel_\text{text} $ and $ \langmodel_\text{debug} $. The resulting prompts are as follows:

The system message is: 
\begin{lstlisting}
  You are an experienced software developer.
  You write concise code in {language}.
  The code must read input from user and return output corresponding to the task description.
\end{lstlisting}

The input to $ \langmodel_\text{synth} $ looks as follows: 
\begin{lstlisting}

Solve the following code contest problem: {problem_name.}
Problem description: {problem_description.}
{program_template}
Only complete the code, do not add triple quotes, do not give explanations.
\end{lstlisting}

Bug explanations are generated with $ \langmodel_\text{text} $ using the following instructions:
\begin{lstlisting}
I'm trying to solve the following code contest problem: {problem_name.}
Problem description: {problem_description.}
Currently, the code is
```
{program_candidate }
```
The issue is 
{stderr or "it must return {expected_output} for input {input},} 
but it returns {output".}
Describe how I should fix the code in a very concise manner. 
\end{lstlisting}

And the debugging model $ \langmodel_\text{debug} $ operates on the following instruction:
\begin{lstlisting}
Solve the following code contest problem: {problem_name.} 
Problem description: {problem_description.} 
Currently, the code is  
```
{program_candidate} 
```
Modify the code as {bug_summary.} 
You must only return correct code.  
Remove any triple quotes, language name or explanations.
\end{lstlisting}



\subsection{Repair-replace Trade-off Settings}
\label{sec:seidr-trade-off-settings}

The settings for tree arity will also be divided into two experiment sets: the ones for GPT-3 and Codex, and the ones for the GPT-3.5 and Llama 3 experiments.

\subsubsection{Tree Arity for the Initial Exploration of SEIDR}
\label{sec:seidr-tree arity-gpt-3}
As described in Section~\ref{sec:seidr-beam-search}, the population size (number of parents to choose from in the tournament selection or beam width from the beam search perspective) $\beamwidth{}$ and tree arity $\treearity{}$ define the repair-replace trade-off, where higher $\beamwidth{}$ and $\treearity{}$ correspond to repair over replace. 
We evaluate four options for these hyperparameters as shown in Table~\ref{tab:seidr:w-n-initial-exploration}. 
We only run the experiments once, due to the experimental timeline and the discontinuation of model support by the model provider. 
% The prompt for tree arity experiments is static and set to~\ref{seidr:prompt-0}.


\begin{table}[t]
\setlength{\tabcolsep}{20pt}
\centering
% % \vspace*{-1ex}
\caption{Initial exploration of SEIDR: hyperparameters in the tree arity experiments.}\small
\label{tab:seidr:w-n-initial-exploration}% \vspace*{-4mm}
%\footnotesize
\begin{tabular}{rcccc}
\toprule
experiment \# & 1 & 2 & 3 & 4 \\
\midrule
population size (beam width), $\beamwidth{}$ & 1 & 10 & 100 & $\infty$ (1000) \\[1pt]
tree arity, $\treearity{}$ & 1 & 10 & 100 & $\infty$ (1000) \\[1pt]
\midrule
max programs generated & \multicolumn{4}{c}{1000} \\[1pt]
prompt & \multicolumn{4}{c}{\ref{seidr:prompt-0}} \\[1pt]
models  & \multicolumn{4}{c}{\parbox{5cm}{\centering Codex as $p_\text{synth} \text{ and } p_\text{debug}$ 
% \\and the static prompt for explanations
}} \\[1pt]
\midrule
\parbox{4cm}{\raggedleft \# restarts (or runs) \\ with the same hyperparameters} &  
% \multicolumn{4}{c}{1, due to discontinued model support} \\[4pt]
\multicolumn{4}{c}{1} \\[8pt]
datasets  & \multicolumn{4}{c}{PSB2} \\[1pt]
languages  & \multicolumn{4}{c}{Python, C++} \\
\bottomrule
\end{tabular}
\end{table}

Because we aim to compare tree search parameters, we fix one default debugging instruction~\ref{seidr:prompt-0} and use the INSTRUCT$^{\text{static}}$ agent.  
Moreover, we set the upper limit for the total number of generated program candidates to 1000 to limit the experimentation time. 
Although some solutions may not be found within the hard limit, we assume\footnote{~This assumption is later confirmed in Section~\ref{sec:seidr-seidr:rqtreearity}.} that 1000 program candidates form a sufficiently large search space for our experiments.
$\beamwidth{} = \treearity{} = \infty$ is achieved in implementation by setting equal $\beamwidth{}$ and $\beamwidth{}$ equal to the upper limit of the program count of 1000.
This ensures that a second generation of programs does not exist.


\subsubsection{Tree Arity for SEIDR Generalizability Experiments}
\label{sec:seidr-tree arity-ollama} 
With the shift to chat and instruction models in the generalizability part of our study, we move from generating one bug explanation and one code draft or update to a batch of those. 
Specifically, each of the three LLMs in SYNTHESIZE, INSTRUCT, and DEBUG  can generate sequences in batches. 
We generate $\treearity_\text{draft}$ programs in the first generation with $ \langmodel_\text{synth} $ model, $\treearity_\text{explain}$ bug explanations with $ \langmodel_\text{text} $ for each program in a generation, and $\treearity_\text{debug}$ candidate repairs for each of the debugging instructions using $ \langmodel_\text{debug} $.
A new generation of $\treearity_\text{explain} \cdot \treearity_\text{debug} \cdot \beamwidth{}$ programs created from each of $ \beamwidth{}$ parents in a previous generation is ranked and filtered to keep the best-performing $\beamwidth{}$ candidates for generating the next candidates. 

To balance between a reasonable number of experiments and diverse sets of hyperparameters, we fix $\treearity_\text{explain}=2$ to moderately vary the bug descriptions and set $\treearity_\text{draft} = \treearity_\text{debug} = \treearity{}.$
As a reference, in the experiments with GPT-3 and Codex, we generated only one bug explanation ($\treearity_\text{explain} = 1$) and used $\treearity_\text{draft} = \treearity_\text{debug} = \treearity{}$ setting, too. 
We evaluate six options of $\treearity{}$ 
% $ \in \{1,4,8,10,16,100\}$ 
as shown in Table~\ref{tab:w-n-generalizability} and use tournament selection as the ranking strategy. 
% in the experiments with average quality-first ranking and four non-corner case options for quality-diversity ranking with lexicase selection. 

The choice of these tree branching hyperparameters and the maximum number of generated programs is motivated by the experiments with GPT-3 and Codex, where the best results were obtained for $\treearity{}=10.$ 
Therefore, we explore the area around this value more closely in the generalizability experiments.
In the same experiments, the majority of problems in PSB2 were solved within the first 100 generated programs.
Therefore, the upper limit for the total number of generated program candidates is set here to 100 to limit the experimentation time.
% Note that the setting $\treearity_\text{draft}=\beamwidth{}=\infty$ ensures that a second generation of programs does not exist.

To account for the stochasticity of language models and the fact that OpenAI's models do not support setting the effective sampling temperature to zero to force deterministic behavior,\footnote{~\url{https://community.openai.com/t/observing-discrepancy-in-completions-with-temperature-0/73380}} we ran the experiments six times with each set of hyperparameters.
This number of runs was selected to hit a sweet spot between the overall running time and costs of the experiments, while at the same time achieving confidence in the stability of the results in the presence of non-determinism. 
Note that reporting results over six runs is considerably better than the common practice of having only one run for every selection of hyperparameters, and it is in line with the best-of-class practice in the field of LLMs for code generation~\cite{ouyang2023:llm}.
% but is smaller than tens or hundreds of runs usually seen in the software engineering and genetic improvement domains~\cite{helmuth2022:applying}.

The total cost of running the experiments with GPT-3 and Codex were around 550 USD, and the experiments with GPT-3.5 amounted to 266 USD.\footnote{~For comparison, one run with GPT-4o cost us 315 USD (early July 2024), so further use of this model was discarded.}
% Because of the cost of each run with GPT-3.5 amounts to {\color{red}??? USD} and 
Moreover, the time to finish one run with several branching factors and all the tests amounts to ca. 42h for PSB2\footnote{~Due to the local setup for testing, API call limits, and the number of tests.} and ca. 156h for HumanEval-X.
Overall, we restart experiments with GPT-3.5 six times for each of the tree arities $ N_{\text{synth}} = N_{\text{debug}} = N^* \in \{1,2, 4,10,16,100\}$ and the same for Llama 3, with a total of $6 \times 6 \times 2 = 72$ experiments. 
For the lexicase selection experiments, we have 6 runs per model but one best-performing tree arity, which adds $12$ experiments to the total count.

\begin{table}[t]
\setlength{\tabcolsep}{10pt}
\centering
\caption{SEIDR generalizability experiments: hyperparameters in the tree arity grid search.}\small
\label{tab:w-n-generalizability}
%\footnotesize
\begin{tabular}{rcccccc}
\toprule
experiment \# & 16 & 17 & 18 & 19 & 20 & 21\\
\midrule
population size (beam width), $\beamwidth{}$ & 1 & 4 & 8 & 10 & 16 & $\infty$ (100) \\[4pt]
\# programs in the 1st generation, $\treearity_\text{draft}$ & 1 & 4 & 8 & 10 & 16 & $\infty$ (100) \\[4pt]
\# bug explanations for candidate, $\treearity_\text{explain}$ & 2 & 2 & 2 & 2 & 2 & - \\[4pt]
\# repairs for each explanation, $\treearity_\text{debug}$ & 1 & 4 & 8 & 10 & 16 & - \\[4pt]
\midrule
max programs generated & \multicolumn{6}{c}{100} \\[4pt]
prompts & \multicolumn{6}{c}{see Section~\ref{sec:seidr-ollama-prompts}} \\[4pt]
models  & \multicolumn{6}{c}{
 \parbox{5cm}{
     (a) GPT-3.5 as $p_\text{synth,} \; p_\text{debug,} \; p_\text{explain,}$ \\
     (b) Llama 3 as $p_\text{synth,} \; p_\text{debug,} \; p_\text{explain}$
     }
} \\[10pt]
\midrule
\# runs per experiment &  \multicolumn{6}{c}{6} \\[4pt]
datasets  & \multicolumn{6}{c}{PSB2, HumanEval-X} \\[4pt] 
languages  & \multicolumn{6}{c}{Python, C++} \\[4pt]
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance Indicators}
\label{sec:seidr-metrics}

\sloppy %
In our experiments, we compare 
the number of fully solved programs obtained with SEIDR with different values of hyperparameters. 
For a more detailed analysis of results, we use \emph{test pass rate (TPR)} and \emph{Excess Programs Generated (EPG)}.
TPR reflects the percentage of fully passed test cases based on the exact match of program output and test output. 
The TPR metric is used for the final evaluation of generated programs and does not reflect partial passing of the I/O test as opposed to the \emph{score} as calculated by the RANK agent (see Section~\ref{sec:seidr-execute}). 

We define \emph{pass@k} as the number of problems that have $TPR=1$ if SEIDR is stopped after generating $k$ programs, following~\cite{kulal2019:spoc} \emph{``success rate at budget of $k$ programs.''}
Note that \cite{jiangSelfEvolveCodeEvolution2023} and \cite{chenTeachingLargeLanguage2023} define $k$ as the number of restarts of the iterative method, the budget in terms of trees of programs.
We choose against this approach, since it threatens the validity of the comparison between iterative tree-based program synthesis and repair-only baseline by giving the iterative approach additional budget in terms of the number of programs it can generate.

Codex \cite{chenEvaluatingLargeLanguage2021} calculates pass@n>k and constructs an unbiased estimator of pass@k with lower variance, ensuring statistically robust results.
We cannot apply this adjustment for SEIDR, since the adjustment assumes that the programs are independent and identically distributed, while SEIDR is a Markov chain with dependencies between iterations. 

EPG reflects the number of programs generated before the first occurrence of the program that passes all validation test cases.
% DEBUG and EXECUTE agents generate a number of programs that are replaced or repaired during the search for solution program. 
% The n is referred to as EPG. 
EPG is indicative of the computational cost of solving a problem distributed in terms of LLM inferences and program compilations and executions.
For a single execution of SEIDR, EPG is equivalent to the smallest $k$ at which pass@k=1.

\subsection{Implementation Details}
\label{sec:seidr-implementation}


To summarize the setup, in this study, we have two groups of experiments. 
% : one with Codex for code generation and GPT-3 for the debug agent, and the other with Llama 3 or GPT-3.5 for all agents. 
% 
The first group of experiments is dedicated to the initial exploration of \rqtreearity{}
% RQ1 
with Codex-edit (code-davinci-edit-001) as the LLM for writing and debugging programs. 
% and GPT-3 (text-davinci-003) for bug summarization via text completion. 
% We ensure that the program candidates generated from the same parent program are different from each other by changing the temperature parameter of Codex-edit.
We have referred to these experiments as \emph{Initial Exploration of SEIDR} with Codex and GPT-3, and test the hyperparameter choices only on PSB2 as detailed in Table~\ref{tab:seidr:w-n-initial-exploration}.
Here, we use wide steps between tree arity values (see Section~\ref{sec:seidr-tree arity-gpt-3}).
 
The second set of experiments mainly focuses on the generalizability (\rqllama{}) of SEIDR
and its robustness to restarting experiments with the same hyperparameters (\rqmultirun{}).
The motivation here is to potentially improve on GPT-3 with a newer, generally more powerful version, GPT-3.5, and its smaller open-source competitor, Llama 3.
GPT-3.5 and Llama 3 are used in more fine-grained repair-replace trade-off exploration and ranking experiments (\rqtreearity{}). 
We have referred to these experiments as \emph{SEIDR Generalizability Experiments} and test SEIDR both on PSB2 and HumanEval.
Building on the findings of the initial exploration, we use more fine-grained tree arity values (see Section~\ref{sec:seidr-tree arity-ollama}, Table~\ref{tab:w-n-generalizability}) and use the prompts from Section~\ref{sec:seidr-ollama-prompts}. 
Thus, INSTRUCT is represented by the INSTRUCT$^{\text{LLM}}$ agent and creates $\treearity_\text{debug}$ bug summaries.
Each program update creates $\treearity{}$ child programs from one parent with the SYNTHESIZE and DEBUG agents.
We also compare the performance of SEIDR with the current state-of-the-art without SEIDR (see Section~\ref{sec:seidr-results-rqllama}).

The second set of experiments is further updated with an alternative ranking strategy, lexicase selection (\rqlexicase{}). 
For each model, dataset, and language, we choose the best-performing tree arity from \rqllama{} and exchange the tournament selection algorithm with the lexicase selection. 
This selection step chooses parents for debugging updates in each generation. 


In all experiments, we set the limit to generate a maximum of $M$ program candidates during the search for the candidate that passes all validation tests. 
If we reach $M$ candidates and none of them pass all validation tests, we store the test pass rate for the last generated candidate and the best test pass rate achieved throughout the search. 
For the first set of experiments, we set $M = 1000,$ and for the generalizability ones, we limit $M$  to $100,$ after finding out that for the majority of problems, a solution is found among the first 100 programs or not found at all.



Following~\cite{psb2}, we use 2000 I/O pairs ($\left(I, O\right)_{test}$ in Figure~\ref{fig:method}) from the test split of PSB2 to evaluate the candidate program that has passed all validation test cases ($\left(I, O\right)_{val}$ in Figure~\ref{fig:method}) during debugging. 
Due to repetitive calls to EXECUTE, we have to resolve the speed of testing versus precision trade-off while choosing the number of validation test pairs.
We resolve the trade-off by fixing the validation set size at 100 for the initial experiments and 50 for the generalizability exploration, which has more runs with the same hyperparameters. 
We have run a preliminary experiment to confirm that we do not lose the final test pass rate points on 2000 tests when we decreased the validation test set size from 100 (which was used in the GECCO-2023 paper) to 50 for the generalizability exploration.
Due to a small number of tests in HumanEval-X, all tests are made visible to the debugging LLM and during the validation step.  
To operate with the chosen LLMs in SEIDR, we use ollama\footnote{~\url{https://ollama.ai/}} and LangChain.\footnote{~\url{https://www.langchain.com/}}  
To ensure that the program candidates generated from the same parent program are different from each other, we change the temperature parameter of the LLMs. 
 

\section{Results and Discussion}
\label{sec:seidr-results}

In this section, we present the results of the initial exploration, where we investigate the repair-replace trade-off in SEIDR with Codex and GPT-3 (\rqtreearity{}) using the PSB2 benchmark.
We then continue with generalizability (\rqllama{}) and repeatability (\rqmultirun{}) experiments with GPT-3.5 and Llama 3 on PSB2 and HumanEval.
Finally, we test lexicase selection as the ranking strategy (\rqlexicase{}).

\subsection{Initial Exploration}

\subsubsection{Repair-replace Trade-off in the Initial Exploration of SEIDR}
\label{sec:seidr-seidr:rqtreearity}
We compare the number of solved problems in the experiments with tree arity of 1, 10, 100, and $\infty$ and fixed debug instruction \ref{seidr:prompt-0} in Python and C++ in Figure~\ref{fig:seidr:solved-vs-bf}. 
The results of SEIDR are compared to the baseline performance of PushGP on the PSB2 benchmark, which solves 17 out of 25 problems. 
Note that experiments with $N=1$ and $N=\infty$ can be considered as ablation studies, where the replace option and repair option are turned off correspondingly. 
 %

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth, trim={0mm 2.8mm 0mm 2mm}, clip]{images/num_solved_problems_vs_bf_1000_v3_zenodo.pdf}
  %
  % % \vspace*{-2mm}
  \caption{Number of solved PSB2 problems depending on tree arity in beam search for prompt type \ref{seidr:prompt-0}.}
  \label{fig:seidr:solved-vs-bf}
\end{figure}

The results highlight the benefit of compromise strategies with tree arity of 10 and 100 over repair-only ($N=1$) and replace-only ($N=\infty$) strategies. 
The results show that the repair-only scheme is outperformed by other strategies. 
We explain the poor performance of the repair-only strategy by the fact that the search space is under-explored. 
Specifically, the replace scenario ensures that the LLM for the debugging component represented by Codex-edit in our experiments generates different updates of program candidates using variable temperatures.
The probability of finding a better fix is higher when more alternatives are generated to update the draft program at $N>1$ compared to $N=1$. 
The search strategy with $N=10$ yields the best results: it performs on par with PushGP for C++ and outperforms the baseline during Python program synthesis by +2 problems, resulting in a total of 19 programs that pass all test cases.
The results imply that generating a moderate number of programs in parallel during the DEBUG step works better than the policies in which more updates are generated for each program (100 or 1000), 
or those in which only one program is updated iteratively.

We present the analogy of the solution speed for all four arities and the fixed default debug instruction in Figure~\ref{fig:seidr:epg-distribution}. 
In detail, we show the distribution of EPG values in all experiments to explore how many candidate updates are generated before the solution is found.
We zoom in to the cases with solutions found with up to the first 10 program candidates in Figure~\ref{fig:seidr:epg-distrib-solved-10} and show the EPG distribution with the step of 100 candidates in Figure~\ref{fig:seidr:epg-distrib-solved-100}. 
In addition, we break down the results into each tree arity in Figure~\ref{fig:seidr:epg-bf}, showing the EPG on a heatmap scale and the TPR as a number between 0 and 1, or the signs ``+'' if a problem is solved and ``-'' if the final TPR is 0. 


\begin{figure}[t]
 %
\begin{subfigure}[t]{\columnwidth}
\centering
\includegraphics[width=.7\linewidth, trim={0mm 4mm 0mm 0mm}]{images/epg_distribution_solved_maxprog_1000_1_v3_zenodo.pdf}
  \caption{0 $\leq$ EPG $\leq$ 10 with step 1.}
  \label{fig:seidr:epg-distrib-solved-10}
\end{subfigure}

% \vspace{2mm}

\begin{subfigure}[t]{\columnwidth}
\centering
\includegraphics[width=.7\linewidth, trim={0mm 4mm 0mm 0mm}]{images/epg_distribution_solved_maxprog_1000_100_v3_zenodo.pdf}
  \caption{0 $\leq$ EPG $\leq$ 1000 with step 100.}
  \label{fig:seidr:epg-distrib-solved-100}
\end{subfigure}
% % \vspace{-2mm}
\caption{Distribution of the number of generated programs during each problem-solving attempt in the experiments with different tree arities where a problem solution is found.}
\label{fig:seidr:epg-distribution}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.56\textwidth, trim={3mm 1.6mm 3mm 2mm}, clip]{images/num_programs_generated_vs_bf_test_pass_rate_vertical_maxprog_1000_v3.pdf}
  \caption{Number of excess programs generated (in color) and test pass rate (as numbers) depending on tree arity. Higher EPG values are shown in darker shades. We denote solved problems with ``+'' (test pass rate = 1), unsolved problems with ``-'' (test pass rate = 0), and show the test pass rate for partially solved problems. }
  \label{fig:seidr:epg-bf}
\end{figure}

Out of 100 experiments for each language, in 21--24\% of runs in Python and C++, the draft program is already the solution (EPG=0). 
For 31--33\% of the experiments, the solution is found after discarding 5 candidates. 
Around half of the experiments do not generate more than 100 programs. 
However, 5 problems are solved with more than 500 generated programs in Python and 1 problem in C++ (with $N=10$).
The results imply that the first steps in updating the draft program are crucial for solving the problem. 
The chances of solving the problem in later stages of the search, such as after 100 programs have been generated, are low.
This confirms our initial assumption in Section~\ref{sec:seidr-trade-off-settings} that 1000 programs are sufficient.

To briefly analyze Figure~\ref{fig:seidr:epg-bf}, we observe that some problems are solved in both languages, whereas some others --- only in Python. 
In addition, only five problems are not solved in any SEIDR configuration in Python (bouncing-balls, bowling, cut-vector, dice-game and leaders) and seven in C++ (bouncing-balls, cut-vector, dice-game,  leaders, shopping-list, snow-day, and vector-distance).
Upon closer inspection of generated programs, we have noticed that in bouncing-balls, the programs have logical errors and differ considerably between programming languages, as well as in the majority of unsolved problems. 
Test cases and debug instructions in bowling frequently skewed the resulting programs to return answers to individual bowling score strings instead of writing an algorithm to calculate the score based on each next character.
The latter mistake happened in other unresolved problems, such as cut-vector.
Qualitative analysis has also shown that some programs failed to read input from the user and instead defined input strings within the code, which limited the program to testing only one I/O pair, although the algorithm was correct.


\begin{highlight}
\textbf{Repair-replace trade-off in the initial exploration (\rqtreearity{}):} 
SEIDR with Codex as the coding LLM outperforms the PushGP baseline on PSB2 in Python and performs on par with it in C++ experiments with tree arity of 10. 
Search strategies with tree arity larger than one benefit from the replace possibility of the SEIDR framework as a consequence of using variable temperature for Codex-edit.
The repair component is also crucial for the framework because the replace-only search policy (with tree arity of $\infty$) performs worse than the policies that alternate between replace and repair during the program update (with tree arity of 10 or 100).  
\end{highlight} 


\subsection{Generalizability Experiments}

In this section, we present and discuss replace-repair trade-off results obtained with GPT-3.5 and Llama 3 and the effect of switching from tournament selection to lexicase selection with the best hyperparameter settings found for the trade-off. 
We count the number of fully solved problems (i.e., reached $TPR=1$) in experiments with the hyperparameter settings described in Table~\ref{tab:w-n-generalizability} in six runs and present the language-specific results for PSB2 and HumanEval.
% in Figure~\ref{fig:repair-replace-trade-off-generalizability}. 
We also explore the total number of programs that need to be generated before a solution is obtained, as well as
% in Figure~\ref{fig:epg-distribution} 
in how many of the six runs each problem is fully solved.
% in Figures~\ref{fig:epg-num-solved-psb2},~\ref{fig:epg-num-solved-he-python}, and~\ref{fig:epg-num-solved-he-c++}. 
The figures are described in detail in the dedicated sections. 

\subsubsection{Repair-replace Trade-off and Robustness to Restarts in the Generalizability Experiments.}
\label{sec:seidr-treearity-ollama}\label{sec:seidr-results-rqllama}

We compare the number of solved problems in the experiments with $\treearity_\text{draft}=\treearity_\text{debug}$ values of 1, 2, 4, 10, 16, $\infty$ (100) and $\treearity_\text{explain}=2$ in Python and C++ and tournament selection ranking strategy in Figure~\ref{fig:repair-replace-trade-off-generalizability}. 
We will refer to the equally set values of $\treearity_\text{draft}$ and $\treearity_\text{debug}$ as $N^*$ hereafter.
As before, experiments with $N^*=1$ and $N^*=\infty$ correspond to ablation studies, where the replace option or the repair option is turned off. 
The results of SEIDR are compared to the baseline performance of PushGP on the PSB2 benchmark, which solves 17 out of 25 problems. 
The boxplots show the inter-quartile range between the first and third quartiles observed in six runs and the median values as horizontal lines within the boxes. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Num solved problems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[bt]
\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 0mm 0mm 0mm}]{images/num_solved_problem_psb2_6runs_boxplot_v5.pdf}
%\vspace{-15pt}
  \caption{PSB2}
  \label{fig:num-solved-psb2-gpt3.5}
\end{subfigure}
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 0mm 0mm 0mm}]{images/num_solved_problem_humaneval_6runs_boxplot_v5.pdf}
%\vspace{-15pt}
  \caption{HumanEval}
  \label{fig:num-solved-he-gpt3.5}
\end{subfigure}
%\vspace{-16pt}
\caption{Repair-replace trade-off as a tree search problem in SEIDR: the total number of solved problems as measured by $TPR=1$ using SEIDR with GPT-3.5 and Llama 3 depending on tree arity $N^*$.}
\label{fig:repair-replace-trade-off-generalizability}
\end{figure}


The trend present for all the datasets and models is that the results for Python are more condensed over six runs than for C++. 
Since access to statistics about the training data for Llama 3 and GPT-3.5 is not provided, reasons for more stable performance in Python across runs can possibly lie in the training data distribution but cannot be confirmed. 
However, the fact that HumanEval-Python is a popular code generation benchmark against which models are compared may affect the results on this and other Python benchmarks.
In the same line of comparison of results between two programming languages, SEIDR with GPT-3.5 performs better in Python than in C++ on PSB2. 

% In the majority of experiments, based on the number of fully solved problems, it is beneficial to use the tree arity larger than 1 and less than $\infty, $ except for GPT-3.5 on HumanEval-C++. 
% This result confirms that, for the majority of cases, using SEIDR that builds a tree of solutions is better than repairing only one program or regenerating the program from scratch every time. 
% 
Llama~3 performs worse than GPT-3.5 on PSB2 in both languages.
This dataset has more test cases in stock than HumanEval and can be considered a more thorough test of the coding and debugging capabilities of LLMs. 
Following a recent trend, where larger models outperform smaller ones, the results of SEIDR on PSB2 confirm that the smaller model (Llama~3) performs worse than the larger one (GPT-3.5).

Looking back at the initial experiments with Codex and PSB2, we notice the degradation of performance from Codex to GPT-3.5: SEIDR with Codex solved 19 problems in Python and 17 in C++ with tree arity 10, while the best-performing result of SEIDR GPT-3.5 is 16 (tree arity of 10, too) in Python and 11 in C++ (several tree arities, but not 10). 
This result can be explained by the focus of LLM builders on the generalization of knowledge and performance on a variety of tasks, while Codex specializes in code generation.
Moreover, due to increased costs from Codex to GPT-3.5, we decrease the maximum number of generated program candidates from 1000 for Codex to 100 for GPT-3.5. 
However, only two problems are solved with Codex with $EPG > 100$: indices of substring (at program candidate \#510) and substitution cipher (at candidate \#210) in Python, bowling (candidate \#912) and indices of substring (\#410) in C++. 
Meanwhile, some problems are solved by Codex earlier than at the debugging attempt \#100 and not solved by GPT-3.5 and vice versa. 
Therefore, the reduction of the maximum generated programs has only a partial effect on the difference between the results of the two models. 


\begin{figure}[bt]
\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 0mm 0mm 0mm}]{images/mean_tpr_psb2_6runs_boxplot_v5.pdf}
%\vspace{-15pt}
  \caption{PSB2}
  \label{fig:mean-tpr-psb2-gpt3.5}
\end{subfigure}
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 0mm 0mm 0mm}]{images/mean_tpr_humaneval_6runs_boxplot_v5.pdf}
%\vspace{-15pt}
  \caption{HumanEval}
  \label{fig:mean-tpr-he-gpt3.5}
\end{subfigure}
%\vspace{-16pt}
\caption{Repair-replace trade-off as a tree search problem in SEIDR: mean $TPR$ measured in \% obtained using SEIDR with GPT-3.5 and Llama 3 depending on tree arity $N^*$.}
\label{fig:mean-tpr-repair-replace-trade-off-generalizability}
\end{figure}


In addition to reporting the number of fully solved problems, Figure~\ref{fig:mean-tpr-repair-replace-trade-off-generalizability} also reports the mean Test Pass Rate measured in \%. 
For HumanEval-C++, SEIDR with GPT-3.5 has better results with smaller tree arity values than with larger ones.
In Python, experiments with $1 < N^* < 100$ (i.e., non-corner case values of $N^*$) yield better mean TPR for SEIDR with GPT-3.5 on PSB2 and with Llama~3 --- for HumanEval-Python.
SEIDR with Llama~3 performs with slightly better mean TPR towards larger $N^*$  on PSB2-C++, and at the same time, it performs well on HumanEval-C++, so the difference between results with different $N^*$ is small.
The maximum of mean TPR for both datasets and models and max number of solved problems are obtained with $N^*=16$ or less, except for Llama 3 in PSB2-C++. 
From this part of the experiments, we notice that moderate or small tree arities $(N^* \le 16)$ are preferred, but there is no one leading tree arity.


The number of runs in which a problem is solved and the average speed of finding those solutions are shown in Figure~\ref{fig:epg-num-solved-psb2} for PSB2, Figure~\ref{fig:epg-num-solved-he-python}
for HumanEval-Python and Figure~\ref{fig:epg-num-solved-he-c++} for HumanEval-C++.
These figures show which problems were not solved in any run of any experiment (a row with zeros colored white) and what problems are easier to solve than others (e.g., solved in all runs, or at least one with each tree arity $N^*,$ or earlier in the search tree and shown in brighter rather than darker color but not white). 
We also show the results of lexicase selection runs marked with ``lex.'' in these three figures, but will discuss them in a separate section. 

The majority of solved PSB2 problems are solved in more than one run per setting with GPT-3.5 and faster (with fewer attempts) in Python than in C++ as illustrated in Figure~\ref{fig:epg-num-solved-psb2}.
For Llama~3, most of the solutions are obtained in 1--3 runs.
The trend for both datasets is that, for problems where a solution is found, SEIDR with GPT-3.5 makes fewer attempts in Python (brighter colors prevail in the corresponding parts of Figures~\ref{fig:epg-num-solved-psb2} and~\ref{fig:epg-num-solved-he-python}) than SEIDR with GPT-3.5 in C++ or SEIDR with Llama~3 in both languages (darker shades in the GPT-3.5 on C++ and Llama~3 parts of Figures~\ref{fig:epg-num-solved-psb2} and \ref{fig:epg-num-solved-he-c++}).


\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth, trim={3mm 2.8mm 3mm 2mm}, clip]{images/epg_mean_and_num_runs_problem_solved_avg_score_check_w_lexicase_psb2_6runs_heatmap_v5.pdf}
  \caption{PSB2: mean Excess Programs Generated (in color) and the number of runs in which a task is solved. The experiments in which a specific problem is not solved in any run are shown in white.}
  \label{fig:epg-num-solved-psb2}
  %\vspace{-3ex}
\end{figure}

\begin{figure}[hbt!]
  \centering
  % {left bottom right top},
  \includegraphics[width=.88\linewidth, trim={0mm 3mm 0mm 2.6mm}, clip]{images/epg_mean_and_num_runs_problem_solved_avg_score_check_w_lexicase_humaneval_Python_6runs_heatmap_v5.pdf}
  \vspace{-4pt}
  \caption{HumanEval-Python: mean Excess Programs Generated (in color) and the number of runs in which a task is solved. The problems that are not solved in any run are colored white.}
  \label{fig:epg-num-solved-he-python}
  \vspace{-12pt}
\end{figure}

\begin{figure}[hbt!]
  \centering
  \includegraphics[width=.88\linewidth, trim={0mm 3mm 0mm 2.6mm}, clip]{images/epg_mean_and_num_runs_problem_solved_avg_score_check_w_lexicase_humaneval_C++_6runs_heatmap_v5.pdf}  %
  \vspace{-4pt}
  \caption{HumanEval-C++: mean Excess Programs Generated (in color) and the number of runs in which a task is solved. The problems that are not solved in any run are colored white.}
  \label{fig:epg-num-solved-he-c++}
  \vspace{-12pt}
\end{figure}


% Given resource constraints, such as the costs of calling GPT-3.5 and the time of running each experiment, we limit restarts to 6 runs in total with each set of hyperparameters.
% However, t

To explore the capabilities of SEIDR with the studied models, we can take a union over experiments and count the number of problems solved at least once across 36\footnote{~We have 6 runs and 6 different values of $N^*.$} restarts with a fixed dataset, language and model.
In this way, in addition to reporting the number of problems solved in each run in the boxplot, we can derive the number of problems solved in any run with any set of hyperparameter using information in Figures~\ref{fig:epg-num-solved-psb2}--\ref{fig:epg-num-solved-he-c++}.
For example, SEIDR with GPT-3.5 does not solve only 7 problems out of 25 PSB2-Python tasks (see Figure~\ref{fig:epg-num-solved-psb2}).
Namely, bouncing-balls, cut-vector, dice-game, indices-of-substring, middle-character, solve-boolean, and substitution-cipher are solved in 0 runs with all the variations of $N^*$.
% $=N_{synth}=N_{debug}.$
In other words, 18 unique problems are solved in the collection of all the experiments using SEIDR with GPT-3.5 on PSB2-Python. 
SEIDR with GPT-3.5 does not solve 12 out of 25 PSB2-C++ tasks with any hyperparameter settings. 
The number of solved problems in any run with SEIDR and Llama~3 are 10 for PSB2 in both languages. 
To support the finding about degradation of performance happening in a non-specialized model GPT-3.5 compared to the code-specialized Codex, we calculate the number of solved problems in any run with Codex and obtain 20 problems solved at least once in Python and 18 in C++. 

Similarly, SEIDR with GPT-3.5 solves 141 out of 164 HumanEval-Python problems at least once in all runs, collectively, and 128 problems with Llama~3 (see Figure~\ref{fig:epg-num-solved-he-python}).
In HumanEval-C++, SEIDR with GPT-3.5 solves 163 out of 164 problems (except CPP/137) in the union of all runs and 162 with Llama~3 (except CPP/137 and CPP/151) as follows from Figure~\ref{fig:epg-num-solved-he-c++}.


In Figure~\ref{fig:epg-distribution}, we present the analogy of the speed of obtaining solutions, EPG. 
We zoom in to the cases with solutions found with up to the first 10 program candidates in Figures~\ref{fig:psb2-epg-distrib-step-1} and~\ref{fig:humaneval-epg-distrib-step-1} for PSB2 and HumanEval-X, respectively. 
The coarser-grained EPG distribution with the step of 10 candidates is shown in Figures~\ref{fig:psb2-epg-distrib-step-10} and~\ref{fig:humaneval-epg-distrib-step-10}. 


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % EPG distribution
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
\begin{subfigure}{.8\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 3mm 0mm 0mm}, clip]{images/epg_distribution_step_1_psb2_6runs_barplot_v5.pdf}
  \caption{PSB2: 0 $\leq$ EPG $\leq$ 10 with step 1.}
  \label{fig:psb2-epg-distrib-step-1}
\end{subfigure}
% 
% 
\begin{subfigure}{.8\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 3mm 0mm 0mm}, clip]{images/epg_distribution_step_10_psb2_6runs_barplot_v5.pdf}
  \caption{PSB2: 0 $\leq$ EPG $\leq$ 100 with step 10.}
  \label{fig:psb2-epg-distrib-step-10}
\end{subfigure}
% 
\begin{subfigure}{.8\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 3mm 0mm 0mm}, clip]{images/epg_distribution_step_1_humaneval_6runs_barplot_v5.pdf}
  \caption{HumanEval: 0 $\leq$ EPG $\leq$ 10 with step 1.}
  \label{fig:humaneval-epg-distrib-step-1}
\end{subfigure}
% 
\begin{subfigure}{.8\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 3mm 0mm 0mm}, clip]{images/epg_distribution_step_10_humaneval_6runs_barplot_v5.pdf}
  \caption{HumanEval: 0 $\leq$ EPG $\leq$ 100 with step 10.}
  \label{fig:humaneval-epg-distrib-step-10}
\end{subfigure}
\caption{Distribution (in \%) of the number of generated programs with GPT-3.5 and Llama 3 during each problem-solving attempt on average over 6 runs with different tree arities $\treearity_\text{draft}, \; \treearity_\text{debug}.$}
\label{fig:epg-distribution}
\end{figure}



In detail, we show the distribution of EPG values in all experiments to explore what proportion of candidate updates is made before a solution is found.
For example, on average, over 70\% of Python solutions by SEIDR with GPT-3.5 are solved from the first attempt, i.e., have EPG$=0$ (see Figure~\ref{fig:psb2-epg-distrib-step-1}).
Python solutions are more frequently found from the first attempt by SEIDR with Llama~3 than later in the tree, although less frequently than with GPT-3.5.
Most of solutions found by SEIDR benefit from the iterative repair and generate up to 10 extra programs with both models (see Figures~\ref{fig:psb2-epg-distrib-step-10}, \ref{fig:humaneval-epg-distrib-step-10}). 
However, some solutions are also found later in the tree search. 

The EPG distribution results for correctly solved problems with $TPR=1$ imply that the first steps in the update of the draft program are crucial for solving the problem. 
The chances of solving the problem in the later stages of the search are low.
This confirms our assumption in Section~\ref{sec:seidr-trade-off-settings} that 100 programs are sufficient in the generalizability experiments.

\begin{highlight}
\textbf{Repair-replace trade-off for SEIDR with GPT-3.5 and Llama~3 in the generalizability experiments (\rqtreearity{}, \rqllama{}, \rqmultirun{}):}
Unlike for SEIDR with Codex and a fixed debugging instruction (see Section~\ref{sec:seidr-seidr:rqtreearity}), SEIDR with GPT-3.5 and Llama~3 does not show any distinct trend for all the languages and datasets in terms of the preferred tree arity value. 
Results over a number of runs with the same hyperparameter settings are more condensed for Python than for C++, which can be a result of optimizing LLMs for high performance on popular coding benchmarks in Python.
If solutions to problems are found by SEIDR, it is done at an earlier tree search step for Python than for C++ (i.e., with a smaller EPG). 
The majority of solutions are found within the first 10 updates of program candidates. 
SEIDR solves 163 out of 164 HumanEval-C++ problems with GPT-3.5 at least once over all runs with all restarts and hyperparameter sets, and 162 --- with Llama~3.
\end{highlight}



\subsubsection{Parent Selection Strategies within the Ranking Agent in the Generalizability Experiments}
\label{sec:seidr-lexicase-results}

Based on the mean TPR over all runs reported in Figure~\ref{fig:mean-tpr-repair-replace-trade-off-generalizability}, we fix the best-performing $N^*$ for each dataset, language and model and run the experiments with lexicase selection instead of tournament selection as a parent selection strategy. 
The hyperparameters are shown in Table~\ref{tab:lexicase-selection-hyperparameters}.
We compare the number of problems solved with these settings and two types of parent selection algorithms in Figure~\ref{fig:num-solved-lexicase-selection} and mean TPR in Figure~\ref{fig:mean-tpr-lexicase-selection}.
Mean TPR with lexicase selection and the best tournament selection configuration are similar for two selection strategies for most of the experiments.
Similar results are obtained for the number of fully solved problems, with the exception of C++ results of SEIDR with GPT-3.5 on both datasets, where lexicase selection improved the results.

\begin{table}[t]
% \vspace{-2.6ex} 
\setlength{\tabcolsep}{4pt}
\centering
\caption{SEIDR generalizability experiments: hyperparameters in the lexicase selection experiments.}\small
\label{tab:lexicase-selection-hyperparameters}
%/footnotesize
\begin{tabular}{rcccc|cccc}
\toprule
experiment \# & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 \\
\midrule
datasets  & \multicolumn{4}{c|}{PSB2} & \multicolumn{4}{c}{HumanEval}  \\ 
\midrule
models  & 
\multicolumn{2}{c|}{GPT-3.5} &
\multicolumn{2}{c|}{Llama 3} &
\multicolumn{2}{c|}{GPT-3.5} &
\multicolumn{2}{c}{Llama 3}\\ 
\midrule
language  & C++ & Python & C++ & \multicolumn{1}{c|}{Python} & C++ & Python & C++ & Python \\
\midrule
population size (beam width), $\beamwidth{}$ & 2 & 16 & 16 & 16 & 2 & 4 & 10 & 10  \\
\# programs in the 1st generation, $\treearity_\text{draft}$ & 2 & 16 & 16 & 16 & 2 & 4 & 10 & 10  \\
\# bug explanations for candidate, $\treearity_\text{explain}$ & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 \\
\# repairs for each explanation, $\treearity_\text{debug}$ & 2 & 16 & 16 & 16 & 2 & 4 & 10 & 10 \\
\midrule
max programs generated & \multicolumn{8}{c}{100} \\
prompts & \multicolumn{8}{c}{see Section~\ref{sec:seidr-ollama-prompts}} \\
\# runs per experiment &  \multicolumn{8}{c}{6} \\
\bottomrule
\end{tabular}
\end{table}



To zoom in on the improvement details, we refer back to Figures~\ref{fig:epg-num-solved-psb2}--\ref{fig:epg-num-solved-he-c++}, where separate columns are dedicated to lexicase selection experiments (marked with ``lex.'').
We do not observe different programs solved with lexicase selection than with tournament selection. 
Some problems are solved in all six runs with lexicase selection and in fewer runs with tournament selection, such as find-pair and fizz-buzz in the C++ experiments with GPT-3.5.

To confirm the effect of lexicase selection on the program candidate search, we have explored the test pass rate dynamics.
An example run of SEIDR with GPT-3.5 on HumanEval is shown in Figure~\ref{fig:lexicase-tpr-jumps}.
We observe that in the vast majority of cases, the test score jumps from 0 to 1 directly, not as a result of reordering candidates, but as a result of SEIDR  bug summarization and candidate update.
Therefore, we appoint differences in results between lexicase and tournament parent selection primarily to the LLMs themselves and the debugging loop rather than to the parent selection strategy. 
Specifically, when an LLM gets the prompt that fixes exactly the error in a program candidate, the score jumps to 1 because of the correct prompt provided to the model more frequently than because a better candidate is chosen on a previous step.



% todo: see https://wandb.ai/codex-for-psb/seidr-telo-psb2-gpt-3.5-turbo-run2/runs/0akm8knh/logs



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Num solved problems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[bt]
\begin{subfigure}{.45\linewidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/num_solved_problem_lexicase_psb2_6runs_boxplot_v5.pdf}
  \caption{PSB2.}
    % \vspace{14pt}
  \label{fig:num-solved-lexicase-selection-psb2}
\end{subfigure}
\begin{subfigure}{.45\linewidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/num_solved_problem_lexicase_humaneval_6runs_boxplot_v5.pdf}
  \caption{HumanEval.}
  \label{fig:num-solved-lexicase-selection-he}
\end{subfigure}
\caption{Number of solved problems in lexicase and tournament selection experiments for SEIDR with GPT-3.5 and Llama 3 and fixed tree arity $N^*$.}
\label{fig:num-solved-lexicase-selection}
\end{figure}


\begin{figure}[bt]
\begin{subfigure}{.45\linewidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/mean_tpr_lexicase_psb2_6runs_boxplot_v5.pdf}
  \caption{PSB2.}
  \label{fig:mean-tpr-lexicase-selection-psb2}
\end{subfigure}
\begin{subfigure}{.45\linewidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/mean_tpr_lexicase_humaneval_6runs_boxplot_v5.pdf}
  \caption{HumanEval.}
  \label{fig:mean-tpr-lexicase-selection-he}
\end{subfigure}
\caption{Mean $TPR$ (measured in \%) in lexicase and tournament selection experiments for SEIDR with GPT-3.5 and Llama 3 and fixed tree arity $N^*.$}
\label{fig:mean-tpr-lexicase-selection}
\end{figure}

\begin{figure}[bt]
\centering
\includegraphics[width=\linewidth, trim={2mm 123mm 3mm 130mm}, clip]{images/best-avg-score-seidr-telo-humaneval-gpt-3.5-turbo-run3.pdf}
\caption{The best score on validation tests (y-axis) obtained up to an indicated logging step (x-axis) for problems solved from the second or later attempts with lexicase selection using SEIDR with GPT-3.5. Results are obtained for HumanEval. ``Step'' on the x-axis corresponds to logging settings and roughly indicates the speed of finding solutions to different problems. Each line stands for a unique solution.}
\label{fig:lexicase-tpr-jumps}
\end{figure}

\begin{highlight}
\textbf{Parent selection strategies for SEIDR with GPT-3.5 and Llama~3 in the generalizability experiments (\rqllama{}, \rqmultirun{}, \rqlexicase{}):} 
No leading ranking strategy is found for SEIDR experiments with GPT-3.5 and Llama~3 as measured by the average test pass rate and the average number of solved problems over six runs. 
In the vast majority of experiments where a solution is found and at least one debugging attempt is made the score jumps from 0 to 1 as opposed to climbing up incrementally and makes the ranking strategy less impactful than effective prompting. 
\end{highlight}


\begin{table}[t]
    \centering
    \caption{Average number of solved PSB2 problems using SEIDR with different LLMs and PushGP. The best results among SEIDR-only experiments at each $k$ are highlighted in bold. If pass@100 is not available, we cite the best available results in brackets. $N^*$ stands for the number of debug candidates and debug instructions to generate (tree arity).}\small
    \label{tab:generalizability-psb2}
\begin{tabular}{llllrrr}
\toprule
Language & Model in SEIDR & $N^*$ & Parent selection &  pass@1 &  pass@10 &  pass@100 \\
\midrule
% 
Python & GPT-3.5 & 1   &         tournament &    10.5 &     12.0 &      12.0 \\
    &        & 2   &         tournament &     9.3 &     11.5 &      12.0 \\
    &        & 4   &         tournament &     9.8 &     12.2 &      13.3 \\
    &        & 10  &         tournament &    10.7 &     \textbf{12.7} &      \textbf{14.5} \\
    &        & 16  &         tournament &    10.0 &     11.3 &      13.3 \\
    &        & 16  &           lexicase &    10.2 &     12.3 &      14.2 \\
    % &        & 100 &         tournament &    \textbf{10.8} &     12.2 &      13.7 \\
    &        & 100 &         tournament &    10.8 &     12.2 &      13.7 \\
\cline{3-7}\\[-8pt]
    &  \multicolumn{3}{l}{solved at least once by SEIDR with GPT-3.5} &  13 &       17 &       18 \\[1pt]
\cline{2-7}\\[-8pt]
        & Codex  & 10 & tournament &      5 &       10 &        14 (pass@1000=19) \\[1pt]
\cline{3-7}\\[-8pt]
       &  \multicolumn{3}{l}{solved at least once by SEIDR with Codex}   & 8 &       13 &        17 (pass@1000=20) \\[3pt]
\cline{2-7}\\[-8pt]
        &  PushGP (no SEIDR)   &    -            &       - &        - &       - &  (17)\\[1pt]
\cline{2-7}\\[-8pt]
% 
        & Llama~3 & 1   &         tournament &     1.2 &      1.3 &       2.3 \\
        &        & 2   &         tournament &     1.0 &      1.5 &       1.5 \\
        &        & 4   &         tournament &     1.0 &      1.5 &       2.3 \\
        &        & 10  &         tournament &     1.5 &      2.0 &       2.2 \\
        % &        & 16  &         tournament &     \textbf{1.7} &      \textbf{2.8} &       2.8 \\
        &        & 16  &         tournament &     1.7 &      \textbf{2.8} &       2.8 \\
        &        & 16  &           lexicase &     1.0 &      1.7 &       2.0 \\
        &        & 100 &         tournament &     1.0 &      1.0 &       \textbf{3.8} \\[1pt]
\cline{3-7}\\[-8pt]
    &  \multicolumn{3}{l}{solved at least once by SEIDR with Llama~3} &   4 &        5 &       11 \\
\midrule\\[-8pt]
 C++ & GPT-3.5 & 1   &         tournament &     0.0 &      5.6 &       5.5 \\
       &        & 2   &         tournament &     1.0 &      5.0 &       6.0 \\
       % &        & 2   &           lexicase &     \textbf{1.3} &     \textbf{ 9.0} &      \textbf{10.0} \\
       &        & 2   &           lexicase &     1.3 &     \textbf{ 9.0} &      \textbf{10.0} \\
       &        & 4   &         tournament &     1.0 &      4.6 &       6.8 \\
       &        & 10  &         tournament &     1.0 &      1.3 &       5.6 \\
       &        & 16  &         tournament &     1.0 &      1.2 &       8.3 \\
       &        & 100 &         tournament &     1.0 &      1.2 &       2.3 \\[1pt]
\cline{3-7}\\[-8pt]
       & \multicolumn{3}{l}{solved at least once by SEIDR with GPT-3.5}   & 2 &       13 &       13 \\[1pt]
% 
\cline{2-7}\\[-8pt]
   & Codex  & 10 & tournament &     3 &       12 &   14 (pass@1000=17)\\[1pt]
\cline{3-7}\\[-8pt]
       & \multicolumn{3}{l}{solved at least once by SEIDR with Codex}   & 10 &       12 &        15 (pass@1000=18) \\[1pt]
\cline{2-7}\\[-8pt]
   &  PushGP (no SEIDR) & -   &    -            &       - &        - &      (17)\\[1pt]
\cline{2-7}\\[-8pt]
        & Llama 3 & 1   &         tournament &     0.0 &      1.0 &       2.0 \\
       &        & 2   &         tournament &     0.0 &      \textbf{2.0} &       2.8 \\
       &        & 4   &         tournament &     1.0 &     \textbf{ 2.0} &       2.6 \\
       &        & 10  &         tournament &     1.0 &      1.4 &       \textbf{4.0} \\
       &        & 16  &         tournament &     0.0 &      1.8 &       3.8 \\
       &        & 16  &           lexicase &    1.0 &      1.4 &       3.5 \\
       &        & 100 &         tournament &     0.0 &      0.0 &       3.7 \\[1pt]
\cline{3-7}\\[-8pt]
       & \multicolumn{3}{l}{solved at least once by SEIDR with Llama~3}  & 4 &        5 &       11  \\
\bottomrule
\\ %% to make this a one
\\ %% page filling table
\end{tabular}
\end{table}


\begin{table}[t]
    \centering
    \caption{Percentage of solved tasks in HumanEval-X using SEIDR with different LLMs. The best results among SEIDR-only experiments at each $k$ are highlighted in bold. $N^*$ stands for the number of debug candidates and debug instructions to generate (tree arity).}\small
    \label{tab:generalizability-he}
\begin{tabular}{llllrrr}
\toprule
Language & Model in SEIDR & $N^*$ & Ranking &  pass@1 &  pass@10 &  pass@100 \\
\midrule
Python & GPT-3.5 & 1   &         tournament &    53.4 &     \textbf{60.4} &      63.3 \\
       &        & 2   &         tournament &    51.3 &     58.2 &      63.7 \\
       &        & 4   &         tournament &    52.1 &     58.4 &      64.8 \\
       &        & 4   &           lexicase &    52.5 &     59.6 &      \textbf{66.1} \\
       &        & 10  &         tournament &    51.1 &     58.3 &      61.7 \\
       &        & 16  &         tournament &    53.0 &     59.6 &      63.5 \\
       % &        & 100 &         tournament &    \textbf{54.0} &     56.7 &      62.5 \\[1pt]
       &        & 100 &         tournament &    54.0 &     56.7 &      62.5 \\[1pt]
\cline{3-7}\\[-8pt]
       & \multicolumn{3}{l}{solved at least once by SEIDR with GPT-3.5}   & 70.7 &     84.1 &      87.8 \\[1pt]
\cline{2-7}\\[-8pt]
    &   Llama 3 & 1   &         tournament &    22.0 &     44.0 &      47.1 \\
       &        & 2   &         tournament &    24.0 &     \textbf{48.5 }&      51.8 \\
       &        & 4   &         tournament &    22.5 &     43.1 &      51.8 \\
       &        & 10  &         tournament &    24.0 &     42.9 &      53.3 \\
       % &        & 10  &           lexicase &    \textbf{26.2} &     44.1 &      \textbf{54.1} \\
       &        & 10  &           lexicase &    26.2 &     44.1 &      \textbf{54.1} \\
       &        & 16  &         tournament &    23.5 &     41.6 &      51.5 \\
       &        & 100 &         tournament &    21.4 &     26.3 &      48.0  \\[1pt]
\cline{3-7}\\[-8pt]
       & \multicolumn{3}{l}{solved at least once by SEIDR with Llama~3} & 56.7 &     73.8 &      79.3 \\[1pt]
\cline{2-7}\\[-8pt]
& \multicolumn{6}{l}{\textbf{LLM results without SEIDR}} \\
 & GPT-3.5 (ChatGPT) & - &  - &  48.1  &  -   &    - \\
 & GPT-4 & - &  - & 67.0   &  -   &    - \\
 & Code Llama 34B & - &  - &  48.8  &  76.8   &    93.0 \\
 & Unnatural Code Llama 34B & - &  - &  62.2  &  85.2   &    95.4 \\
 & CodeGeeX & - &  - &  22.89  &  39.57   &    60.92 \\[-2pt]
\midrule\\[-10pt]
C++    & GPT-3.5 & 1   &         tournament &     3.4 &    \textbf{ 55.9} &      \textbf{78.5} \\
       % &        & 2   &         tournament &     \textbf{4.7} &     36.6 &      69.6 \\ 
       &        & 2   &         tournament &     4.7 &     36.6 &      69.6 \\
       &        & 2   &           lexicase &     4.3 &     35.9 &      73.6 \\
       &        & 4   &         tournament &     4.5 &     27.7 &      62.9 \\
       &        & 10  &         tournament &     3.6 &     13.7 &      49.7 \\
       &        & 16  &         tournament &     4.5 &     13.2 &      51.1 \\
       &        & 100 &         tournament &     3.6 &      7.1 &      31.3 \\[1pt]
\cline{3-7}\\[-8pt]
       & \multicolumn{3}{l}{solved at least once by SEIDR with GPT-3.5} & 20.1 &     89.0 &      99.4 \\[1pt]
\cline{2-7}\\[-8pt]
    &   Llama 3 & 1   &         tournament &    29.6 &     59.5 &      79.5 \\
    % &   Llama 3 & 1   &         tournament &    \textbf{29.6} &     59.5 &      79.5 \\
       &        & 2   &         tournament &    18.6 &     51.6 &      71.4 \\
       &        & 4   &         tournament &    20.0 &     50.6 &      73.9 \\
       &        & 10  &         tournament &    25.6 &     56.4 &      80.0 \\
       &        & 10  &           lexicase &    25.9 &     \textbf{60.9} &      \textbf{84.2} \\
       &        & 16  &         tournament &    20.9 &     53.0 &      75.5 \\
       &        & 100 &         tournament &    27.1 &     39.7 &      79.2 \\[1pt]
\cline{3-7}\\[-8pt]
       & \multicolumn{3}{l}{solved at least once by SEIDR with Llama~3} & 72.0 &     97.6 &      98.8  \\[1pt]
\cline{2-7}\\[-8pt]
& \multicolumn{6}{l}{\textbf{LLM results without SEIDR}} \\
& Code Llama 34B & - &  - &  47.8  &  -   &    - \\
& CodeGeeX & - &  - &  17.06  &  32.21   &    51.00 \\[-2pt]
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Overall Generalizability of SEIDR with parent selection strategies and repair-replace trade-off.}
\label{sec:seidr-overall-generalizability}

We combine all results in Table~\ref{tab:generalizability-psb2} for PSB2 and Table~\ref{tab:generalizability-he} for HumanEval-X, where we show the number of solved problems at different cutoffs ($k$).  
An average is taken over all runs for the experiments with multiple restarts, and the results for one run are shown for SEIDR with Codex. 

Note that at cutoff $k=1$, iterations of SEIDR are not taken into account, because the first program candidate is the one LLMs generate from a template, and the trees at cutoff $k=1$ have only one program in each of them, regardless of the parent selection strategy or tree arity. 
Here, the difference between pass@1 results with different parent selection and tree arity is explained by the stochastic behavior of the underlying LLMs. 
We keep pass@1 results to inform the reader about the stochasticity of the models and for the ease of visual comparison of values in each row.

We compare the performance of PSB2 solutions synthesized with SEIDR to the PushGP genetic programming system with down-sampled lexicase selection~\cite{helmuth2022:problemsolving}. 
To compare the performance of SEIDR with other LLMs without SEIDR, we report the average percentage of solved problems for HumanEval-X. 
For HumanEval-X, we cite the performance of the state-of-the-art models as reported by their authors without SEIDR, such as CodeGeex~\cite{zheng2023:codegeex}, GPT-3.5 and GPT-4~\cite{openai2023:gpt4}, for reference, and compare SEIDR with GPT-3.5 and Code Llama~\cite{roziere2023:code} results at different cutoffs ($k$ for $pass@k$).
Furthermore, we report the number of solved problems in the union of all experiments for a given dataset, model, and language, because different problems are solved in different restarts of the method. 
We refer to these results as ``solved at least once'' hereafter.

In PSB2-Python experiments, SEIDR with Codex has a higher pass@1000 than the maximum pass@100 measured for SEIDR with GPT-3.5. SEIDR with Codex also solves 20 problems at least once, whereas SEIDR with GPT-3.5 solves 18.
SEIDR with Codex also outperforms other LLMs on PSB2 in C++.
No parent selection strategy or tree arity $N^*$ is leading across all PSB2 experiments. 

In the HumanEval-C++ experiments, SEIDR performs well at larger $k,$ i.e., when debugging steps are made. 
Remarkably, the union of SEIDR experiments with both GPT-3.5 and a much smaller Llama~3 solve 163 problems (or 99.4\% in Table~\ref{tab:generalizability-he}) and 162 problems (or 98.8\%) in HumanEval-C++, correspondingly.
SEIDR does not outperform other LLMs on HumanEval-Python, which, as mentioned earlier, could be the effect of HumanEval-Python being a popular benchmark for testing LLMs and indirectly optimizing the models.

Direct comparison of SEIDR results with other iterative program synthesis frameworks is challenging because, in addition to differences in benchmarks used, \cite{jiangSelfEvolveCodeEvolution2023} and \cite{chenTeachingLargeLanguage2023} do not always report the number of generated programs, which is the primary metric that we use to compare iteration against a replace-only baseline.\footnote{\cite{chenTeachingLargeLanguage2023} does have a sentence "Note that typically one debugging turn is sufficient"}


\begin{highlight}
\textbf{Summary of SEIDR results:} 
SEIDR with Codex benefits from the repair-replace trade-off when building a search tree of PSB2 solutions. The best results on PSB2 are achieved with Codex, tree arity of 10 and a maximum of 1000 programs generated. A trend of having the best tree arity set to 10 does not fully hold for other tested models, GPT-3.5 and Llama~3, where different tree arities performed better in different settings, e.g., depending on the parent selection algorithm, dataset, and programming language. 
Due to increasing costs of newer models time required for testing, we have run GPT-3.5 and Llama~3 six times with each set of hyperparameters and stopped building the search tree at 100 programs. 
In HumanEval-C++, the union of these runs has solved 163 problems with GPT-3.5, 162 problems with a much smaller Llama~3-8B model.
SEIDR runs have also solved 18 PSB2 problems in C++ and 20 in Python with Codex at least once in all the experiments. 
The numbers for SEIDR with GPT-3.5 are lower than for Codex, possibly due to the focus of GPT-3.5 on general reasoning and of Codex --- on coding.
A smaller Llama~3 model performs poorly on PSB2, which we appoint to the difficulty of the benchmark and the popularity of HumanEval and to the fact that the models can be indirectly optimized for higher performance on HumanEval than on other programming benchmarks.
\end{highlight}


\subsection{Threats to Validity}
\label{sec:seidr-threats}

External threats to validity concern SEIDR performance with benchmarks and language models different from those tested. 
Specifically, PSB2 and HumanEval-X contain programming tasks that require smaller functions to be generated than production-scale software.
Although some canonical solutions in HumanEval-Python have been criticised~\cite{liuYourCodeGenerated2023}, we primarily use unit tests to evaluate the output of SEIDR and do not compute the exact match. Therefore, these weaknesses do not impact our results.
% We plan to extend our experiments in future work to explore the generalizability of results to more complex benchmarks.

Internal threats relate to the implementation.
We use PSB2, which has both corner cases and regular tests available. 
To ensure a fair comparison with other studies on PSB2, we evaluate and report results on the provided test set of PSB2, of which we randomly pick 2000 tests. 
A risk that a program does not pass tests other than the ones picked persists but is assumed to be low given the large enough number of 2000 tests. 

The large language models for code editing and text completion used in this study are non-deterministic, 
which can affect the results. 
Due to prohibitive model inference costs and discontinuation of support for earlier models, each experiment with Codex and GPT-3 is run only once. 
Experiments with Llama~3 and GPT-3.5 are restarted six times. 
We acknowledge that more restarts can provide a more complete picture of the approach performance,
although this is not the standard practice in the LLM domain~\cite{ouyang2023:llm} 
and can come at considerable financial and environmental costs.
Our temperature-based sampling procedure described in Section\ref{sec:seidr-synth} reduces this stochasticity significantly, especially for low-EPG results, because earlier solutions in the search tree are obtained with lower temperatures, and lower temperature limits the stochasticity.
Codex, GPT-3, and GPT-3.5 are black-box models and can generate malicious code~\cite{pearceAsleepKeyboardAssessing2022}. 
Therefore, when coding in real life, the LLM output should be carefully reviewed before running it.
% \ag{The Codex model was pre-trained on an unbalanced dataset across programming languages~\cite{chen2021:evaluating}. Thus, 

The results can be skewed towards high performance in the programming languages prevailing in the pre-training dataset used by the authors of the tested LLMs -- an issue also known as \emph{data contamination}.
However, if the models were able to directly reproduce the solutions seen in the training data, 
these problems would have been solved from the first attempt.
Since we observe $EPG>1$ values, data contamination is likely to have only a partial effect on the results.
Moreover, the results can be affected by popular evaluation benchmarks: even though the benchmarks themselves are usually not parts of the pre-training dataset, the published LLMs are likely to be optimized for high performance on these benchmarks.   

\section{Conclusion}
\label{sec:seidr-conclusion}

In this study, we propose SEIDR, a multi-agent framework to solve the challenge of fully autonomous programming. 
In SEIDR, the program synthesis procedure is augmented from the direct generation of code with large language models instructions to iterative calls to a DEBUG agent followed by the RANK agent. 
The DEBUG agent performs a tree search across program candidates generated by a large language model for code.
The LLM used for code repair takes imperfect program candidates and instructions for their improvement as prompts. 
The instructions are obtained from both static templates with failing test case descriptions and templates with auto-generated bug summaries by a text completion language model. 

In addition to the initial exploration of hyperparameters that influence the population size at each generation and the number of children for each parent in SEIDR iterations, we extend the framework to test two different ranking strategies (tournament selection and lexicase selection), three models in the coding part of SEIDR (Codex, GPT-3.5 and Llama~3), two datasets (PSB2 and HumanEval-X), two programming languages (Python and C++), and various branching factors. 
With the update to newer models, the prompts and parameters of the tree search are updated accordingly. 

\paragraph{Contributions}
We run one set of initial exploration experiments and two sets of generalizability experiments. 
In the initial exploration, we test SEIDR with Codex-edit as the model for draft program synthesis and debugging in Python and C++ on the PSB2 benchmark. 
In our experiments, SEIDR outperforms the PushGP baseline and achieves the state-of-the-art result with 19 solved problems out of 25. 
It requires under 1000 program executions to solve them, in stark contrast to billions\footnote{~A problem is considered ``solved'' by PushGP if at least 1 of 100 runs, each with a limit of 60 million programs, was successful.} of executions in PushGP, making it feasible in the areas with costly testing, such as robotics.
Investigation of the repair-replace trade-off shows that SEIDR with tree arity of 10 outperforms both the replace-only strategy and the repair-only approach. 
% Our prompt engineering study shows that bug summaries generated with ``confidence indicators,'' such as ``obviously'' improve the performance of SEIDR during C++ code synthesis. 
% Overall, our framework shows low performance variability with different prompts, which indicates its robustness.%

To study the generalizability of SEIDR, we experiment with GPT-3.5 and Llama 3-8B as the models for draft program synthesis, explaining errors in synthesized programs and their debugging in Python and C++ on the PSB2 and HumanEval-X benchmarks. 
Our experiments with lexicase and tournament parent selection in the RANK agent do not show consistent improvement with either policy or any tree arity. 
One observation is that program candidates in generations prior to the final solution do not pass any test in the test suite in the vast majority of cases. 
The test pass rate for the final solution abruptly increases in one generation from 0 to all tests passed. 

In our generalizability experiments, SEIDR shows better performance than using LLMs without SEIDR with the same prompts and the same budget in terms of programs to generate.
The method achieves high results of 78.5\% average pass@100 on HumanEval-C++ with GPT-3.5 and 84.2\% with Llama~3.
Remarkably, in HumanEval-C++, the union of SEIDR restarts with different hyperparameters has solved 163 problems with GPT-3.5, 162 problems with a much smaller Llama~3-8B model.
The union of SEIDR runs has solved 18 PSB2 problems in C++ and 20 in Python with Codex. 
% Moreover, the method achieves the state-of-the-art result for pass@10 (74.44\%) with Code Llama 34B and for pass@100 (93.29\%) with GPT-3.5 on HumanEval-C++. 
% The approach requires under 100 program executions to obtain the reported results with GPT-3.5 and Code Llama.
% SEIDR with moderate number of generated programs for each bug explanation is favorable both for obtaining a solution and for the speed of solving a task. 

\paragraph{Future work}
Further investigation of SEIDR generalizability and ranking strategies are some of the areas for future work. 
Benchmarks with more tests than in HumanEval-X may shed more light on the most effective choice of the number of programs generated from each bug explanation, as well as the framework's comparison on large-context projects. 
As an agent-based framework, SEIDR shows how LLM-based agents and other non-LLM agents or components can collaboratively interact and solve software engineering tasks.

\section{Data Availability}
\todo{Replace with citeself}

To support open science and allow for replication and verification of our work, a replication package containing the code and results is publicly available in Zenodo.\footnote{~Zenodo DOI: \href{https://doi.org/10.5281/zenodo.13754705}{10.5281/zenodo.13754705}.
}
% \footnote{~While the manuscript is under review, the replication package is made available as a shared Google Drive to allow for improvements \url{https://drive.google.com/drive/folders/1SP0CVJQ6CdktFtksVs7tYu9jdLHIjHxa?usp=sharing}. We have reserved Zenodo DOI \href{https://doi.org/10.5281/zenodo.13754705}{10.5281/zenodo.13754705} to upload it after the review process has concluded.
% }

