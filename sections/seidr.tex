\chapter{Multiagent Programming with LLMs}\label{ch:seidr}
\newcommand{\smalltt}[1]{\texttt{\small#1}}

\section{Introduction}
\label{sec:seidr-intro}



Given these challenges, research in machine learning on source code~\cite{allamanis2018:survey} tends to focus on restricted domain-specific languages~\cite{chen2021:latent,flashmeta,liventsev2021:bf} or automating specific parts\footnote{~similarly to autonomous driving~\cite{grigorescu2020:survey,marcano2020:review}} of the software development process~\cite{lu2021:codexglue,niu2023:crosscodebench} such as code search~\cite{husain2020:codesearchnet}, code translation~\cite{roziere2020:unsupervised}, detection of issues~\cite{fernandes2016:reviewbased,chakraborty2021:deep}, improvement~\cite{petke2018:genetic} and repair~\cite{gouesAutomatedProgramRepair2019} rather than fully autonomous programming in a programming language popular with human developers~\cite{:tiobe}.
However, two recent innovations potentially make the latter task tractable.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth,trim={0mm 8mm 0mm 0mm}]{codex-for-psb-seidr-methodology-4.drawio.pdf}
    \caption{Overview of Synthesize, Execute, Instruct, Debug, and Rank}
    \label{fig:method}
\end{figure}

One is \emph{Synthesize, Execute, Debug}~\cite{guptaSynthesizeExecuteDebug2020}, a framework that attempts to bridge the "last mile" gap by introducing program repair into the program synthesis algorithm. 
A programming task is specified using both a natural language description and a set of input/output (I/O) pairs demonstrating what output is expected of the program, thereby combining text to code ~\cite{iyer2018:mapping} and programming by example~\cite{halbertProgrammingExample1984,gulwani2016:programming} paradigms typical for competitive programming~\cite{zavershynskyi2018:naps}.
\emph{Synthesize, Execute, Debug} creates a first draft program using a generative model, compiles and executes it with given input examples.
This is followed by a program repair step to fix the identified errors.

Another relevant innovation is instruction fine-tuned large language models~\cite{ouyang2022:training}. Instruction fine-tuned models use human feedback in their training process and are designed to explicitly or implicitly admit two inputs: a source text (or code) and a textual command instructing the model to edit the source in a particular way, i.e., "summarize" or "translate to Python".
These models have been shown to be highly successful in automatic program repair~\cite{fanAutomatedRepairPrograms2023}. 
However, given the free-form nature of these instructions\footnote{Throughout this chapter we avoid other definitions of \emph{instruction}, such as \emph{an individual operation in code}, to prevent ambiguity.} how one should engineer instructions that maximize repair performance is an open question. 

Section~\ref{sec:seidr-methodology} presents a framework that adapts \emph{Synthesize, Execute, Debug} to instruction fine-tuned Large Language Models in agents for solving programming tasks in an autonomous fashion. 
We discuss related work in Section~\ref{sec:related-work}, introduce experiments to establish optimal search and prompting strategies for this framework in Section~\ref{sec:eval}. 
Finally, we demonstrate in Section~\ref{sec:seidr-results} that our framework outperforms conventional automatic programming techniques, such as genetic programming and naive application of large language models that generate one solution per problem without updating it iteratively. 

\newpage
\section{Methodology}
\label{sec:seidr-methodology}
The proposed 5-agent SEIDR framework  is summarized in Figure~\ref{fig:method}, which we discuss in detail in Section~\ref{sec:seidr-ingredients}.
To solve a programming task defined as a text description and a collection of I/O examples, we split I/O examples into prompt and validation sets and use the prompt set in a large language model to \synthesize{} a population of candidate solutions.
We \execute{} the solutions, test them against the validation set, generate a text description of the identified problems used to \instruct{} a large language model to produce repaired candidate solutions similar to the way a human developer \debug{}s a program.
We \rank{} the candidates
by correctness measured by matching I/O pairs, discard the worst candidates, and repeat until a fully correct solution is found.

\begin{figure}[H]
    \centering
    % \includegraphics[width=\linewidth,trim={0mm 0mm 0mm 0mm}]{img/methodology/codex-for-psb-seidr-methodology-4.drawio.pdf}
    \includegraphics[width=\linewidth,trim={0mm 0mm 0mm 0mm}]{images/codex-for-psb-seidr-methodology-5.drawio.pdf}
    \caption{Overview of the SEIDR framework, an LLM-based Synthesize, Execute, Instruct, Debug, and Rank approach.}
    \label{fig:method}
\end{figure}
 

\newpage \subsection{Ingredients}
\label{sec:seidr-ingredients}

SEIDR makes use of instruction fine-tuned large language models: a \emph{synthesis} model \synthmodel{}, a \emph{debugging} model \debugmodel{}, as well as a model \textmodel{} that can be used for writing textual instructions, which are forwarded to the code generation model \debugmodel{} for code updates. 
Therefore, the setup can be described as two agents communicating with each other, whereby one generates code and another one provides critical or supervising comments on what should be changed in the generated code. 

The models \synthmodel{}, \debugmodel{}, and \textmodel{} can be either separate models or the same model.
The pre-requisites are that \synthmodel{} and \debugmodel{} models are able to understand natural language (descr) and partial or full programs (code) and generate code based on them. 
The model \textmodel{} should be able to understand code and natural language and either auto-complete or generate the debugging instruction from scratch. 
Note that the debugging instructions can be generated from failing tests using static templates 
%as described in~\cite{liventsev2023:fully}. 
However, we focus on the implementation in which an LLM generates debug instructions, which is also suitable for chat models in addition to instruction fine-tuned models. 
In general, SEIDR requires a sequence-to-sequence generative model for these agents. 
We have chosen the state-of-the-art transformer models~\cite{vaswaniAttentionAllYou2023} for \synthmodel{}, \debugmodel{}, and \textmodel{} in our experiments as described in Section~\ref{sec:seidr-models}. 

Each LLM is a highly parameterised probability distribution over the space of (code, description)-tuples with parameters estimated on a large diverse (i.e., non-task-specific) corpus.
This stochastic nature of language models is an important prerequisite for SEIDR, since it lets us sample batches of diverse candidate solutions from \synthmodel{}, \debugmodel{}, and \textmodel{}. 
We denote the number of generated outputs with $\treearitydraft{},$ $\treearitydebug{},$ and $\treearityexplain{},$ correspondingly.
Moreover, each model generates the most probable and less probable outputs in each batch, which helps diversify problem solving attempts. 
In the implementation-related Sections further, we explain how we vary with the number of candidate solutions, debug instructions, and repairs generated in a batch by each LLM in SEIDR.


\paragraph{Synthesize}
\label{sec:seidr-synth}


The framework starts with the \synthesize{} agent, which is responsible for generating initial draft solutions to programming tasks to be repaired in the later stages of SEIDR.
We start with a basic template for a chosen programming language that contains a number of standard library imports as shown in Figure~\ref{fig:template}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth, trim={0mm 40mm 0mm 0mm}, clip]{images/Templates-new-v2.pdf}
    \caption{Anatomy of \synthesize{} templates}
    \label{fig:template}
\end{figure}

We populate this template with a comment indicating a text description of a task at hand and several I/O examples from the prompt training set.
We design the templates with prompt engineering guidelines \cite{PromptEngineering}$^{,}$ and prior work (chapter \ref{ch:tree2tree}) in mind.
We then sample $\treearitydraft{}$ programs from \synthmodel{}, setting \texttt{code} to the populated template and \texttt{description} to the natural language description of what the model should generate.
We use temperature sampling with a monotonically increasing temperature schedule where $i$-th program is sampled with temperature $t_i \approx \frac{i}{\treearitydraft{}}$ (approximate equality enables efficient implementation by means of batching, i.e., generating several inputs with the same input pair of (code, descr)).
Thus, the sampling procedure for the first programs approximates deterministic maximum likelihood estimation.
Ultimately, this approach ensures that samples are diverse, but always contain the likeliest programs.

\paragraph{Execute}
\label{sec:seidr-execute}

The \execute{} agent compiles the programs (if necessary) and launches them using the standard tools for the programming language.
The program is run once for every I/O pair in the validation set. 
Its \texttt{stdin} stream receives all the input lines in a given input pair, and its \texttt{stdout} and \texttt{stderr} streams are captured and saved.
We then measure the \emph{score} of the program defined as accuracy over output lines, with \expectedoutput{} being the expected output, and $n=\max\{|\expectedoutput{}|, |\text{stdout}|\}$:
\[    
\text{score}(\expectedoutput{}, \text{stdout}) = \frac{\sum^{n}_i{\mathbb{I}[\text{stdout}_i = O_i]}}{n} 
\]
unless \texttt{stderr} is non-empty during compilation or execution, which is considered to indicate failure and is assigned a score of 0.

\paragraph{Instruct}
\label{sec:seidr-instruct}

The goal of the \instruct{} agent is to provide instructions that summarize bugs in a program candidate and suggest a solution for \debugmodel{}. 
The resulting instruction(s) with the bug summary should indicate what requirement is violated and instruct the LLM to edit the candidate program accordingly. 
In SEIDR, we generate instructions using template engines. 
In general, template engines replace placeholders in files or strings with input values and return a formatted string. 
With template engines, we can create templates that will be adapted dynamically based on the results of program candidate execution. 
In recent chat and instruction-based LLMs, the terms \emph{template engine} and \emph{prompt template} are used interchangeably.

We consider two different designs of the \instruct{} agent: \instructs{} and \instructllm{} shown in Figure~\ref{fig:method-instruct}. 
Both types use failing I/O pairs from the validation set and \texttt{stderr} output of the candidate execution. 
In both cases, if \texttt{stderr} is not empty, i.e., execution exits with code 0 before getting any output to compare it with the expected output, the \texttt{stderr}-based Template engine generates the instruction to fix the error. 
However, the designs differ in the way they transform failing I/O pairs to generate instructions in case \texttt{stderr} is empty.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth,trim={0mm 0mm 0mm 0mm}]{images/codex-for-psb-seidr-instruct-2.drawio.pdf}
    \caption{\instruct{} agent.}
    \label{fig:method-instruct}
    % \vspace*{-3ex}
\end{figure}

\instructs{} uses a fixed template and substitutes placeholders for input and output with the corresponding strings of the first failing test case in its template engine.
For example, we show the resulting instruction for an exemplar template in Figure~\ref{fig:method-instruct}.
By contrast, \instructllm{} uses the failing I/O pair in the LLM for text completion, thereby prompting the text LLM to produce the bug explanation and a summary for debugging. 
In addition to providing a failing test case or \texttt{stderr}, one may choose to give the model \textmodel{} more context, such as the problem name, task description and the code generated so far. 
Each call to \textmodel{} can result in $\treearityexplain{}\ge1$ instructions, a batch output of an LLM.
Prompt templates used for experiments are detailed in Section~\ref{sec:seidr-prompts}.

% An exemplar output of the code behavior template engine in Figure~\ref{fig:method-instruct} describes that the code returns output O instead of expected output O$_{\text{val}}$ for the failing test case with input string I$_{\text{val}}.$
% The LLM is then prompted to auto-complete this description of program behavior with the bug summary. 
% The bug summary is passed further to the next template engine that uses it as debugging instruction, such as ``\emph{Fix \{bug summary\}}''.







\paragraph{Debug}

The main component of SEIDR which addresses the ``near miss syndrome'' is the \debug{} agent.  
This agent iterates over all programs in the population to repair candidate programs and pass more tests. 
It uses the instructions written by \instruct{} to sample from \debugmodel{} model $\treearitydebug{}$ times
to repair every candidate and create a new population of \treearity{} candidates.
For \debugmodel{}, the parameter \texttt{code} is set to the current version of the candidate solution and \texttt{descr} to the output of \instruct{} and any additional context chosen for a specific implementation.
The current generation of candidates is then replaced with \treearity{} outputs of \debug{}.

\paragraph{Rank}

The \rank{} agent implements what is known in genetic programming as \emph{parent selection}~\cite{koza1994:genetic}: it selects the best $\beamwidth{}$ programs to be further improved by the \debug{} agent.
We consider two different parent selection algorithms: simple ranking and lexicase selection. 
See Section~\ref{sec:seidr-lexicase-results} for their empirical comparison.

\emph{Simple ranking} sorts the programs by their average test score and selects top $\beamwidth{}$ candidates. 
Such ranking approach is a \emph{quality-based} selection method.
The programs are selected based on the intuition that repair of the best so far (but imperfect) programs begets good programs.

\emph{Lexicase selection}~\cite{helmuth2015:solving} is an approach from the \emph{quality diversity} family that maximizes diversity of selected candidates in addition to their score.
Lexicase selection ensures diversity by keeping the program candidates which perform the best on unique tests as opposed to the program candidates that perform best on average on all tests but, possibly, do not perform perfectly on any unique test.
The algorithm is as follows:
\begin{enumerate}

\setlength{\parskip}{0pt}
\setlength\itemsep{0pt}

    \item randomly shuffle the set of tests;
    \item select a program with the best score on test 1;
    \item if several programs are tied, resolve the tie by selecting the best program on test 2;
    \item repeat for tests $3,4,\dots,$ until only one program is left;
    \item mark this program as "selected";
    \item if less than $\beamwidth$ programs are selected, go back to step 1.
\end{enumerate}
This ensures that even if the average quality of selected candidates is lower, the batch of $\beamwidth{}$ programs collectively contains a higher number of requisite "skills", as measured by tests.



\newpage \subsection{Meaning of Hyperparameters}
\label{sec:seidr-beam-search}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth, trim={0mm 4mm 0mm 0mm}]{images/beamsearch.pdf}
    \caption{Repair-replace trade-off as a tree search problem.}
    \label{fig:beam-search}
    % \vspace*{-4ex}
\end{figure}

After evaluating a given candidate solution in \execute{}, SEIDR supports two approaches to addressing the candidate's flaws:
\begin{itemize}
\setlength{\parskip}{0pt}
\setlength\itemsep{0pt}
  \item \emph{Replace} the candidate with another sample from the current population.
  \item Use \instruct{} and \debug{} to repair the candidate.
\end{itemize}
We refer to this problem as the \emph{repair-replace trade-off}, by analogy with production economics~\cite{jack2000:optimal}. 

How does the choice of hyperparameters $\treearity{},$ the total number of candidate programs in each generation, and $\beamwidth{},$ the number of selected repairs to be preserved in a generation, influence the flow of SEIDR?
$\treearity$ and $\beamwidth{}$ act as upper bounds on the \emph{replace} option by limiting the size of the population.
In the edge cases, $\treearity{} = \beamwidth{} = 1$ corresponds to a repair-only process, while $\treearity{} = \beamwidth{} = \infty$ corresponds to replace-only, as illustrated in Figure~\ref{fig:beam-search}. 
Note that $\treearity{}$ is defined by $\treearitydraft{}$ for the initial draft solutions in the first generation and $\treearityexplain{} \cdot \treearitydebug{}$ for later generations. 

Observe that a mutation-only genetic algorithm with population size $\beamwidth{},$ such as SEIDR, is equivalent to \emph{local beam search} with beam width $\beamwidth{}$ on an $\treearity{}$-ary tree ~\cite[Section 4.1.4]{russell2010:artificial}. This corresponds to a known property of local beam search: it degenerates into depth-first search when $\beamwidth{} = 1$, whereas setting $\beamwidth{} = \infty$ yields breadth-first search.

% Hence, we refer to $\treearity{}$ as \emph{tree arity} and $\beamwidth{}$ as \emph{beam width}.

\newpage
\section{Related Work}
\label{sec:seidr-related-work}

Until recently, the tasks of program synthesis \cite{gulwaniProgramSynthesis2017} and program repair \cite{gouesAutomatedProgramRepair2019, petke2018:genetic} have been considered separately.
However, a number of important studies bridging this gap in the application of Large Language Models have been carried out concurrently with this work, listed below.

\cite{fanAutomatedRepairPrograms2023} incorporate a program repair step into a program synthesis pipeline where the initial synthesis is done by the Codex model \cite{chenEvaluatingLargeLanguage2021} and for program repair both Codex and state of the art specialized repair tools such as TBar and Recoder \cite{Defects4JDatabaseExisting} are considered and compared. \cite{zhangSelfEditFaultAwareCode2023} do the same, but fine-tune PyCodeGPT-110M \cite{zanCERTContinualPretraining2022} to use it as a repair model. The resulting framework is a two-step process (1 draft step and 1 debug step), while iterative evolution and search are not explored. 

Evolution through Large Models (ELM) \cite{lehmanEvolutionLargeModels2022} proposes using a language model in place of a mutation operator within a traditional genetic programming framework \cite{koza1994:genetic}. They use an type of instruction fine-tuned model trained on git commit messages known as a diff model \cite{DiffModelsNew2023}. However, the model is not directed to solve the programming problem at hand nor to fix bugs, instead, it is provided with generic instructions such as "Change function f". This approach is meant for fields where creative open-endedness~\cite{stanleyWhyGreatnessCannot2015a}, rather then satisfying concrete requirements, is encouraged.

\cite{dongSelfcollaborationCodeGeneration2023} are guided by the metaphor of a multi-agent conversation: their program synthesis loop begins with an analyst LLM generating a plan in natural language, a coder LLM executing it and a tester LLM giving feedback. They implement a repair-only approach and their \execute{} agent is a language model that predicts the output of a program without compilation and/or execution - an instance of chain-of-thought prompting \cite{yuBetterChainofThoughtPrompting2023} for program synthesis.

\cite{jiangSelfEvolveCodeEvolution2023} is a repair-only version of SEIDR that demonstrates the benefits of LLMs evolving not just the source code, but an additional natural language text file that acts as the system's knowledge base, to be included in the model prompt when generating code.

\cite{xiaConversationalAutomatedProgram2023,chenTeachingLargeLanguage2023,shinnReflexionLanguageAgents2023} explore an iterative approach to program synthesis not unlike SEIDR. However, they do not explore the repair-replace tradeoff and exclusively implement the repair-only approach that is prone to local minima.

Finally, Self-Taught Optimizer (STOP) \cite{zelikmanSelfTaughtOptimizerSTOP2023} takes the concept of self-improvement to the next (delightfully meta) level and uses a large language model to edit the evolutionary algorithm itself (i.e. fig. \ref{fig:method}). See their section 8 for reflections on AI Safety.

\newpage
\section{Experimental Setup}
\label{sec:seidr-eval}

To explore the capabilities of SEIDR and its generalizability  we test the framework on two benchmarks  with varied search strategies and use two programming languages. 
The problems in the benchmarks originate from coding competitions and human-written programming assignments. 
During our empirical evaluation of SEIDR performance, we address the following research questions:
\head{RQ1. Repair-replace trade-off exploration} 
What is the impact of using different tree search strategies
in the autonomous programming setting? 
We experiment with six different tree arities for the quality-based ranking and study their impact on the number of resolved problems as well as the speed of obtaining solutions.
\head{RQ2. Prompt engineering} What is the effect of using LLM-produced bug summaries compared to static instructions on the repair of automatically synthesized code? We test six static debug instructions that describe bug behavior based on violated requirements and five debug prompts. 
\head{RQ3. Quality-diversity vs. average quality-first based ranking} 
How does the quality-diversity ranking strategy impact the performance in comparison to average quality-first ranking strategy? 
We run experiments with lexicase selection as the ranking strategy and four non-corner case tree arities.

\head{RQ4. Generalizability of the approach to different LLMs} 
How does the choice of an LLM affect the performance of SEIDR? 
We choose the best-performing combination of hyperparameters, i.e., a tree arity and a ranking strategy, for experiments with other LLMs and compare the results with previous work obtained without SEIDR. 

\newpage \subsection{Data}
\label{sec:seidr-data}

Our experiments use the Program Synthesis Benchmark~2 (PSB2)~\cite{helmuth2022:applying} and HumanEval-X~\cite{zheng2023:codegeex} in C++ and Python. 
The key criteria for the benchmarks choice are availability task descriptions in English and unit tests in Python and C++ or language-agnostic unit tests. 
We focus on the widely used benchmarks in the areas of generative LLMs and genetic programming. 

\paragraph{PSB2}
The first dataset is a benchmark suite of 25 problems for program synthesis that resemble small real-world tasks. PSB2 was developed as a more realistic and challenging version of PSB1~\cite{helmuth2015:general}, the latter consisting of textbook problems and is widely used in genetic programming~\cite{sobania2022:choose}. 
The problems require different data structures and control flows to be used for effective solutions and are taken from sources, such as competitive programming platforms and educational courses. 
The problems have descriptions in English, as well as 1 million~(M) tests for training and 1M testing-stage tests, including edge or corner cases that test the resulting program on complicated inputs. 
The tests are provided as I/O pairs and are distributed together with the problem descriptions as a PyPI package.\footnote{~\url{https://pypi.org/project/psb2/}} 

In PSB1, the training set consists of the edge test cases and is augmented by random test cases if the number of edge tests is not enough. The test set is formed by random test cases. 
This terminology is preserved in PSB2. 
We use the PSB2 training set for ranking and selection of programs (validation) within an experiment and the test set for reporting the result thereof (testing).
We will thus refer to the PSB2 training set as the \emph{validation set}, to be more consistent with how it's used in SEIDR.

\paragraph{HumanEval-X}
The second dataset we use is a development from the original set of human-written programming tasks in HumanEval~\cite{chen2021:evaluating}, which a standard code generation benchmark for LLMs.
HumanEval consists of 164 problems with a docstring representing problem description, a function signature, a correct solution and unit tests in Python. 
HumanEval-X is a result of translation of correct programs and unit tests to five programming languages. 
We use HumanEval-Python for experiments in Python to ensure comparison with other models in the setup without SEIDR. 
In addition, we test SEIDR on the HumanEval-C++ part of HumanEval-X to compare with results obtained on PSB2. %here and in an earlier SEIDR study~\cite{liventsev2023:fully}. 

The test functions of HumanEval-X contain all tests in one function. We split the aggregated test functions into separate tests so that the \rank{} agent can evaluate the \text{score}. 
On average, the number of tests in HumanEval-Python is 7.25 and 6.95 in HumanEval-C++, which is appointed to a repeated additional test present in some HumanEval-Python examples of the following type: \texttt{assert True, "This prints if this assert fails 1 (good for debugging!)"}.
This test type is not present in HumanEval-C++.
To reiterate, we keep the original HumanEval-Python setup for direct comparison with models tested on this benchmark without SEIDR. 
Because of the limited number of tests, we pass up to five tests to the draft prompt and make all tests visible to SEIDR for the debugging loop. 
In other words, we do not have a held-out test split for HumanEval-X in the same manner as we have it for PSB2.


\newpage \subsection{Models}
\label{sec:seidr-models}

SEIDR uses up to three LLMs, \synthmodel{}, \textmodel{}, and 
% \\\debugmodel{}
$p_\text{debug}(\text{code},$ $\text{descr})$
in \synthesize{}, \instruct{} (if \instructllm{} is activated), and \debug{}, correspondingly. 
These models can be instantiated with the same LLM or different ones. 
The main pre-requisite is that \synthmodel{} and \debugmodel{} are a text-to-code models which take both a textual description and a draft code as input.
Therefore, \synthmodel{} and \debugmodel{} can be a chat model, a code completion model, an instruction fine-tuned, or a foundation generative language model pre-trained on code in addition to text. 
By analogy, the text-to-text \textmodel{} can be a chat, instruction fine-tuned, a text completion, or text generation model pre-trained on text and code.
In our experiments, all three models are instantiated with the same fixed model. 

In our experiments, we use Open AI Generative Pre-trained Transformer (GPT) models by Open AI and Code Llama by Meta~\cite{roziere2023:code}. 
GPT models are auto-regressive transformer models that have the decoder-only architecture as opposed to the original full encoder-decoder transformer.
They are pre-trained on both text and code and excel at sequence-to-sequence generative tasks, including code-to-code, text-to-code, and code-to-text.

Namely, we use Codex-edit (code-davinci-edit-001)\footnote{~\url{https://platform.openai.com/docs/guides/code/editing-code}} as the LLM for writing and debugging programs and GPT-3 (text-davinci-003)\footnote{~\url{https://platform.openai.com/docs/models/gpt-3}} for bug summarization via text completion~\cite{brown2020:language}.
We also use the GPT-3.5 model \texttt{gpt-3.5-turbo}\footnote{\url{https://platform.openai.com/docs/model-index-for-researchers}} for program synthesis, bug summarization and debugging. 
The GPT-3.5 model is an improvement over the GPT-3 175B-parameter model optimized for chat available via an API.
This switch is mainly motivated by rapid model updates, an OpenAI announcement that the former GPT-3 model was due to become obsolete, i.e., not actively supported by the company, along with the recommendation to switch to the newer alternative, GPT-3.5. 
In addition, experiments with GPT-3.5 fall into the generalizability part of our study.

To further evaluate generalization capabilities of SEIDR, we have chosen an open-source competitor of GPT models, Code Llama Instruct 34B~\cite{roziere2023:code}. 
This model has the standard transformer architecture in the backbone with a number of optimizations.
Code Llama is an advancement on the Llama 2 model~\cite{touvron2023:llama}, an auto-regressive transformer model fine-tuned for code infilling (cloze-style generation), processing longer context windows and instructions.

\newpage \subsection{Prompts}
\label{sec:seidr-prompts}


\paragraph{Prompting Engineering Experiments in Initial Exploration of SEIDR}
\label{sec:seidr-prompt-strategies}
% Initial Exploration with GPT-3

The prompts discussed in this Section are used in the initial exploratory experiments dedicated to RQ2. 
The prompt for the LLM model consists of the input for editing --- candidate program generated so far --- and a debug instruction to repair the candidate. 
We test SEIDR on 11 debug instructions to explore whether the use of the LLM for text completion benefits the performance of our framework, as well as what effect different phrases have on the debug process. 
We aim at comparing debug instructions that use neutral phrases with those that use more confident language and mimic experienced software developers, as well as shorter and longer instructions with different amounts of details about code behavior.
To alleviate the effect of beam width and tree arity, we set these parameters to 1 and test the repair-only beam search strategy shown in Figure~\ref{fig:beam-search}. 
This strategy is used to gradually improve one program candidate throughout the search with no competing programs in the same generation. 

The debug instructions are formulated as templates. The instructions describe the violated requirements in terms of the wrong output in a failing I/O test or summarize the bug to capture issues in code logic.
We present debug instructions using the template engine format: the brackets \{ \} denote that the placeholder in the brackets will be replaced with the value generated during execution, \{I$_{\text{val}}$\} and \{O$_{\text{val}}$\} stand for values of the validation set I/O pair.
As shown in Figure~\ref{fig:method-instruct}, the instruction to fix execution errors which abort the program before the resulting output is obtained with \texttt{stderr} lines: Fix \{stderr\}. 
Debug instructions that do not use LLM for bug summarization are as follows:
\begin{enumerate}[label=S\arabic*]
\setcounter{enumi}{-1}
    \item \label{seidr:prompt-0} \smalltt{Make sure that \{I$_{\text{val}}$\} -> \{O$_{\text{val}}$\}};
    \item \label{seidr:prompt-1} \smalltt{Make sure the code returns \{O$_{\text{val}}$\} for input \{I$_{\text{val}}$\}};
    \item \label{seidr:prompt-2} \smalltt{Ensure that input \{I$_{\text{val}}$\} yields output \{O$_{\text{val}}$\}};
    \item \label{seidr:prompt-3} \smalltt{Modify code to get \{O$_{\text{val}}$\} from \{I$_{\text{val}}$\}};
    \item \label{seidr:prompt-4} 
    \smalltt{Code must correspond instructions in comments and \newline
    \{I$_{\text{val}}$\} must yield \{O$_{\text{val}}$\}};
    \item \label{seidr:prompt-5} \smalltt{See comments in code and return \{O$_{\text{val}}$\} for input \{I$_{\text{val}}$\}}.
\end{enumerate}

The instruction \ref{seidr:prompt-0} is the default instruction for tree arity experiments. 
It has an intuitive symbolic notation (->) instead of the word ``return'' or ``yield''. 
In instructions \ref{seidr:prompt-1}--\ref{seidr:prompt-3}, we experiment with verbs in the instruction and the order of output and input mentions. 
Alternatively, in debug instructions \ref{seidr:prompt-4}--\ref{seidr:prompt-5}, we prompt the model to address the comments in the program which contain task description in addition to providing the details of the failing I/O pair. 
Overall, instructions \ref{seidr:prompt-0}--\ref{seidr:prompt-5} indicate the requirements to be met, but do not describe the current program's behavior. 

The second set of instructions use the LLM for text completion. 
The instructions are designed so that the LLM is prompted to complete the sentence that should describe an error. 
In addition to validation I/O pairs, the following notation is used: \{O$_{\text{p}}$\} denotes the program candidate output for input \{I$_{\text{val}}$\}, \{task\} is a placeholder for a problem description in English. 
Note that we do not include the incorrect output $O_p$ of a generated candidate program in debug instructions S0-S5, because it is recommended to avoid asking the model what not to do \cite{BestPracticesPrompt}. 
We denote the text completion LLM's output as \{bug\} which should constitute the bug summary.
Input templates to use LLM for bug description followed by debugging instruction templates (after ``$\rightarrow$'') are as follows:
\begin{enumerate}[label=M\arabic*]
\setcounter{enumi}{5}
    % 
    \item \label{seidr:prompt-6} 
    \smalltt{The code should solve the following problem: \{task\}. \newline 
    The code must return \{O$_{\text{val}}$\} for input \{I$_{\text{val}}$\} but it returns \{O$_{\text{p}}$\}. \newline 
    Obviously, the error is that... \newline 
    $\rightarrow$ Fix \{bug\}};
    % 
    \item \label{seidr:prompt-7} 
    \smalltt{The code should solve the following problem: \{task\}. \newline
    The code must return \{O$_{\text{val}}$\} for input \{I$_{\text{val}}$\} but it returns \{O$_{\text{p}}$\}. \newline
    The error is that... \newline 
    $\rightarrow$ Fix \{bug\}};
    % 
    \item \label{seidr:prompt-8} 
    \smalltt{Problem description: \{task\}. \newline
    The code must return \{O$_{\text{val}}$\} for input \{I$_{\text{val}}$\}, but it returns \{O$_{\text{p}}$\}. \newline
    It is clear the error is that... \newline 
    $\rightarrow$ Fix \{bug\}};
    % 
    \item \label{seidr:prompt-9} 
    \smalltt{There is clearly a bug in code, because the code returns \{O$_{\text{p}}$\} \newline
    for input \{I$_{\text{val}}$\} but output \{O$_{\text{val}}$\} is expected. The bug is that... \newline 
    $\rightarrow$ Fix \{bug\}};
    % 
    \item \label{seidr:prompt-10} 
    \smalltt{There is clearly a bug in code, because the code returns \{O$_{\text{p}}$\} \newline
    for input \{I$_{\text{val}}$\} but output \{O$_{\text{val}}$\} is expected. The bug is that... \newline 
    $\rightarrow$ Fix \{bug\} and modify the code to return \{O$_{\text{val}}$\} for input~\{I$_{\text{val}}$\}.}
    % 
\end{enumerate}
Note that the text completion LLM does not use program candidates in its input, but only template inputs \ref{seidr:prompt-6}--\ref{seidr:prompt-10} before the arrow. 

Input \ref{seidr:prompt-6} for the text completion LLM is used to evaluate the effect of the ``confidence'' sentiment on the bug summaries and debugging process. 
It is identical to input \ref{seidr:prompt-7} except for the word ``obviously'' which should reflect experience and/or confidence of the comment. 
Inputs \ref{seidr:prompt-7} and \ref{seidr:prompt-8} can be compared in the way the problem description is introduced, i.e., as a separate sentence similar to a spoken situation in prompt \ref{seidr:prompt-7} or as a short title in \ref{seidr:prompt-8}.

Input templates \ref{seidr:prompt-9} and \ref{seidr:prompt-10} for text completion LLM are identical, but the instruction templates are different.
Text completion inputs start with a ``confidently'' phrased statement that a bug is present in code.  
We include both the LLM output \{bug\} and description of the failing validation test case in debug instruction \ref{seidr:prompt-10}.
Therefore, instructions \ref{seidr:prompt-6}--\ref{seidr:prompt-9} rely mainly on the LLM output to summarize the bug, whereas instruction \ref{seidr:prompt-10} also provides information about the expected output. 


\paragraph{Prompts for Instruction Fine-tuned and Chat Models}
\label{sec:seidr-ollama-prompts}
% Experiments with GPT-3.5 and Code Llama: 

The prompts presented in this Section are used in the experiments with GPT-3.5 and Code Llama.
This implementation is optimized for instruction and chat models, which use prompts as inputs represented as text, partially with code fragments.
The models use a system message which describes the ``role'' of the LLM and a regular message that works as an instruction or a chat message from a user.
To provide more context, we always use a problem description, problem name, programming language to the model as textual input (\texttt{descr}). 
As code, we also add an initial template depicted in Figure~\ref{fig:template} to \synthmodel{} and the current program candidate to \textmodel{} and \debugmodel{}.

The resulting prompts are as follows.
System message is: \newline
    \smalltt{You are an experienced software developer.} \newline
    \smalltt{You write concise code in \{language\}.}\newline
    \smalltt{The code must read input from user and return output corresponding to the task description.}

\vspace{4pt}

Input to \synthmodel{} looks as follows:\newline 
\smalltt{
Solve the following code contest problem: \{problem\_name\}. \newline
Problem description: \{problem\_description\}.\newline
\{program\_template\}\newline
Only complete the code, do not add triple quotes, \newline
do not give explanations.
}

\vspace{4pt}

Bug explanations are generated with \textmodel{} using the following instructions:\newline
\smalltt{I'm trying to solve the following code contest problem: \{problem\_name\}. \newline
Problem description: \{problem\_description\}.\newline
Currently, the code is \newline
\`{}\`{}\`{}\newline
\{program\_candidate\}\newline
\`{}\`{}\`{}\newline
The issue is \newline
\{stderr\} or "it must return \{expected\_output\} for input \{input\}, \newline
but it returns \{output\}".\newline
Describe how I should fix the code in a very concise manner.
}

\vspace{4pt}

Debugging model \debugmodel{} operates on the following instruction:\newline
\smalltt{
Solve the following code contest problem: \{problem\_name\}.\newline
Problem description: \{problem\_description\}.\newline
Currently, the code is \newline
\`{}\`{}\`{}\newline
\{program\_candidate\}\newline
\`{}\`{}\`{}\newline
Modify the code as \{bug\_summary\}.\newline
You must only return correct code. \newline
Remove any triple quotes, language name or explanations.\newline
}



\newpage \subsection{Repair-replace Trade-off Settings}
\label{sec:seidr-trade-off-settings}

The settings for tree arity will also be divided into two experiment sets: the ones for GPT-3 and Codex and the ones for GPT-3.5 and Code Llama experiments.

\paragraph{Tree Arity for Initial Exploration of SEIDR}
\label{sec:seidr-tree-arity-gpt-3}
As described in Section~\ref{sec:seidr-beam-search}, the beam width $\beamwidth{}$ and tree arity $\treearity{}$ define the repair-replace trade-off where higher $\beamwidth{}$ and $\treearity{}$ correspond to repair over replace. 
We evaluate four options for these hyperparameters as shown in Table~\ref{tab:seidr:w-n}. 

\begin{table}[h]
    \centering
    % % \vspace*{-1ex}
    \caption{Tree search hyperparameters in the initial exploration.}\small
    \label{tab:seidr:w-n}% \vspace*{-4mm}
    \begin{tabular}{r|c|c|c|c}
    experiment & 1 & 2 & 3 & 4 \\
    \midrule
     beam width $\beamwidth{}$ & 1 & 10 & 100 & $\infty$ \\
     tree arity $\treearity{}$ & 1 & 10 & 100 & $\infty$
    \end{tabular}
    % % \vspace*{-1.8ex}
\end{table}

Because we aim to compare tree search parameters, we fix one default debugging instruction and use the \instructs{} block. 
Moreover, we set the upper limit for the total number of generated program candidates to 1000 to limit the experimentation time. 
Although some solutions may not be found within the hard limit, we assume\footnote{~This assumption is later confirmed in Section~\ref{sec:seidr-seidr:rq1}} that 1000 program candidates form a sufficiently large search space for our experiments.
$\beamwidth{} = \treearity{} = \infty$ is achieved in implementation by setting equal $\beamwidth{}$ and $\beamwidth{}$ equal to the upper limit on program count of 1000.
This ensures that a second generation of programs does not exist.


% \paragraph{Tree Arity Values for SEIDR with GPT-3.5 and Code Llama}

\paragraph{Tree Arity for SEIDR Generalizability Experiments}
\label{sec:seidr-tree-arity-ollama}
Each of the three LLMs in \synthesize{}, \instruct{}, and \debug{}  can generate sequences in batches. 
We generate $\treearitydraft{}$ programs in the first generation with \synthmodel{}, $\treearityexplain{}$ bug explanations with \textmodel{} for each program in the first generation, and $\treearitydebug{}$ candidate repairs for each of the debugging instructions using \debugmodel{}.
A new generation of $\treearityexplain{} \cdot \treearitydebug{}$ programs is ranked and filtered to keep the best performing $\beamwidth{}$ candidate programs for generating the next candidates. 

To balance between a reasonable number of experiments and diverse sets of hyperparameters, we fix $\treearityexplain{}=2$ to moderately vary the bug descriptions and set $\treearitydraft{} = \treearitydebug{}.$
In the experiments with GPT-3 and Codex, we generated only one bug explanation ($\treearityexplain{} = 1$) and used $\treearitydraft{} = \treearitydebug{} = \treearity{}$ setting, too.
The upper limit for the total number of generated program candidates is set to 100 to limit the experimentation time. 
We evaluate six options for these hyperparameters as shown in Table~\ref{tab:w-n} in the experiments with average quality-first ranking and four non-corner case options for quality-diversity ranking with lexicase selection. 
The choice of these tree branching hyperparameters and the maximum number of generated programs is motivated by the experiments with GPT-3 and Codex, where the best results were obtained for $\treearity{}=10,$ so we explore the area around this value more closely.
In the same experiments, the majority of problems in PSB2 were solved within the first 100 generated programs. Note that the setting $\treearitydraft{}=\beamwidth{}=\infty$ ensures that a second generation of programs does not exist.

\begin{table}[h]
    \centering
    \caption{Tree search hyperparameters in the generalizability experiments.}\small
    \label{tab:w-n}
    \begin{tabular}{rcccccc}
    \toprule
    experiment & 1 & 2 & 3 & 4 & 5 & 6 \\
    \midrule
     beam width, $\beamwidth{}$ & 1 & 4 & 8 & 10 & 16 & $\infty$ (100) \\
     \# programs in the 1st generation, $\treearitydraft{}$ & 1 & 4 & 8 & 10 & 16 & $\infty$ (100) \\
     \# bug explanations for candidate, $\treearityexplain{}$ & 2 & 2 & 2 & 2 & 2 & - \\
     \# repairs for each explanation, $\treearitydebug{}$ & 1 & 4 & 8 & 10 & 16 & - \\
     \midrule
     used in quality-diversity ranking & 
     no & yes & yes & yes & yes & no \\
     used in average quality-first ranking  & 
     yes & yes & yes & yes & yes & yes \\
     \bottomrule
    \end{tabular}
\end{table}



\newpage \subsection{Performance Indicators}
\label{sec:seidr-metrics}

\sloppy %
In our experiments, we compare 
the number of fully solved programs obtained using SEIDR with different values of hyper-parameters. 
For a more detailed analysis of results, we use \emph{test pass rate (TPR)} and \emph{Excess Programs Generated (EPG)}.
TPR reflects the percentage of fully passed test cases based on the exact match of program output and test output. 
The TPR metric is used for the final evaluation of generated programs and does not reflect partial passing of the I/O test as opposed to the \emph{score} as calculated by the \rank{} agent. 
To compare our results with recent LLMs, we use \emph{pass@k} metric that shows how many problems have $TPR=1$ if we cut off the tree after generating $k$ programs. 

\debug{} and \execute{} agents generate a number of programs that are replaced or repaired during the search for solution program. 
The number of programs generated before the first occurrence of the program that passes all validation test cases is referred to as EPG. 
EPG is indicative of the computational cost of solving a problem distributed in terms of LLM inferences and program compilations and executions.

\newpage \subsection{Implementation Details}
\label{sec:seidr-implementation}


In this study, we have three sets of experiments: one with Codex for code generation and GPT-3 for bug summaries or static templates, and the other two with Code Llama or GPT-3.5 for all the agents. 
% : one set of initial exploration of SEIDR, one set of generalizability experiments with extra focus on ranking strategies and more fine-grained tree arity choice, and the third batch of experiments are dedicated only to generalizability of the findings.

The first one is dedicated to the initial exploration of RQ1 and RQ2 with Codex-edit (code-davinci-edit-001) as the LLM for writing and debugging programs and GPT-3 (text-davinci-003) for bug summarization via text completion. 
We ensure that the program candidates generated from the same parent program are different from each other by changing the temperature parameter of Codex-edit.
We will refer to these experiments as \emph{Initial Exploration of SEIDR} with Codex and GPT-3, and test the hyperparameter choices on PSB2 only.
Here, we use wide steps between tree arity values (see Section~\ref{sec:seidr-tree-arity-gpt-3}) and explore static and GPT-3-based prompting strategies described in Section~\ref{sec:seidr-prompt-strategies}, where \synthmodel{} generates only one bug summary for each program and Codex generates \treearity{} program updates based on the bug summary.
 
The second set aims to investigate generalizability (RQ4) of SEIDR and potentially improve over GPT-3 with a newer, generally more powerful version, GPT-3.5.
In detail, GPT-3.5 is used in more fine-grained repair-replace trade-off exploration (RQ1) and ranking experiments (RQ3). 
We will refer to these experiments as \emph{SEIDR Generalizability  Experiments with  GPT-3.5} and test it both on PSB2 and HumanEval.
Here, we explore a more fine-grained tree arity values (see Section~\ref{sec:seidr-tree-arity-ollama}) and different ranking strategies but fix the same prompts from Section~\ref{sec:seidr-ollama-prompts}. 
\instruct{} is represented by \instructllm{} agent and creates $\treearitydebug{}$ bug summaries.
Each program update creates $\treearity{}$ child programs from one parent in the \synthesize{} and \debug{} agents or blocks.

The third set of experiments concerns the generalizability of the results and thus uses Code Llama to explore RQ4 further. 
We will refer to these experiments as \emph{SEIDR Generalizability Experiments with Code Llama.}
This model is run with the best-performing set of hyperparameters found for SEIDR with GPT-3.5 on each of the benchmarks in C++ and Python.
We also compare the performance of SEIDR with the current state-of-the-art without SEIDR (see Section~\ref{sec:seidr-results-rq3}).


In all the experiments, we set the limit to generate a maximum of $M$ program candidates during the search of the candidate that passes all validation tests. 
If we reach $M$ candidates and none of them passes all validation tests, we report the test pass rate for the last generated candidate. 
For the first set of experiments, we set $M = 1000,$ and for the other two we limit it to $M = 100,$ after finding out that for the majority of problems a solution is found among the first 100 programs or not found at all.

To operate with the chosen LLMs in SEIDR, we use ollama\footnote{\url{https://ollama.ai/}} and LangChain.\footnote{\url{https://www.langchain.com/}}  
To ensure that the program candidates generated from the same parent program are different from each other, we change the temperature parameter of LLMs. 

Due to repetitive calls to \execute{}, we have to resolve the speed of testing versus precision trade-off while choosing the number of validation test pairs.
We resolve the trade-off by fixing the validation set size at~100. Due to a small number of tests in HumanEval-X, all tests are made visible during the validation step. 
We use 2000 I/O pairs from the test split of PSB2 to evaluate the candidate program that has passed all the validation test cases during debugging. 

 
\newpage
\section{Results and Discussion}
\label{sec:seidr-results}

In this Section, we first present the results of the initial exploration, where we explore the repair-replace trade-off in SEIDR with Codex and GPT-3 (RQ1) and present prompt engineering experiments (RQ2) --- using the PSB2 benchmark.
We then continue with generalizability experiments (RQ4) with GPT-3.5 on PSB2 and HumanEval, two ranking strategies (RQ3), and more tree arities (RQ1). 
% For these datasets and GPT-3.5, we use a more fine-grained choice of the number of programs or bug descriptions to generate from each parent program (RQ1).
% Simultaneously, we test both ranking strategies (RQ3), lexicase selection and tree (beam) search for SEIDR with GPT-3.5 on PSB2 and HumanEval.
Finally, we present the generalizability results of SEIDR with Code Llama and set the best-performing parameters from SEIDR with GPT-3.5.

\newpage \subsection{Initial Exploration}
% of SEIDR with GPT-3 and Codex}

\paragraph{RQ1. Repair-replace Trade-off}
\label{sec:seidr-seidr:rq1}
We compare the number of solved problems in the experiments with tree arity of 1, 10, 100, and $\infty$ and fixed debug instruction \ref{seidr:prompt-0} in Python and C++ in Figure~\ref{fig:seidr:solved-vs-bf}. 
The results of SEIDR are compared to the baseline performance of PushGP on the PSB2 benchmark which solves 17 out of 25 problems. 
Note that experiments with $N=1$ and $N=\infty$ can be considered as ablation studies, where the replace option and repair option is turned off, correspondingly. 
 %

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\linewidth, trim={0mm 2.8mm 0mm 2mm}, clip]{images/fig_5_num_solved_problems_vs_bf_1000_v3_zenodo.pdf}
  %
  % % \vspace*{-2mm}
  \caption{Number of solved PSB2 problems depending on the tree arity in beam search for the fixed prompt type \ref{seidr:prompt-0}.}
  \label{fig:seidr:solved-vs-bf}
  \vspace*{-2ex}
\end{figure}

The results highlight the benefit of compromise strategies with tree arity of 10 and 100 over repair-only ($N=1$) and replace-only ($N=\infty$) strategies. 
The repair-only scheme is outperformed by other strategies. 
We explain the poor performance of repair-only strategy by the fact that the search space is under-explored. 
Specifically, replace scenario ensures the LLM for debugging component represented by Codex-edit in our experiments generates different updates of program candidates using variable temperature.
The probability of finding a better fix is higher when more alternatives are generated to update the draft program at $N>1$ compared to $N=1$. 
The search strategy with $N=10$ yields the best results: it performs on par with PushGP for C++ and outperforms the baseline during Python program synthesis by +2 problems resulting in a total of 19 programs that pass all test cases.
The results imply that generating a moderate number of programs in parallel during the \debug{} step works better than the policies in which more updates are generated for each program (100 or 1000) or only one program is updated iteratively.

We present the analogy of the solution speed for all four arities and fixed default debug instruction in Figure~\ref{fig:seidr:epg-distribution}. 
In detail, we show the distribution of EPG values in all experiments to explore how many candidate updates are generated before the solution is found.
We zoom in to the cases with solutions found with up to the first 10 program candidates in Figure~\ref{fig:seidr:epg-distrib-solved-10} and show the EPG distribution with the step of 100 candidates in Figure~\ref{fig:seidr:epg-distrib-solved-100}. 

\begin{figure}[t]
 %
\begin{subfigure}[t]{\columnwidth}
\centering
\includegraphics[width=.8\linewidth, trim={0mm 4mm 0mm 0mm}]{images/fig_6_epg_distribution_solved_maxprog_1000_1_v3_zenodo.pdf}
  \caption{0 $\leq$ EPG $\leq$ 10 with step 1.}
  \label{fig:seidr:epg-distrib-solved-10}
\end{subfigure}

% \vspace{2mm}

\begin{subfigure}[t]{\columnwidth}
\centering
\includegraphics[width=.8\linewidth, trim={0mm 4mm 0mm 0mm}]{images/fig_6_epg_distribution_solved_maxprog_1000_100_v3_zenodo.pdf}
  \caption{0 $\leq$ EPG $\leq$ 1000 with step 100.}
  \label{fig:seidr:epg-distrib-solved-100}
\end{subfigure}
% % \vspace{-2mm}
\caption{Distribution of the number of generated programs during each problem-solving attempt in the experiments with different tree arities where a problem solution is found.}
\label{fig:seidr:epg-distribution}
\vspace{-2ex}
\end{figure}

Out of 100 experiments for each language, in 21--24\% of runs in Python and C++, the draft program is already the solution (EPG=0). 
For 31--33\% of experiments, the solution is found after discarding 5 candidates. 
Around half of experiments do not generate more than 100 programs. 
However, 5 problems are solved with more than 500 generated programs in Python and 1 problem in C++ (with $N=10$).
The results imply that the first steps in the update of the draft program are crucial for solving the problem. 
The chances of solving the problem on the later stages of the search, such as after 100 programs have been generated, are low.
This confirms our initial assumption in Section~\ref{sec:seidr-trade-off-settings} that 1000 programs are sufficient.

% \begin{mdframed}[style=mystyle]
\begin{framed}
\noindent
\textbf{Answer to RQ1 (repair-replace trade-off in the initial exploration):} 
SEIDR outperforms the PushGP baseline on PSB2 in Python and performs on par with it in C++ experiments with tree arity of 10. 
Search strategies with tree arity larger than one benefit from the replace possibility of the SEIDR framework as a consequence of using variable temperature for Codex-edit.
The repair component is also crucial for the framework because the replace-only search policy (with tree arity of $\infty$) performs worse than the policies that alternate between replace and repair during program update (with tree arity of 10 or 100).  
\end{framed} 
% \end{mdframed}


\paragraph{RQ2. Prompt Engineering in Initial Exploration}
We report the number of solved problems for different static and GPT-assisted debug instructions in Figure~\ref{fig:seidr:solved-vs-prompt-id}. 
Because debug instructions are parts of prompts for LLMs and the program candidate format does not change, we will use the term prompt during the analysis of experiment results with different instructions.
Overall, the performance of the framework is robust to the debug prompt choice, both with LLM-generated and static templates. 
The number of solved problems differs for Python and C++ in our experiments.

\begin{figure}[htb]
  \centering
  \includegraphics[width=.8\linewidth, trim={0mm 2mm 0mm 2mm}, clip]{images/fig_7_num_solved_problems_vs_prompt_id_maxprog_5_v3_zenodo.pdf}
  %
  % % \vspace*{-7mm}
  \caption{Number of solved PSB2 problems depending on the instruction choice for the fixed tree arity of 1. 
  %
  }
  \label{fig:seidr:solved-vs-prompt-id}
  \vspace*{-2ex}
\end{figure}

For C++, all debug prompts except \ref{seidr:prompt-2} result in the same or higher performance than the instruction \ref{seidr:prompt-0} which is used in the repair-replace trade-off experiments. 
The debug instruction \ref{seidr:prompt-2} contains the verbs ``yield'' and ``ensure'' which are probably rarely used in code documentation. 
The best debug instruction for C++ is the LLM-assisted template \ref{seidr:prompt-6} containing the word ``obviously'', which should indicate the confidence of the author of bug summary whom GPT-3 should mimic during autocompletion.



\begin{figure}[H]
  \centering
  % \includegraphics[width=.99\textwidth, trim={3mm 2mm 3mm 2mm}, clip]{img/seidr/results/fig_8_num_programs_generated_vs_propmt_id_test_pass_rate_vertical_maxprog_5_v3_zenodo.png}
  \includegraphics[width=.99\textwidth, trim={3mm 3mm 3mm 2mm}, clip]{images/fig_8_num_programs_generated_vs_propmt_id_test_pass_rate_vertical_maxprog_5_v3_thesis.pdf}
  % % \vspace*{-2mm}
  \caption{Number of excess programs generated (in color) and test pass rate (as numbers) depending on the type of debug prompt. Higher EPG values are shown in darker shades than low EPG. We denote solved problems with ``+'' (test pass rate = 1), unsolved problems with ``-'' (test pass rate = 0), and show the test pass rate for partially solved problems. }
  \label{fig:seidr:epg-prompt-test}
  %
  \vspace*{-3ex}
\end{figure}

Python programs do not show the same effect during experiments with different prompts. 
The overall performance drops in comparison with using the prompt \ref{seidr:prompt-0}. 
By limiting the total number of generated programs from 1000 
to 5 in the current set of experiments, we lose 2 problem solutions in Python with \ref{seidr:prompt-0}. 
The prompt that results in the best performance in C++ for the EPG limit of 5 corresponds to the worst performance in Python. 
This result can occur due to the small tree arity and low variability of debugging updates of the initial draft. 
Another reason is that the GPT summary of bugs may not point to logical errors. The model for text autocompletion frequently outputs bug summaries that mention ``the code is not accepting the input correctly.''
Note that such bug summary appears in other debug prompts, too. 

To analyze the effect of using different prompts on a problem level, we present a heatmap of EPG for all 25 problems in Figure~\ref{fig:seidr:epg-prompt-test}. 
We add the values of test pass rate in numbers or signs and show EPG in color. 
Empty cells denote that the search halts due to other OpenAI exceptions, such as \texttt{APIError} \cite{OpenAIPlatform}.
In addition, if the framework halts before max programs attempts (light-blue cells with a~``-''), it is due to the input length limit of underlying LLMs, i.e., the generated code is too long and does not fit as input to the LLM.

Some problems are solved with all prompts, while other problems are solved with only a subset of prompts, solved partially, or not solved at all. 
A number of problems are solved with all or the majority of prompts in both languages, such as basement, fizz-buzz, paired-digits, and twitter.
Other problems pass all tests in only one of the languages, such as luhn, vector-distance, fuel-cost, or substitution-cipher. 
Most of the solved problems are generated as the first draft or within 1--2 debug steps. 
However, some problems pass 90\% of test cases at the fifth step, such as substitution-cipher in Python with prompts \ref{seidr:prompt-4} and \ref{seidr:prompt-8} or shopping-list in C++ with prompts \ref{seidr:prompt-0}, \ref{seidr:prompt-1}, \ref{seidr:prompt-5} and \ref{seidr:prompt-7}. 
These runs are likely to be updated with the fully correct programs in the following several steps, according to the results in section 5.1, but the experiments
are stopped for the fairness of inter-prompt comparison.

The most interesting cases concern the problems that are solved only with LLM bug summary or only with static prompts. 
For example, the gcd problem is solved only with prompts \ref{seidr:prompt-6}--\ref{seidr:prompt-10} in C++ and is not solved with either of \ref{seidr:prompt-0}--\ref{seidr:prompt-5}. 
A similar result is obtained for spin-words and coin-sums in C++.
In Python, we observe only the cases where solutions are obtained with static prompts and are not obtained with GPT-assisted prompts, e.g., for find-pair, camel-case. In addition, several prompts work well from both S and M categories as for gcd. 


% \begin{mdframed}[style=mystyle]
\begin{framed}
% \noindent
\textbf{Answer to RQ2 (prompt engineering in the initial exploration):} 
Program synthesis in C++ with SEIDR achieves better performance in the repair-only setting with both GPT-assisted prompts that summarize bugs in code and static templates that describe failing I/O cases. 
The best-performing C++ instruction is obtained with GPT for text completion that contains the word ``obviously''.
Results differ for PSB2 solutions in Python: the static prompt template \ref{seidr:prompt-0} results in the best performance. 
Overall, SEIDR performance is stable with different debugging prompts submitted to Codex-edit.
\end{framed}
% \end{mdframed}
% \vspace*{-2mm}


\newpage \subsection{Generalizability Experiments}
% for SEIDR with GPT-3.5 and Code Llama}

In this Section, we present combined results for the replace-repair trade-off and ranking approaches obtained with GPT-3.5 and Code Llama and discuss them in the dedicated Sections. 
We count the number of fully solved problems or tasks as measured by $TPR=1$ in experiments with the settings described in Table~\ref{tab:w-n} and present the language-specific results for PSB2 and HumanEval in Figure~\ref{fig:repair-replace-trade-off-gpt3.5}. 
We also explore the speed of obtaining each individual solution with the same experiment settings and report them for PSB2 in Figure~\ref{fig:epg-psb2}, HumanEval-Python and HumanEval-C++ in Figures~\ref{fig:epg-humaneval-python} and~\ref{fig:epg-humaneval-c++}, correspondingly.
Other experiment-specific graphs and tables are described in the dedicated Sections. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Num solved problems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
 %
\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/num_solved_problems_vs_bf_psb2_gpt35_01.pdf}
  \caption{PSB2.}
  \label{fig:psb2-gpt3.5}
\end{subfigure}
% \vspace{2mm}
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/num_solved_problems_vs_bf_humaneval_gpt35_01.pdf}
  \caption{HumanEval}
  \label{fig:he-gpt3.5}
\end{subfigure}
% \vspace{-2mm}
\caption{Repair-replace trade-off as a tree search problem realised with average quality-first (no lexicase selection) and quality-diversity (lexicase  selection) ranking: the total number of solved problems as measured by $TPR=1$ using SEIDR with GPT-3.5 for the experiments with different numbers of generated programs per instruction ($\treearitydraft$ and $\treearitydebug{}$) and ranking types.}
\label{fig:repair-replace-trade-off-gpt3.5}
\vspace{-2ex}
\end{figure}




\paragraph{RQ1. Repair-replace Trade-off Exploration}
\label{sec:seidr-treearity-ollama}

We compare the number of solved problems in the experiments with $\treearitydraft{}=\treearitydebug{}$ values of 1, 2, 4, 10, 16, $\infty$ (100) and $\treearityexplain{}=2$ in Python and C++ in Figure~\ref{fig:repair-replace-trade-off-gpt3.5}. 
We will refer to the (equally set) values of $\treearitydraft{}$ and $\treearitydebug{}$ as $N^*$ further in the text.
The results of SEIDR are compared to the baseline performance of PushGP on the PSB2 benchmark, which solves 17 out of 25 problems. 
As previously, the experiments with $N^*=1$ and $N^*=\infty$ correspond to ablation studies, where the replace option or repair option is turned off. 



For PSB2, the results highlight the benefit of compromise strategies with tree arity of 2, 4, 10, and 16 over repair-only ($N^*=1$) and replace-only ($N^*=\infty$) strategies (see Figure~\ref{fig:psb2-gpt3.5}). 
The repair-only scheme shows the worst performance. 
% We explain the poor performance of repair-only strategy by the fact that the search space is under-explored. 
% Specifically, the replace scenario ensures the LLM for debugging represented by GPT-3.5 in our experiments generates different updates of program candidates using variable temperature.
% The probability of finding a better fix is higher when more alternatives are generated to update the draft program at $N^*>1$ compared to $N^*=1$. 
The search strategy with lexicase selection and $N^*=16$ yields the best results on Python and tree arities  $N^*=4, \; 10, \; 16$ with the average quality-first search strategy outperform other tree arities but do not outperform PushGP.
Note that SEIDR with Codex (GPT-3 trained on code) as \synthmodel{} and \debugmodel{} and GPT-3 as \textmodel{} does outperform PushGP on Python and performs on par with PushGP in C++ with $N^*=10$ and $\treearityexplain{}=1$. %as described by \cite{liventsev2023:fully}.
This can be explained by the generic chat-tuned nature of GPT-3.5 as opposed to Codex' code-specific pre-training. 
% The results imply that generating a moderate number of programs in parallel during the \debug{} step works better than the policies in which more updates are generated for each program (2-16) or only one program is updated iteratively on the datasets .

For HumanEval-C++, the trend is that the larger number of tasks are solved with smaller tree arities holds (see Figure~\ref{fig:he-gpt3.5}). 
However, the leading tree arity value is $N^*=1,$ which diverges from the PSB2 findings. 
In HumanEval-Python, the best result is obtained with  $N^*=2$ with lexicase selection, while the second best result is observed for $N^*=16$ with lexicase  selection and $N^* = \infty \; (N^*=\text{max programs}=100)$ without lexicase selection.
It is notable that HumanEval-X has less tests than PSB2 which suggests that with more thorough testing, the leading tree arity can be different.

The speed of finding a solution broken down to the problem level is shown in Figure~\ref{fig:epg-psb2} for PSB2, Figure~\ref{fig:epg-humaneval-python}
for HumanEval-Python and Figure~\ref{fig:epg-humaneval-c++} for HumanEval-C++.
In PSB2, the fastest solutions to all validation sets without lexicase  selection in Python is found using $N^*=16$ (observed in columns with the prevalence of light-colored cells over the dark-colored cells), both with and without lexicase  selection, and in C++  using $N^*=2$, with and without lexicase selection, with a runner-up of $N^*=16$ with lexicase  selection.
Bowling in Python, PSB2, and task 162 in HumanEval-Python experiments are the only problems solved by a unique set of hyperparameters, $N^*=10$ with lexicase selection, and not solved by any other set of hyperparameters.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPG heatmaps for each problem
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth, trim={3mm 2.8mm 3mm 2mm}, clip]{images/epg_psb2_gpt35_01.pdf}
  \caption{PSB2: EPG (color) and Test Pass Rate (``+'' for $TPR=1;$ ``$-$'' for $TPR=0$).
  % Higher EPG values are shown in darker shades than low EPG. We denote solved problems with ``+'' (test pass rate = 1), unsolved problems with ``$-$'' (test pass rate = 0), and show the test pass rate for partially solved problems.
  }
  \label{fig:epg-psb2}
  % \vspace{-4ex}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.85\linewidth, trim={0mm 2.8mm 0mm 2mm}, clip]{images/epg_humaneval_Python_gpt35_01.pdf}
  \caption{HumanEval-Python: Excess Programs Generated (in color, white stands for $EPG=0$) and Test Pass Rate (``+'' for $TPR=1;$ ``$-$'' for $TPR=0$).}
  \label{fig:epg-humaneval-python}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.85\linewidth, trim={0mm 2.8mm 0mm 2mm}, clip]{images/epg_humaneval_C++_gpt35_01.pdf}  %
  \caption{HumanEval-C++: Excess Programs Generated (in color, white stands for $EPG=0$)  and Test Pass Rate (``+'' for $TPR=1;$ ``$-$'' for $TPR=0$).}
  \label{fig:epg-humaneval-c++}
\end{figure}


In HumanEval-Python, for the majority of solved tasks, the solutions are found at the first attempt which negatively influences the analysis of repair-replace trade-off on this dataset, since all tree search algorithms are equivalent when the root node of the tree is the solution.
For HumanEval-Python, task IDs 11, 27, 33, 101-104, 129-130 are solved faster with $N^*=4, \; 10$ with lexicase selection and $N^*=4, \; 10, \; 16$ without lexicase selection more frequently than with other values of $N^*$ (see Figure~\ref{fig:epg-humaneval-python}).
The trend of better results with smaller values of $N^*$ is true both for the number of solved problems (Figure~\ref{fig:he-gpt3.5}) and higher speed of finding solutions (see Figure~\ref{fig:epg-humaneval-c++}) in C++.
% Smaller values of $N^*$ result in both faster and more solutions (without lexicase selection or on par with it) for HumanEval-C++, which is indicated by brighter colors in the columns on the left of the heatmaps compared to the columns on the right.

% \begin{mdframed}[style=mystyle]
\begin{framed}
% \noindent
\textbf{Answer to RQ1 (repair-replace trade-off for SEIDR with GPT-3.5):} 
Search strategies in SEIDR with tree arity $N^*$ larger than one and lower than $\infty$ benefit from the replace possibility of the SEIDR framework as a consequence of using variable temperature for GPT-3.5 on PSB2.
In the experiments, where the majority of problems are solved with more than one attempt, such as HumanEval-C++, lower tree arities (1, 2, 4) yield better results, which indicates the importance of the \debug{} agent in SEIDR. 
No single leading tree arity $N^*$ was found for SEIDR with GPT-3.5 on PSB2 and HumanEval-X.
\end{framed}
% \end{mdframed}



\paragraph{RQ3. Experimenting with Tree Search and Lexicase Selection Ranking Strategies}
\label{sec:seidr-lexicase-results}

Quality-diversity ranking is implemented as lexicase selection and tested with the number of generated programs in the first generation (drafts) and candidates generated from bug explanations (tree arities $N^*=2,\; 4, \; 10, \; 16$) as mentioned in Table~\ref{tab:w-n}.
We have partially discussed the results concerning the use of lexicase selection and average quality-first ranking while describing Figures~\ref{fig:repair-replace-trade-off-gpt3.5} and~\ref{fig:epg-psb2}-\ref{fig:epg-humaneval-c++}. 
Therefore, in this Section, we focus on the cases where lexicase selection-based ranking outperformed average quality-first ranking and the speed of obtaining solutions with lexicase selection and average quality-based ranking.

We present the analogy of the solution speed metric for all tree arity values and fixed default debug instruction in Figure~\ref{fig:epg-distribution}. 
In detail, we show the distribution of EPG values in all experiments to explore how many candidate updates are generated before the solution is found.
We zoom in to the cases with solutions found with up to the first 10 program candidates in Figures~\ref{fig:psb2-epg-distrib-step-1} and~\ref{fig:humaneval-epg-distrib-step-1} for PSB2 and HumanEval-X, respectively. 
The coarser-grained EPG distribution with the step of 10 candidates is shown in Figures~\ref{fig:psb2-epg-distrib-step-10} and~\ref{fig:humaneval-epg-distrib-step-10}. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % EPG distribution
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
 %
\begin{subfigure}{.9\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/epg_distribution_psb2_gpt35_1_01.pdf}
  \caption{PSB2: 0 $\leq$ EPG $\leq$ 10 with step 1.}
  \label{fig:psb2-epg-distrib-step-1}
\end{subfigure}
% 
% 
\begin{subfigure}{.9\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/epg_distribution_psb2_gpt35_10_01.pdf}
  \caption{PSB2: 0 $\leq$ EPG $\leq$ 1000 with step 100.}
  \label{fig:psb2-epg-distrib-step-10}
\end{subfigure}
% 
\begin{subfigure}{.9\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/epg_distribution/epg_distribution_humaneval_gpt35_1_01.pdf}
  \caption{HumanEval: 0 $\leq$ EPG $\leq$ 10 with step 1.}
  \label{fig:humaneval-epg-distrib-step-1}
\end{subfigure}
% 
\begin{subfigure}{.9\columnwidth}
\centering
\includegraphics[width=\linewidth, trim={0mm 4mm 0mm 0mm}]{images/epg_distribution_humaneval_gpt35_10_01.pdf}
  \caption{HumanEval: 0 $\leq$ EPG $\leq$ 1000 with step 100.}
  \label{fig:humaneval-epg-distrib-step-10}
\end{subfigure}
\caption{Distribution of the number of generated programs with GPT-3.5 during each problem-solving attempt in the experiments with different tree arities $\treearitydraft{}, \; \treearitydebug{},$ where a problem solution is found as measured by $TPR=1.$}
\label{fig:epg-distribution}
\end{figure}

In the experiments where a solution was found, more runs without lexicase selection obtained $TPR=1$ earlier than the runs with lexicase selection (see Figure~\ref{fig:epg-distribution}).
In general, out of 250 experiments for each language for PSB2, in 18.8\% of runs in Python and 33.2\% runs in C++, the draft program is already the solution (EPG=0). 
For 35.2\% of experiments in Python and 44.0\% runs in C++, the solution is found after discarding 5 candidates. 

The majority of experiments do not generate more than 10 programs. 
However, 8 PSB2 problems are solved with more than 50 generated programs in Python and in C++, 4 in each language.
By contrast, in HumanEval-X experiments, the problems are solved faster in Python than in C++, with 72.9\% tasks achieving $TPR=1$ with the draft solution in Python and 12.0\% tasks in C++.
Only 1.7\% experiments in Python reached a solution after discarding 50 candidates, while the analogical measure for C++ is 11.8\%.
Remarkably, although the speed of obtaining solution with $TPR=1$ in C++ is considerably lower, SEIDR solved 12 more tasks in C++ (154) than in Python (142). 


% % TODO for extension: Are they the same tasks or different ones? Does seidr solves collectively more than other models or some uniqye problems than other models? 

The EPG distribution results for correctly solved problems with $TPR=1$ imply that the first steps in the update of the draft program are crucial for solving the problem. 
The chances of solving the problem on the later stages of the search are low.
This confirms our assumption in Section~\ref{sec:seidr-trade-off-settings} that 100 programs are sufficient in the generalizability experiments.



Quality-diversity ranking strategy outperformed the average quality-first ranking in the Python experiments, both on PSB2 ($N^*=4, \;, 10, \; 16$) and HumanEval-Python ($N^*=2, \;, 10, \; 16$). 
To explore the impact of using quality-diversity over average quality-based ranking, we filter out the problems solved with more than one attempt and show the best average score described in Section~\ref{sec:seidr-execute} in Figure~\ref{fig:avg-score-lexicase} for PSB2 and HumanEval-Python. 
We can see that the majority of problems are solved with a jump of average score from zero to one, except for one case in HumanEval-Python and two cases in PSB2 as shown by two angled lines in Figure~\ref{fig:psb2-score-lexicase} and one such line in Figure~\ref{fig:humaneval-score-lexicase}. 
These three experiments were the only cases where the ranking strategy worked. 
In other experiments with zero average scores reported up until the moment the solution is found, lexicase selection does not reorder the program candidates, which is also true for the majority of experiments with average quality-based ranking.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEST AVG SCORE FOR LEXICASE IN  PYTHON
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
 % trim={left lower right upper}
\begin{subfigure}{\textwidth}
\centering
% \includegraphics[width=.94\linewidth, trim={0mm 120mm 0mm 133mm}, clip]{img/results/best_avg_score_lexicase/lexicase-psb2-2_compressed.pdf}
\includegraphics[width=.94\linewidth, trim={0mm 120mm 0mm 133mm}, clip]{images/lexicase-psb2-2.pdf}
  \caption{PSB2.}
  \label{fig:psb2-score-lexicase}
\end{subfigure}

% \vspace{2mm}

\begin{subfigure}{.94\textwidth}
\centering
% \includegraphics[width=\linewidth, trim={0mm 110mm 0mm 147mm}, clip]{img/results/best_avg_score_lexicase/lexicase-humaneval-2_compressed.pdf}
\includegraphics[width=\linewidth, trim={0mm 110mm 0mm 147mm}, clip]{images/lexicase-humaneval-2.pdf}
  \caption{HumanEval-Python.}
  \label{fig:humaneval-score-lexicase}
\end{subfigure}
% 
\caption{Rolling best average score for problems solved from the second or later attempts with lexicase selection.}
\label{fig:avg-score-lexicase}
% 
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \begin{mdframed}[style=mystyle]
\begin{framed}
\noindent
\textbf{Answer to RQ3 (ranking strategies in the generalizability experiments with GPT-3.5): } 
While quality-diversity ranking (regular tree search, or beam search) outperforms average quality-first ranking (lexicase selection) in several Python experiments, the number of actual reranking steps performed is low. 
No leading ranking strategy is found for SEIDR experiments with GPT-3.5 on PSB2 and HumanEval-X in Python and C++.
\end{framed}
% \end{mdframed}
% \vspace*{-2mm}


\begin{table}[htb]
    \centering
    \caption{Number of solved PSB2 problems using SEIDR with different LLMs and PushGP. The best results among SEIDR-only experiments at each $k$ are highlighted in bold. The best overall scores are underlined. If pass@100 is not available, the best reported results for a given method are mentioned in brackets.}\small
    \label{tab:generalizability-psb2}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{llllrrr}
\toprule
Model & Language & $N^*$ & Ranking &  pass@1 &  pass@10 &  pass@100 \\
\midrule
 SEIDR w/ GPT-3.5 & Python & 1   & average quality &       4 &        7 &         9 \\
&       & 2   & average quality &       6 &       11 &        12 \\
 &      &     & quality diversity  &       3 &        9 &         9 \\
 &      & 4   & average quality &       5 &        9 &        12 \\
&       &     & quality diversity  &       3 &       \textbf{12} &        13 \\
 &      & 10  & average quality &       6 &       10 &        12 \\
 &      &     & quality diversity  &       \textbf{7} &       \textbf{12} &        14 \\
 &      & 16  & average quality &       3 &       10 &        12 \\
 &      &     & quality diversity  &       6 &       \textbf{12} &        \textbf{15} \\
 &      & 100 & average quality &       4 &        5 &        14 \\
 \midrule
SEIDR w/ Code Llama 34B & Python &  16  & quality diversity &    0 &        0 &         0 \\
 SEIDR w/ GPT-3 & Python  & 10 & average quality &       - &        - &      (pass@1000=\underline{19}) \\
  \midrule
 PushGP & Python &  -   &    -            &       - &        - &      (17)\\
\midrule
\midrule
SEIDR w/ GPT-3.5 & C++ & 1   & average quality &       9 &       12 &        12 \\
  &     & 2   & average quality &      \textbf{10} &       \textbf{14} &        14 \\
  &     &     & quality diversity  &       8 &       13 &        13 \\
  &     & 4   & average quality &       8 &       \textbf{14} &        \textbf{15} \\
  &     &     & quality diversity  &      \textbf{10} &       11 &        14 \\
  &     & 10  & average quality &       9 &       12 &        \textbf{15} \\
  &     &     & quality diversity  &       8 &       11 &        14 \\
  &     & 16  & average quality &      \textbf{10} &       13 &        \textbf{15} \\
  &     &     & quality diversity  &       7 &       11 &        14 \\
  &     & 100 & average quality &       4 &        4 &         8 \\
  \midrule
SEIDR w/ Code Llama 34B & C++ &  16  & average quality  &    5 &        6 &         8 \\
SEIDR w/ GPT-3 & C++  & 10 & average quality &       - &        - &      (pass@1000=\underline{17})\\
\midrule
PushGP & C++ &  -   &    -            &       - &        - &      (\underline{17})\\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}




\begin{table}[htb]
    \centering
    \caption{Percentage of solved tasks for HumanEval-X using SEIDR with different LLMs. The best results among SEIDR-only experiments at each $k$ are highlighted in bold. The best overall scores are underlined. If pass@100 is not available, the best reported results are mentioned as in the corresponding papers. }\small
    \label{tab:generalizability-he}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{llllrrr}
\toprule
Model & Language & $N^*$ & Ranking &  pass@1 &  pass@10 &  pass@100 \\
\midrule
SEIDR w/ GPT-3.5 & Python & 1   & average quality &   56.71 &    60.37 &     64.02 \\
&        & 2   & average quality &   \textbf{58.54 }&    63.41 &     67.07 \\
&        &     & quality diversity  &   54.27 &    56.71 &     59.15 \\
&        & 4   & average quality &   \textbf{58.54} &    \textbf{65.24} &     \textbf{68.90} \\
&        &     & quality diversity  &   54.88 &    59.76 &     61.59 \\
&        & 10  & average quality &   53.66 &    60.37 &     61.59 \\
&        &     & quality diversity  &   55.49 &    59.76 &     61.59 \\
&        & 16  & average quality &   \textbf{58.54} &    64.63 &     65.85 \\
&        &     & quality diversity  &   54.27 &    60.37 &     63.41 \\
&        & 100 & average quality &   56.10 &    57.32 &     64.02 \\
\midrule
SEIDR w/ Code Llama 34 & Python & 4 &  average quality &   34.76 &    49.39 &     54.27 \\
\midrule
Code Llama 34B & Python & - &  - &  48.8  &  76.8   &    93.0 \\
Unnatural Code Llama 34B & Python & - &  - &  62.2  &  \underline{85.2}   &    \underline{95.4} \\
GPT-3.5 (ChatGPT) & Python & - &  - &  48.1  &  -   &    - \\
GPT-4 & Python & -&  - & \underline{67.0}   &  -   &    - \\
CodeGeeX & Python & - &  - &  22.89  &  39.57   &    60.92 \\
\midrule
 SEIDR w/ GPT-3.5 & C++ & 1   & average quality &   10.98 &    67.68 &     \underline{\textbf{93.29}} \\
&        & 2   & average quality &   11.59 &    53.05 &     90.85 \\
&        &     & quality diversity  &   11.59 &    46.34 &     87.80 \\
&        & 4   & average quality &   \textbf{14.63} &    43.90 &     87.20 \\
&        &     & quality diversity  &   14.02 &    43.29 &     87.20 \\
&        & 10  & average quality &   12.80 &    31.10 &     73.17 \\
&        &     & quality diversity  &   12.20 &    31.71 &     70.12 \\
&        & 16  & average quality &    9.15 &    28.66 &     77.44 \\
&        &     & quality diversity  &   11.59 &    26.83 &     76.22 \\
&        & 100 & average quality &   10.98 &    14.63 &     64.63 \\
\midrule
SEIDR w/ Code Llama 34B & C++ & 1 & average quality &    0.61 &    \underline{\textbf{77.44}} &     85.37 \\
\midrule
Code Llama 34 & C++ & - &  - &  \underline{47.8}  &  -   &    - \\
CodeGeeX & C++ & - &  - &  17.06  &  32.21   &    51.00 \\
\bottomrule
\end{tabular}
\end{adjustbox}

% \vspace{-4ex}
\end{table}


\paragraph{RQ4. Overall Generalizability of SEIDR}
\label{sec:seidr-results-rq3}

We choose the best performing tree arity for each benchmark and language to run SEIDR with Code Llama 34B and compare the results to the findings for GPT-3.5 and Code Llama 34B without SEIDR in Table~\ref{tab:generalizability-psb2} for PSB2 and Table~\ref{tab:generalizability-he} for HumanEval-X. 
The number of problems solved at different cutoffs ($k$) is reported for PSB2 and the percentage of solved problems at different $k$ is shown for HumanEval-X.
We compare the performance of PSB2 solutions synthesized with SEIDR to the PushGP genetic programming system with down-sampled lexicase selection~\cite{helmuth2022:problemsolving}. 
For HumanEval-X, we cite the performance of the state-of-the-art models as reported by their authors without SEIDR, such as CodeGeex~\cite{zheng2023:codegeex}, GPT-3.5 and GPT-4~\cite{openai2023:gpt4}, for reference, and compare SEIDR with GPT-3.5 and CodeLlama results at different cutoffs ($k$ for $pass@k$).

In the PSB2 experiments, SEIDR with GPT-3.5 and Code Llama 34B does not outperform SEIDR with GPT-3, where Codex was used for code generation and repair steps. 
This finding suggests that using instruction models pre-trained for code generation is a better choice for  \synthesize{} and \debug{} agents of SEIDR. 
SEIDR with Code Llama performs worse than other models used in SEIDR and does not solve any problem in Python. 
Closer inspection into generated candidate programs reveals that the model keeps generating near-misses which resemble correct code but do not pass tests.
Besides, during our experiments with Code Llama 34B loaded to ollama, we have found that the model needs frequent restarts. 

While SEIDR with GPT-3.5 and Code Llama 34B does not outperform other state-of-the-art models, SEIDR experiments with GPT-3.5 and $N^*<100,$ i.e., using repair steps as well as replacement as opposed to replacement-only evaluation, show consistently better results for GPT-3.5, which is also true for PSB2.  
Importantly, SEIDR  with Code Llama 34B outperforms the current state-of-the-art model CodeGeeX, on HumanEval-C++ when comparing pass@10 achieving 77.44\% of solved tasks.
Moreover, SEIDR with GPT-3.5 results in higher pass@100 than CodeGeeX: 93.29\% compared to 51.00\%. 



% \begin{mdframed}[style=mystyle]
\begin{framed}
\noindent
\textbf{Answer to RQ4:} 
The use of SEIDR with GPT-3.5 outperforms the use of GPT-3.5 without SEIDR, when the model is called with the same prompts as in SEIDR: higher pass@k values are observed with $N^*<100$ than with $N^*=100.$ SEIDR with Code Llama and GPT-3.5 does not outperform the current state-of-the-art results methods on PBS2, but does so for HumanEval-C++. 
\end{framed}
% \end{mdframed}


% \vspace{-10pt}

\newpage \subsection{Threats to Validity}
\label{sec:seidr-threats}

External threats to validity concern SEIDR performance on different other benchmarks and the use of more language models than the tested ones. 
Specifically, PSB2 and HumanEval-X contain programming tasks which require smaller functions to be generated than production-scale software.
While some canonical solutions provided in HumanEval-Python have been criticised~\cite{liu2023:your}, we primarily use unit tests to evaluate the output of SEIDR. Therefore, their weaknesses do not impact our results.
% We plan to extend our experiments in future work to explore the generalizability of results to more complex benchmarks.

Internal threats relate to the implementation.
We use PSB2, which has corner case tests in the training set and test regular cases in the test set. 
To ensure a fair comparison with other studies on PSB2, we evaluate and report results on the provided test set of PSB2 which risks that the synthesized programs do not pass some of the training cases. 
Large language models for code editing and text completion used in this study are nondeterministic, which impacts results. 
Due to prohibitive model inference costs, each experiment was only run once.
However, our temperature sampling procedure described in Section\ref{sec:seidr-synth} reduces this stochasticity significantly, especially for low-EPG results.
Codex, GPT-3, and GPT-3.5 are black-box models and may generate malicious code~\cite{pearce2022:asleep}. 
The results can be skewed towards high performance in popular programming languages in the pre-training dataset used by authors of the tested LLMs.

\newpage
\section{Conclusion}
\label{sec:seidr-conclusion}

In this study, we propose the multi-agent framework, SEIDR, to solve the challenge of fully autonomous programming. 
In SEIDR, the program synthesis procedure is originally augmented from generation of code with large language models instructions to iterative calls to a \debug{} agent with a successive \rank{} component. 
The \debug{} agent performs a tree search across the program candidates generated by a large language model for code.
The LLM used for code repair takes imperfect program candidates and instructions for their improvement as prompts. 
The instructions are obtained from both static templates with failing test case descriptions and templates with auto-generated bug summaries by a text completion language model. 
We explore 11 prompting strategies and the repair-replace trade-off of updating the draft program.

In addition to the initial exploration of hyperparameters that influence population size at each generation and number of children for each parent in SEIDR iterations, we extend the framework to two different ranking strategies and two additional models, making the study run three sets of experiments.

\head{Contributions}
We run one set of initial exploration experiments and two sets of generalizability experiments. 
In the initial exploration, we test SEIDR with the Codex-edit as the model for draft program synthesis and debugging in Python and C++ on the PSB2 benchmark. 
In our experiments, SEIDR outperforms the PushGP baseline and achieves the state-of-the-art result with 19 solved problems out of 25. 
It requires under 1000 program executions to solve them, in stark contrast to billions\footnote{A problem is considered "solved" by PushGP if at least 1 of 100 runs, each with a limit of 60 million programs, was successful.} of executions in PushGP, making it feasible in the areas with costly testing, such as robotics.
Investigation of the repair-replace trade-off shows that SEIDR with tree arity of 10 outperforms both the replace-only strategy and the repair-only approach. 
Our prompt engineering study shows that bug summaries generated with ``confidence indicators'', such as ``obviously'' improve the performance of SEIDR during C++ code synthesis. 
% Overall, our framework shows low performance variability with different prompts, which indicates its robustness.%

In the generalizability sets of experiments, we use SEIDR with GPT-3.5 and Code Llama 34B as the model for draft program synthesis, explaining errors in synthesized programs and their debugging in Python and C++ on the PSB2 and HumanEval-X benchmarks. 
Our experiments with ranking the code candidates at each generation with lexicase selection and the average test pass rate score do not show consistent improvement with either of the ranking policies. 
One observation is that the program candidates in generations prior to the final solution do not pass any test in the test suite in the vast majority of cases. 
The test pass rate for the final solution abruptly increases in one generation from 0 to all tests passed. 
In our generalizability experiments, SEIDR does not outperform the state-of-the-art baselines for PSB2 and HumanEval-Python but shows better performance than using LLMs without SEIDR with the same prompts as in SEIDR.
Moreover, the method achieves the state-of-the-art result for pass@10 (74.44\%) with Code Llama 34B and for pass@100 (93.29\%) with GPT-3.5 on HumanEval-C++. 
The approach requires under 100 program executions to obtain the reported results with GPT-3.5 and Code LLama..
SEIDR with moderate number of generated programs for each bug explanation is favourable both for obtaining a solution and for the speed of solving a task. 

\head{Future work}
Further investigation of SEIDR generalizability and ranking strategies are some of the areas for future work. 
Benchmarks with more tests than in HumanEval-X may shed more light on the most effective choice of the number of programs generated from each bug explanation, as well as the framework's comparison on large-context projects. 